{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "NORMALIZATION_METHODS = {\n",
    "    \"min_max\": \"Min-Max normalization to range [0,1]\",\n",
    "    \"z_score\": \"Z-score normalization (mean/std)\",\n",
    "    \"median\": \"Divide by median normalization\",\n",
    "    \"sigmoid\": \"Sigmoid function normalization\",\n",
    "    \"tanh_estimator\": \"Hyperbolic tangent estimator\", \n",
    "}\n",
    "\n",
    "class NormalizationResearch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_path: str = \"data\",\n",
    "        start_date: str = \"2019-01-01\",\n",
    "        end_date: str = \"2024-12-31\",\n",
    "        prediction_horizon: int = 5,\n",
    "        lookback_window: int = 20,\n",
    "        market_index: str = \"OSEBX.OL\",\n",
    "        fast_mode: bool = False,\n",
    "        random_seed: int = 42,\n",
    "        norm_methods: List[str] = None,\n",
    "        baseline_method: str = \"standard\",\n",
    "        n_folds: int = 5,\n",
    "        time_series_split: bool = True,\n",
    "        gap: int = 0,\n",
    "        test_size: float = 0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the NormalizationResearch class with k-fold cross-validation.\n",
    "        \n",
    "        Args:\n",
    "            cache_path: Path to cache data\n",
    "            start_date: Start date for data analysis\n",
    "            end_date: End date for data analysis\n",
    "            prediction_horizon: Number of days to predict ahead\n",
    "            lookback_window: Number of days to look back for LSTM\n",
    "            market_index: Stock market index to analyze\n",
    "            fast_mode: Whether to use a shortened time period for faster execution\n",
    "            random_seed: Random seed for reproducibility\n",
    "            norm_methods: List of normalization methods to evaluate\n",
    "            baseline_method: Baseline normalization method\n",
    "            n_folds: Number of folds for cross-validation\n",
    "            time_series_split: Whether to use time series split (vs random)\n",
    "            gap: Gap between train and test sets in days (to prevent data leakage)\n",
    "            test_size: Proportion of data to use for testing in each fold\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "        \n",
    "        if fast_mode:\n",
    "            print(\"Fast mode enabled - using shortened time period\")\n",
    "            start_date = (datetime.strptime(end_date, \"%Y-%m-%d\") - \n",
    "                         timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        self.cache_path = cache_path\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.lookback_window = lookback_window\n",
    "        self.market_index = market_index\n",
    "        self.fast_mode = fast_mode\n",
    "        self.random_seed = random_seed\n",
    "        self.n_folds = n_folds\n",
    "        self.time_series_split = time_series_split\n",
    "        self.gap = gap\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        self.data = None\n",
    "        self.features = None\n",
    "        self.feature_names = []\n",
    "        self.target = None\n",
    "        self.dates = None\n",
    "        self.results = {}\n",
    "        self.baseline_perf = {}\n",
    "        self.fold_results = {}\n",
    "        self.actual_returns = None\n",
    "        self.daily_returns = None\n",
    "        self.feature_distributions = {}\n",
    "        \n",
    "        if norm_methods is None:\n",
    "            self.norm_methods = list(NORMALIZATION_METHODS.keys())\n",
    "        else:\n",
    "            self.norm_methods = [method for method in norm_methods if method in NORMALIZATION_METHODS]\n",
    "            if not self.norm_methods:\n",
    "                raise ValueError(f\"No valid normalization methods specified. Available methods: {list(NORMALIZATION_METHODS.keys())}\")\n",
    "        \n",
    "        if baseline_method not in NORMALIZATION_METHODS:\n",
    "            raise ValueError(f\"Baseline method '{baseline_method}' not valid. Available methods: {list(NORMALIZATION_METHODS.keys())}\")\n",
    "        \n",
    "        self.baseline_method = baseline_method\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "    def fetch_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetches historical market data from Yahoo Finance.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Market data\n",
    "        \"\"\"\n",
    "        print(f\"Fetching {self.market_index} data from {self.start_date} to {self.end_date}...\")\n",
    "        if not os.path.exists(self.cache_path):\n",
    "            os.makedirs(self.cache_path)\n",
    "        else:\n",
    "            cache_file = f\"{self.cache_path}/{self.market_index}_{self.start_date}_{self.end_date}.parquet\"\n",
    "            if os.path.exists(cache_file):\n",
    "                print(\"Data already downloaded. Loading from file...\")\n",
    "                self.data = pd.read_parquet(cache_file)\n",
    "                if \"Date\" in self.data.columns:\n",
    "                    self.data.set_index(\"Date\", inplace=True)\n",
    "                return self.data\n",
    "    \n",
    "        buffer_days = max(100, self.lookback_window * 2)\n",
    "        buffer_start = (datetime.strptime(self.start_date, \"%Y-%m-%d\") - \n",
    "                    timedelta(days=buffer_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        market_data = None\n",
    "\n",
    "        market_data = yf.download(self.market_index, start=buffer_start, end=self.end_date, progress=False)\n",
    "        \n",
    "        required_columns = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "        for col in required_columns:\n",
    "            if col not in market_data.columns:\n",
    "                print(f\"Warning: {col} not found in data, using Close as fallback\")\n",
    "                market_data[col] = market_data[\"Close\"] if \"Close\" in market_data.columns else None\n",
    "        \n",
    "        data = pd.DataFrame(index=market_data.index)\n",
    "        data[\"close\"] = market_data[\"Close\"]\n",
    "        data[\"high\"] = market_data[\"High\"]\n",
    "        data[\"low\"] = market_data[\"Low\"]\n",
    "        data[\"open\"] = market_data[\"Open\"]\n",
    "        data[\"volume\"] = market_data[\"Volume\"]\n",
    "        \n",
    "        for col in [\"close\", \"high\", \"low\", \"open\"]:\n",
    "            data[col] = data[col].ffill().bfill()\n",
    "\n",
    "        self.data = data.loc[self.start_date:]\n",
    "        print(f\"Data loaded: {len(self.data)} trading days\")\n",
    "\n",
    "        self.daily_returns = self.data[\"close\"].pct_change().dropna()\n",
    "\n",
    "        cache_file = f\"{self.cache_path}/{self.market_index}_{self.start_date}_{self.end_date}.parquet\"\n",
    "        self.data.to_parquet(cache_file)\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def engineer_features(self) -> None:\n",
    "        \"\"\"\n",
    "        Engineers features from raw market data with strict time-awareness to prevent data leakage.\n",
    "        Features are calculated using only past data points for each time step.\n",
    "        \"\"\"\n",
    "        print(\"Engineering features (time-aware approach)...\")\n",
    "        df = self.data.copy()\n",
    "\n",
    "        for period in [1, 2, 3, 5, 10, 21]:\n",
    "            if period < len(df):\n",
    "                df[f\"return_{period}d\"] = df[\"close\"].pct_change(period)\n",
    "        \n",
    "        for period in [10, 20, 50]:\n",
    "            if period < len(df):\n",
    "                df[f\"ma_{period}d\"] = df[\"close\"].rolling(window=period).mean()\n",
    "                # Distance to moving average\n",
    "                df[f\"ma_dist_{period}d\"] = (df[\"close\"] / df[f\"ma_{period}d\"] - 1) * 100\n",
    "        \n",
    "        if \"return_1d\" in df.columns:\n",
    "            for period in [10, 21]:\n",
    "                if period < len(df):\n",
    "                    # Historical volatility calculation\n",
    "                    df[f\"vol_{period}d\"] = df[\"return_1d\"].rolling(window=period).std() * np.sqrt(252)\n",
    "        \n",
    "        if \"ma_20d\" in df.columns and \"ma_50d\" in df.columns:\n",
    "            df[\"ma_cross_20_50\"] = (df[\"ma_20d\"] > df[\"ma_50d\"]).astype(int)\n",
    "        \n",
    "        df[\"high_low_ratio\"] = df[\"high\"] / df[\"low\"]\n",
    "        df[\"close_open_ratio\"] = df[\"close\"] / df[\"open\"]\n",
    "        \n",
    "        if \"volume\" in df.columns and df[\"volume\"].sum() > 0:\n",
    "            df[\"volume_ma_10d\"] = df[\"volume\"].rolling(window=10).mean()\n",
    "            df[\"volume_ratio\"] = df[\"volume\"] / df[\"volume_ma_10d\"]\n",
    "\n",
    "        if self.prediction_horizon < len(df):\n",
    "            df[f\"target_return_{self.prediction_horizon}d\"] = df[\"close\"].pct_change(\n",
    "                self.prediction_horizon\n",
    "            ).shift(-self.prediction_horizon)\n",
    "            \n",
    "            df[\"target\"] = (df[f\"target_return_{self.prediction_horizon}d\"] > 0).astype(int)\n",
    "        else:\n",
    "            print(f\"Warning: Not enough data points for prediction horizon of {self.prediction_horizon}\")\n",
    "            shorter_horizon = max(1, min(3, len(df) // 3))\n",
    "            print(f\"Using shorter prediction horizon: {shorter_horizon} days\")\n",
    "            df[f\"target_return_{shorter_horizon}d\"] = df[\"close\"].pct_change(\n",
    "                shorter_horizon\n",
    "            ).shift(-shorter_horizon)\n",
    "            \n",
    "            df[\"target\"] = (df[f\"target_return_{shorter_horizon}d\"] > 0).astype(int)\n",
    "        \n",
    "        self.feature_names = [col for col in df.columns \n",
    "                            if not col.startswith(\"target\") \n",
    "                            and col != \"close\"\n",
    "                            and col != \"high\"\n",
    "                            and col != \"low\"\n",
    "                            and col != \"open\"\n",
    "                            and col != \"volume\"]\n",
    "                            \n",
    "        if not self.feature_names:\n",
    "            print(\"Warning: No features generated. Creating basic features.\")\n",
    "            df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "            self.feature_names = [\"log_return\"]\n",
    "        \n",
    "        df = df.dropna()\n",
    "        \n",
    "        self.features = df[self.feature_names]\n",
    "        self.target = df[\"target\"]\n",
    "        self.dates = df.index\n",
    "        self.actual_returns = df[f\"target_return_{self.prediction_horizon}d\"] if f\"target_return_{self.prediction_horizon}d\" in df.columns else df[\"target_return_3d\"]\n",
    "        \n",
    "        print(f\"Features prepared: {len(self.features)} samples with {len(self.feature_names)} features\")\n",
    "        print(f\"Class distribution: {df['target'].value_counts(normalize=True).to_dict()}\")\n",
    "        \n",
    "        if self.fast_mode and len(self.feature_names) > 15:\n",
    "            print(f\"Fast mode: limiting to 15 features (from {len(self.feature_names)})\")\n",
    "            self.feature_names = self.feature_names[:15]\n",
    "            self.features = self.features[self.feature_names]\n",
    "    \n",
    "    def engineer_advanced_features(self) -> None:\n",
    "        \"\"\"\n",
    "        Adds advanced technical indicators and market regime features.\n",
    "        All calculations are time-aware to prevent data leakage.\n",
    "        \"\"\"\n",
    "        print(\"Adding advanced features (time-aware approach)...\")\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        for period in [7, 14, 21]:\n",
    "            delta = df[\"close\"].diff()\n",
    "            gain = delta.mask(delta < 0, 0)\n",
    "            loss = -delta.mask(delta > 0, 0)\n",
    "            avg_gain = gain.rolling(window=period).mean()\n",
    "            avg_loss = loss.rolling(window=period).mean()\n",
    "            rs = avg_gain / avg_loss\n",
    "            df[f\"rsi_{period}\"] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        ema12 = df[\"close\"].ewm(span=12).mean()\n",
    "        ema26 = df[\"close\"].ewm(span=26).mean()\n",
    "        df[\"macd\"] = ema12 - ema26\n",
    "        df[\"macd_signal\"] = df[\"macd\"].ewm(span=9).mean()\n",
    "        df[\"macd_hist\"] = df[\"macd\"] - df[\"macd_signal\"]\n",
    "        \n",
    "        for period in [20]:\n",
    "            mid = df[\"close\"].rolling(window=period).mean()\n",
    "            std = df[\"close\"].rolling(window=period).std()\n",
    "            df[f\"bb_upper_{period}\"] = mid + 2*std\n",
    "            df[f\"bb_lower_{period}\"] = mid - 2*std\n",
    "            df[f\"bb_width_{period}\"] = (df[f\"bb_upper_{period}\"] - df[f\"bb_lower_{period}\"]) / mid\n",
    "            df[f\"bb_position_{period}\"] = (df[\"close\"] - df[f\"bb_lower_{period}\"]) / (df[f\"bb_upper_{period}\"] - df[f\"bb_lower_{period}\"])\n",
    "        \n",
    "        high_low = df[\"high\"] - df[\"low\"]\n",
    "        high_close = (df[\"high\"] - df[\"close\"].shift()).abs()\n",
    "        low_close = (df[\"low\"] - df[\"close\"].shift()).abs()\n",
    "        ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "        true_range = ranges.max(axis=1)\n",
    "        df[\"atr_14\"] = true_range.rolling(14).mean()\n",
    "        \n",
    "        tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "        tp_ma = tp.rolling(window=20).mean()\n",
    "        tp_md = tp.rolling(window=20).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "        df[\"cci_20\"] = (tp - tp_ma) / (0.015 * tp_md)\n",
    "        \n",
    "        low_min = df[\"low\"].rolling(window=14).min()\n",
    "        high_max = df[\"high\"].rolling(window=14).max()\n",
    "        df[\"stoch_k\"] = 100 * ((df[\"close\"] - low_min) / (high_max - low_min))\n",
    "        df[\"stoch_d\"] = df[\"stoch_k\"].rolling(window=3).mean()\n",
    "        \n",
    "        obv = [0]\n",
    "        for i in range(1, len(df)):\n",
    "            if df[\"close\"].iloc[i] > df[\"close\"].iloc[i-1]:\n",
    "                obv.append(obv[-1] + df[\"volume\"].iloc[i])\n",
    "            elif df[\"close\"].iloc[i] < df[\"close\"].iloc[i-1]:\n",
    "                obv.append(obv[-1] - df[\"volume\"].iloc[i])\n",
    "            else:\n",
    "                obv.append(obv[-1])\n",
    "        df[\"obv\"] = obv\n",
    "        \n",
    "        if \"vol_10d\" in df.columns:\n",
    "            df[\"volatility_ratio\"] = df[\"vol_10d\"].pct_change(5)\n",
    "            \n",
    "            if \"ma_dist_10d\" in df.columns:\n",
    "                df[\"trend_strength\"] = abs(df[\"ma_dist_10d\"]) / df[\"vol_10d\"]\n",
    "        \n",
    "        df[\"day_of_week\"] = pd.to_datetime(df.index).dayofweek\n",
    "        df[\"month\"] = pd.to_datetime(df.index).month\n",
    "        df[\"quarter\"] = pd.to_datetime(df.index).quarter\n",
    "        df[\"weekday\"] = pd.to_datetime(df.index).weekday < 5\n",
    "        \n",
    "        if \"vol_10d\" in df.columns and \"rsi_14\" in df.columns:\n",
    "            df[\"vol_rsi_14\"] = df[\"vol_10d\"] * df[\"rsi_14\"]\n",
    "            \n",
    "        if \"macd\" in df.columns and \"rsi_14\" in df.columns:\n",
    "            df[\"macd_rsi_14\"] = df[\"macd\"] * df[\"rsi_14\"]\n",
    "        \n",
    "        df[\"momentum_10d\"] = df[\"close\"] / df[\"close\"].shift(10) - 1\n",
    "        df[\"momentum_30d\"] = df[\"close\"] / df[\"close\"].shift(30) - 1\n",
    "        \n",
    "        df[\"roc_5d\"] = (df[\"close\"] / df[\"close\"].shift(5) - 1) * 100\n",
    "        df[\"roc_21d\"] = (df[\"close\"] / df[\"close\"].shift(21) - 1) * 100\n",
    "\n",
    "        self.data = df\n",
    "\n",
    "    def create_time_series_splits(self) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Creates time series splits for cross-validation with proper temporal gaps.\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[np.ndarray, np.ndarray]]: List of (train_idx, test_idx) pairs\n",
    "        \"\"\"\n",
    "        n_samples = len(self.features)\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        if self.time_series_split:\n",
    "            print(f\"Creating {self.n_folds} time series splits with gap={self.gap}\")\n",
    "\n",
    "            splits = []\n",
    "            fold_size = n_samples // (self.n_folds + 1)\n",
    "            \n",
    "            for fold in range(self.n_folds):\n",
    "                test_start = n_samples - (self.n_folds - fold) * fold_size\n",
    "                train_end = test_start - self.gap - 1\n",
    "                \n",
    "                if train_end <= self.lookback_window:\n",
    "                    print(f\"Warning: Not enough data for fold {fold+1}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                train_indices = indices[:train_end]\n",
    "                test_indices = indices[test_start:test_start + fold_size]\n",
    "                \n",
    "                if len(test_indices) == 0:\n",
    "                    print(f\"Warning: Empty test set for fold {fold+1}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Fold {fold+1}: train={len(train_indices)}, test={len(test_indices)}, \"\n",
    "                     f\"gap={test_start-train_end-1} days\")\n",
    "                \n",
    "                splits.append((train_indices, test_indices))\n",
    "        else:\n",
    "            print(f\"Creating {self.n_folds} random k-fold splits (no time awareness)\")\n",
    "            kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_seed)\n",
    "            splits = list(kf.split(indices))\n",
    "        \n",
    "        return splits\n",
    "\n",
    "    def prepare_train_test_data(self, train_idx: np.ndarray, test_idx: np.ndarray) -> Tuple:\n",
    "        \"\"\"\n",
    "        Prepares training and test data for a given fold.\n",
    "        \n",
    "        Args:\n",
    "            train_idx: Training indices\n",
    "            test_idx: Test indices\n",
    "            \n",
    "        Returns:\n",
    "            Tuple: X_train, X_test, y_train, y_test, dates_test, returns_test\n",
    "        \"\"\"\n",
    "        X_train = self.features.iloc[train_idx].copy()\n",
    "        X_test = self.features.iloc[test_idx].copy()\n",
    "        y_train = self.target.iloc[train_idx].copy()\n",
    "        y_test = self.target.iloc[test_idx].copy()\n",
    "        dates_test = self.dates[test_idx]\n",
    "        returns_test = self.actual_returns.iloc[test_idx].copy()\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, dates_test, returns_test\n",
    "    \n",
    "    def apply_normalization(self, method: str, X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Applies normalization method to features, fitting only on training data.\n",
    "        \n",
    "        Args:\n",
    "            method: Normalization method name\n",
    "            X_train: Training features\n",
    "            X_test: Test features\n",
    "            \n",
    "        Returns:\n",
    "            Tuple: Normalized training and test features\n",
    "        \"\"\"\n",
    "        self.feature_distributions[method] = {\n",
    "            'before': {\n",
    "                'train': X_train.copy(),\n",
    "                'test': X_test.copy()\n",
    "            },\n",
    "            'after': {}\n",
    "        }\n",
    "            \n",
    "        if method == \"min_max\":\n",
    "            X_train_norm = X_train.copy().values\n",
    "            X_test_norm = X_test.copy().values\n",
    "            \n",
    "            for col in range(X_train.shape[1]):\n",
    "                col_min = X_train.iloc[:, col].min()\n",
    "                col_max = X_train.iloc[:, col].max()\n",
    "                col_range = col_max - col_min\n",
    "                \n",
    "                if col_range != 0:\n",
    "                    X_train_norm[:, col] = (X_train.iloc[:, col].values - col_min) / col_range\n",
    "                    X_test_norm[:, col] = (X_test.iloc[:, col].values - col_min) / col_range\n",
    "                    X_test_norm[:, col] = np.clip(X_test_norm[:, col], 0, 1)\n",
    "        \n",
    "        elif method == \"z_score\":\n",
    "            X_train_norm = X_train.copy().values\n",
    "            X_test_norm = X_test.copy().values\n",
    "            \n",
    "            for col in range(X_train.shape[1]):\n",
    "                col_mean = X_train.iloc[:, col].mean()\n",
    "                col_std = X_train.iloc[:, col].std()\n",
    "                \n",
    "                if col_std != 0:\n",
    "                    X_train_norm[:, col] = (X_train.iloc[:, col].values - col_mean) / col_std\n",
    "                    X_test_norm[:, col] = (X_test.iloc[:, col].values - col_mean) / col_std\n",
    "        \n",
    "        elif method == \"median\":\n",
    "            X_train_norm = X_train.copy().values\n",
    "            X_test_norm = X_test.copy().values\n",
    "            \n",
    "            for col in range(X_train.shape[1]):\n",
    "                col_median = X_train.iloc[:, col].median()\n",
    "                \n",
    "                if col_median != 0:\n",
    "                    X_train_norm[:, col] = X_train.iloc[:, col].values / col_median\n",
    "                    X_test_norm[:, col] = X_test.iloc[:, col].values / col_median\n",
    "        \n",
    "        elif method == \"sigmoid\":\n",
    "            X_train_norm = X_train.copy().values\n",
    "            X_test_norm = X_test.copy().values\n",
    "            \n",
    "            for col in range(X_train.shape[1]):\n",
    "                X_train_norm[:, col] = 1 / (1 + np.exp(-X_train.iloc[:, col].values))\n",
    "                X_test_norm[:, col] = 1 / (1 + np.exp(-X_test.iloc[:, col].values))\n",
    "        \n",
    "        elif method == \"tanh_estimator\":\n",
    "            X_train_norm = X_train.copy().values\n",
    "            X_test_norm = X_test.copy().values\n",
    "            \n",
    "            for col in range(X_train.shape[1]):\n",
    "                col_mean = X_train.iloc[:, col].mean()\n",
    "                col_std = X_train.iloc[:, col].std()\n",
    "                \n",
    "                if col_std != 0:\n",
    "                    X_train_norm[:, col] = 0.5 * (np.tanh(0.01 * ((X_train.iloc[:, col].values - col_mean) / col_std)) + 1)\n",
    "                    X_test_norm[:, col] = 0.5 * (np.tanh(0.01 * ((X_test.iloc[:, col].values - col_mean) / col_std)) + 1)\n",
    "        \n",
    "            X_train_norm = np.clip(X_train_norm, -10, 10)\n",
    "            X_test_norm = np.clip(X_test_norm, -10, 10)\n",
    "            X_train_norm = np.nan_to_num(X_train_norm)\n",
    "            X_test_norm = np.nan_to_num(X_test_norm)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "        \n",
    "        self.feature_distributions[method]['after'] = {\n",
    "            'train': pd.DataFrame(X_train_norm, columns=X_train.columns),\n",
    "            'test': pd.DataFrame(X_test_norm, columns=X_test.columns)\n",
    "        }\n",
    "        \n",
    "        return X_train_norm, X_test_norm\n",
    "    \n",
    "    def optimize_lightgbm(self, X_train: np.ndarray, y_train: np.ndarray, class_weights: Dict[int, float] = None) -> lgb.LGBMClassifier:\n",
    "        \"\"\"Optimize LightGBM hyperparameters using Bayesian optimization with CV.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training target\n",
    "            class_weights: Optional class weights for imbalanced data\n",
    "            \n",
    "        Returns:\n",
    "            lgb.LGBMClassifier: Optimized model\n",
    "        \"\"\"\n",
    "        print(\"Optimizing LightGBM hyperparameters with CV...\")\n",
    "        \n",
    "        if len(X_train) < 100:\n",
    "            print(f\"Warning: Not enough data for reliable optimization ({len(X_train)} samples). Using default parameters.\")\n",
    "            return self.train_lightgbm_default(X_train, y_train, class_weights)\n",
    "        \n",
    "        n_trials = 15 if self.fast_mode else 50\n",
    "        n_inner_folds = 5\n",
    "        \n",
    "        scale_pos_weight = None\n",
    "        if class_weights is not None:\n",
    "            if 1 in class_weights and 0 in class_weights:\n",
    "                scale_pos_weight = class_weights[1] / class_weights[0]\n",
    "                \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"objective\": \"binary\",\n",
    "                \"metric\": \"binary_logloss\",\n",
    "                \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\"]),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 50),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 5.0, log=True),\n",
    "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 5.0, log=True),\n",
    "                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 50),\n",
    "                \"random_state\": self.random_seed,\n",
    "                \"verbosity\": -1\n",
    "            }\n",
    "            \n",
    "            if scale_pos_weight is not None:\n",
    "                params[\"scale_pos_weight\"] = scale_pos_weight\n",
    "\n",
    "            kf = KFold(n_splits=n_inner_folds, shuffle=True, random_state=self.random_seed)\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_train):\n",
    "                X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "                \n",
    "                if len(val_idx) < 10 or len(np.unique(y_val)) < 2:\n",
    "                    continue\n",
    "                \n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_tr, y_tr)\n",
    "                \n",
    "                preds = model.predict_proba(X_val)[:, 1]\n",
    "                score = roc_auc_score(y_val, preds)\n",
    "                cv_scores.append(score)\n",
    "\n",
    "            if cv_scores:\n",
    "                return np.mean(cv_scores)\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\", \n",
    "            sampler=TPESampler(seed=self.random_seed),\n",
    "            pruner=pruner\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            timeout = 300 if self.fast_mode else 900\n",
    "            study.optimize(objective, n_trials=n_trials, timeout=timeout, \n",
    "                        catch=(Exception,))\n",
    "            \n",
    "            if len(study.trials) == 0 or study.best_trial is None:\n",
    "                print(\"Optimization failed to complete any trials. Using default parameters.\")\n",
    "                return self.train_lightgbm_default(X_train, y_train, class_weights)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Optimization error: {str(e)}. Using default parameters.\")\n",
    "            return self.train_lightgbm_default(X_train, y_train, class_weights)\n",
    "        \n",
    "        try:\n",
    "            if not study.best_params:\n",
    "                print(\"No valid parameters found. Using default parameters.\")\n",
    "                return self.train_lightgbm_default(X_train, y_train, class_weights)\n",
    "                \n",
    "            print(\"\\nBest LightGBM Parameters:\")\n",
    "            for key, value in study.best_params.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            \n",
    "            best_params = study.best_params\n",
    "            best_params[\"objective\"] = \"binary\"\n",
    "            best_params[\"random_state\"] = self.random_seed\n",
    "            best_params[\"verbosity\"] = -1\n",
    "            \n",
    "            if scale_pos_weight is not None:\n",
    "                best_params[\"scale_pos_weight\"] = scale_pos_weight\n",
    "            \n",
    "            final_model = lgb.LGBMClassifier(**best_params)\n",
    "            final_model.fit(X_train, y_train)\n",
    "            \n",
    "            return final_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error building final model with best parameters: {str(e)}. Using default parameters.\")\n",
    "            return self.train_lightgbm_default(X_train, y_train, class_weights)\n",
    "    \n",
    "    def train_lightgbm_default(self, X_train: np.ndarray, y_train: np.ndarray, class_weights: Dict[int, float] = None) -> lgb.LGBMClassifier:\n",
    "        \"\"\"Train a LightGBM model with default parameters.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training target\n",
    "            class_weights: Optional class weights for imbalanced data\n",
    "            \n",
    "        Returns:\n",
    "            lgb.LGBMClassifier: Trained model\n",
    "        \"\"\"\n",
    "        print(\"Training LightGBM model with default parameters...\")\n",
    "        \n",
    "        scale_pos_weight = None\n",
    "        if class_weights is not None:\n",
    "            if 1 in class_weights and 0 in class_weights:\n",
    "                scale_pos_weight = class_weights[1] / class_weights[0]\n",
    "        \n",
    "        params = {\n",
    "            \"objective\": \"binary\",\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"n_estimators\": 1000,\n",
    "            \"max_depth\": 6,\n",
    "            \"num_leaves\": 20,\n",
    "            \"min_child_samples\": 50,\n",
    "            \"min_split_gain\": 0.01,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"reg_alpha\": 0.5,\n",
    "            \"reg_lambda\": 5.0,\n",
    "            \"random_state\": self.random_seed,\n",
    "            \"verbosity\": -1\n",
    "        }\n",
    "        \n",
    "        if scale_pos_weight is not None:\n",
    "            params[\"scale_pos_weight\"] = scale_pos_weight\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def prepare_lstm_data(self, X_train_norm: np.ndarray, X_test_norm: np.ndarray, \n",
    "                     y_train: np.ndarray, y_test: np.ndarray, batch_size: int = 32) -> Tuple:\n",
    "        \"\"\"\n",
    "        Prepares sequential data for LSTM model with time-aware approach.\n",
    "        Adapts lookback window if necessary to ensure data availability.\n",
    "        \n",
    "        Args:\n",
    "            X_train_norm: Normalized training features\n",
    "            X_test_norm: Normalized test features\n",
    "            y_train: Training target\n",
    "            y_test: Test target\n",
    "            batch_size: Batch size for DataLoader\n",
    "            \n",
    "        Returns:\n",
    "            Tuple: train_loader, X_test_lstm, y_test_lstm, lookback\n",
    "        \"\"\"\n",
    "        max_possible_lookback = min(len(X_train_norm), len(X_test_norm)) - 5\n",
    "        \n",
    "        if max_possible_lookback < 3:\n",
    "            raise ValueError(f\"Insufficient data for LSTM sequences. Need at least 8 samples, got {len(X_train_norm)} train, {len(X_test_norm)} test\")\n",
    "        \n",
    "        lookback = min(self.lookback_window, max_possible_lookback)\n",
    "        if lookback < self.lookback_window:\n",
    "            print(f\"Warning: Reduced lookback window from {self.lookback_window} to {lookback} due to limited data\")\n",
    "        \n",
    "        X_train_seq = []\n",
    "        y_train_seq = []\n",
    "        \n",
    "        for i in range(lookback, len(X_train_norm)):\n",
    "            X_train_seq.append(X_train_norm[i-lookback:i])\n",
    "            y_train_seq.append(y_train.iloc[i])\n",
    "        \n",
    "        X_test_seq = []\n",
    "        y_test_seq = []\n",
    "        \n",
    "        for i in range(lookback, len(X_test_norm)):\n",
    "            X_test_seq.append(X_test_norm[i-lookback:i])\n",
    "            y_test_seq.append(y_test.iloc[i])\n",
    "        \n",
    "        if not X_train_seq or not X_test_seq:\n",
    "            raise ValueError(f\"Could not create any valid sequences with lookback={lookback}\")\n",
    "        \n",
    "        X_train_seq = np.array(X_train_seq)\n",
    "        y_train_seq = np.array(y_train_seq)\n",
    "        X_test_seq = np.array(X_test_seq)\n",
    "        y_test_seq = np.array(y_test_seq)\n",
    "        \n",
    "        print(f\"Created LSTM sequences with lookback={lookback}: {len(X_train_seq)} train, {len(X_test_seq)} test\")\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train_seq)\n",
    "        y_train_tensor = torch.FloatTensor(y_train_seq).unsqueeze(1)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        batch_size = min(batch_size, len(X_train_tensor))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        return train_loader, torch.FloatTensor(X_test_seq), torch.FloatTensor(y_test_seq), lookback\n",
    "\n",
    "    class BidirectionalLSTM(nn.Module):\n",
    "        \"\"\"\n",
    "        Bidirectional LSTM neural network model with dropout for regularization.\n",
    "        \"\"\"\n",
    "        def __init__(self, input_dim: int, hidden_dim: int = 64, dropout: float = 0.3):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.lstm = nn.LSTM(\n",
    "                input_dim, \n",
    "                hidden_dim, \n",
    "                batch_first=True, \n",
    "                bidirectional=True,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            \n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(32, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            \n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            lstm_out = lstm_out[:, -1]\n",
    "            return self.fc(lstm_out)\n",
    "    \n",
    "    def train_lstm(self, train_loader: DataLoader, input_dim: int, lstm_params: Dict = None) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Trains an LSTM model with early stopping to prevent overfitting.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader with training data\n",
    "            input_dim: Number of input features\n",
    "            lstm_params: Optional hyperparameters from optimization\n",
    "            \n",
    "        Returns:\n",
    "            nn.Module: Trained LSTM model\n",
    "        \"\"\"\n",
    "        print(\"Training LSTM model...\")\n",
    "        \n",
    "        if lstm_params is None:\n",
    "            hidden_dim = 64\n",
    "            dropout = 0.3\n",
    "            learning_rate = 0.001\n",
    "            weight_decay = 1e-5\n",
    "        else:\n",
    "            hidden_dim = lstm_params.get(\"hidden_dim\", 64)\n",
    "            dropout = lstm_params.get(\"dropout\", 0.3)\n",
    "            learning_rate = lstm_params.get(\"learning_rate\", 0.001)\n",
    "            weight_decay = lstm_params.get(\"weight_decay\", 1e-5)\n",
    "        \n",
    "        model = self.BidirectionalLSTM(input_dim, hidden_dim=hidden_dim, dropout=dropout).to(self.device)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        num_epochs = 5 if self.fast_mode else 100\n",
    "        patience = 10\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "                if not self.fast_mode:\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience and not self.fast_mode:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def optimize_lstm(self, X_train_norm: np.ndarray, y_train: np.ndarray, lookback: int) -> Dict:\n",
    "        \"\"\"Optimize LSTM hyperparameters using Bayesian optimization with CV.\n",
    "        \n",
    "        Args:\n",
    "            X_train_norm: Normalized training features\n",
    "            y_train: Training target\n",
    "            lookback: Lookback window size\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Optimized hyperparameters\n",
    "        \"\"\"\n",
    "        print(\"Optimizing LSTM hyperparameters with CV...\")\n",
    "        \n",
    "        if len(X_train_norm) < 100:\n",
    "            print(f\"Warning: Not enough data for reliable LSTM optimization ({len(X_train_norm)} samples). Using default parameters.\")\n",
    "            return {\n",
    "                \"hidden_dim\": 64,\n",
    "                \"dropout\": 0.3, \n",
    "                \"learning_rate\": 0.001,\n",
    "                \"batch_size\": 32,\n",
    "                \"weight_decay\": 1e-5\n",
    "            }\n",
    "        \n",
    "        n_trials = 10 if self.fast_mode else 100\n",
    "        n_inner_folds = 3\n",
    "        \n",
    "        X_train_seq = []\n",
    "        y_train_seq = []\n",
    "        \n",
    "        for i in range(lookback, len(X_train_norm)):\n",
    "            X_train_seq.append(X_train_norm[i-lookback:i])\n",
    "            y_train_seq.append(y_train.iloc[i])\n",
    "        \n",
    "        X_train_seq = np.array(X_train_seq)\n",
    "        y_train_seq = np.array(y_train_seq)\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train_seq)\n",
    "        y_train_tensor = torch.FloatTensor(y_train_seq).unsqueeze(1)\n",
    "        \n",
    "        num_samples = len(X_train_tensor)\n",
    "        feature_dim = X_train_norm.shape[1]\n",
    "        \n",
    "        def objective(trial):\n",
    "            hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "            dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "            \n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "            batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "            weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "            \n",
    "            kf = KFold(n_splits=n_inner_folds, shuffle=True, random_state=self.random_seed)\n",
    "            cv_scores = []\n",
    "            \n",
    "            try:\n",
    "                for train_idx, val_idx in kf.split(range(num_samples)):\n",
    "                    X_tr, X_val = X_train_tensor[train_idx], X_train_tensor[val_idx]\n",
    "                    y_tr, y_val = y_train_tensor[train_idx], y_train_tensor[val_idx]\n",
    "                    \n",
    "                    if len(val_idx) < 10 or len(torch.unique(y_val)) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    train_dataset = TensorDataset(X_tr, y_tr)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    \n",
    "                    model = self.BidirectionalLSTM(\n",
    "                        input_dim=feature_dim,\n",
    "                        hidden_dim=hidden_dim,\n",
    "                        dropout=dropout\n",
    "                    ).to(self.device)\n",
    "                    \n",
    "                    criterion = nn.BCELoss()\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "                    \n",
    "                    epochs = 3 if self.fast_mode else 100\n",
    "                    for epoch in range(epochs):\n",
    "                        model.train()\n",
    "                        for inputs, labels in train_loader:\n",
    "                            inputs = inputs.to(self.device)\n",
    "                            labels = labels.to(self.device)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            \n",
    "                            loss.backward()\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                            optimizer.step()\n",
    "                    \n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        X_val = X_val.to(self.device)\n",
    "                        y_val = y_val.to(self.device)\n",
    "                        \n",
    "                        outputs = model(X_val)\n",
    "                        probs = outputs.cpu().numpy().flatten()\n",
    "                        y_true = y_val.cpu().numpy().flatten()\n",
    "                        \n",
    "                        try:\n",
    "                            score = roc_auc_score(y_true, probs)\n",
    "                            cv_scores.append(score)\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                if cv_scores:\n",
    "                    return np.mean(cv_scores)\n",
    "                else:\n",
    "                    return 0.0\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"LSTM trial failed: {str(e)}\")\n",
    "                return 0.0\n",
    "        \n",
    "        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\", \n",
    "            sampler=TPESampler(seed=self.random_seed),\n",
    "            pruner=pruner\n",
    "        )\n",
    "\n",
    "        lstm_default_params = {\n",
    "            \"hidden_dim\": 64,\n",
    "            \"dropout\": 0.3, \n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 32,\n",
    "            \"weight_decay\": 1e-5\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            timeout = 300 if self.fast_mode else 900\n",
    "            study.optimize(objective, n_trials=n_trials, timeout=timeout, \n",
    "                        catch=(Exception,))\n",
    "            \n",
    "            if len(study.trials) == 0 or study.best_trial is None:\n",
    "                print(\"LSTM optimization failed to complete any trials. Using default parameters.\")\n",
    "                return lstm_default_params\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"LSTM optimization error: {str(e)}. Using default parameters.\")\n",
    "            return lstm_default_params\n",
    "        \n",
    "        try:\n",
    "            if not study.best_params:\n",
    "                print(\"No valid parameters found for LSTM. Using default parameters.\")\n",
    "                return lstm_default_params\n",
    "                    \n",
    "            print(\"\\nBest LSTM Parameters:\")\n",
    "            for key, value in study.best_params.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            \n",
    "            return study.best_params\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error building LSTM model with best parameters: {str(e)}. Using default parameters.\")\n",
    "            return lstm_default_params\n",
    "    \n",
    "    def evaluate_models(self, lgb_model: lgb.LGBMClassifier, lstm_model: nn.Module, \n",
    "                      X_test: np.ndarray, y_test: np.ndarray, \n",
    "                      X_test_lstm: torch.Tensor, y_test_lstm: torch.Tensor,\n",
    "                      actual_returns: pd.Series, dates_test: pd.DatetimeIndex) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluates trained models on test data with comprehensive metrics.\n",
    "        Handles potential length mismatches between LSTM and regular test data.\n",
    "        \n",
    "        Args:\n",
    "            lgb_model: Trained LightGBM model\n",
    "            lstm_model: Trained LSTM model\n",
    "            X_test: Test features for LightGBM\n",
    "            y_test: Test target for LightGBM\n",
    "            X_test_lstm: Test features for LSTM\n",
    "            y_test_lstm: Test target for LSTM\n",
    "            actual_returns: Actual returns for the test period\n",
    "            dates_test: Dates for the test data\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"Evaluating models...\")\n",
    "        \n",
    "        lgb_preds = lgb_model.predict(X_test)\n",
    "        lgb_preds_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        predictions_df = pd.DataFrame({\n",
    "            'date': dates_test,\n",
    "            'actual_return': actual_returns,\n",
    "            'actual_direction': y_test,\n",
    "            'lgb_pred': lgb_preds,\n",
    "            'lgb_pred_proba': lgb_preds_proba\n",
    "        })\n",
    "        predictions_df.set_index('date', inplace=True)\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        metrics[\"lightgbm\"] = {\n",
    "            \"accuracy\": accuracy_score(y_test, lgb_preds),\n",
    "            \"f1\": f1_score(y_test, lgb_preds),\n",
    "            \"precision\": precision_score(y_test, lgb_preds),\n",
    "            \"recall\": recall_score(y_test, lgb_preds),\n",
    "            \"roc_auc\": roc_auc_score(y_test, lgb_preds_proba) if len(np.unique(y_test)) > 1 else 0.5\n",
    "        }\n",
    "        \n",
    "        cm = confusion_matrix(y_test, lgb_preds)\n",
    "        metrics[\"lightgbm\"][\"confusion_matrix\"] = cm\n",
    "        \n",
    "        metrics[\"lightgbm\"][\"classification_report\"] = classification_report(y_test, lgb_preds, output_dict=True)\n",
    "        \n",
    "        if lstm_model is not None and len(X_test_lstm) > 0 and len(y_test_lstm) > 0:\n",
    "            try:\n",
    "                if len(X_test_lstm) != len(y_test_lstm):\n",
    "                    print(f\"Warning: LSTM test data length mismatch - X: {len(X_test_lstm)}, y: {len(y_test_lstm)}\")\n",
    "                    print(\"Using minimum length for evaluation\")\n",
    "                    min_len = min(len(X_test_lstm), len(y_test_lstm))\n",
    "                    X_test_lstm = X_test_lstm[:min_len]\n",
    "                    y_test_lstm = y_test_lstm[:min_len]\n",
    "                \n",
    "                lstm_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    lstm_outputs = lstm_model(X_test_lstm.to(self.device))\n",
    "                    lstm_preds_proba = lstm_outputs.cpu().numpy().flatten()\n",
    "                    \n",
    "                    if len(lstm_preds_proba) != len(y_test_lstm):\n",
    "                        print(f\"Warning: LSTM output length mismatch - pred: {len(lstm_preds_proba)}, y: {len(y_test_lstm)}\")\n",
    "                        min_len = min(len(lstm_preds_proba), len(y_test_lstm))\n",
    "                        lstm_preds_proba = lstm_preds_proba[:min_len]\n",
    "                        y_test_lstm = y_test_lstm[:min_len]\n",
    "                    \n",
    "                    lstm_preds = (lstm_preds_proba > 0.5).astype(int)\n",
    "                \n",
    "                y_test_lstm_np = y_test_lstm.numpy()\n",
    "                \n",
    "                metrics[\"lstm\"] = {\n",
    "                    \"accuracy\": accuracy_score(y_test_lstm_np, lstm_preds),\n",
    "                    \"f1\": f1_score(y_test_lstm_np, lstm_preds),\n",
    "                    \"precision\": precision_score(y_test_lstm_np, lstm_preds),\n",
    "                    \"recall\": recall_score(y_test_lstm_np, lstm_preds),\n",
    "                    \"roc_auc\": roc_auc_score(y_test_lstm_np, lstm_preds_proba) if len(np.unique(y_test_lstm_np)) > 1 else 0.5\n",
    "                }\n",
    "                \n",
    "                cm_lstm = confusion_matrix(y_test_lstm_np, lstm_preds)\n",
    "                metrics[\"lstm\"][\"confusion_matrix\"] = cm_lstm\n",
    "                \n",
    "                diff = len(predictions_df) - len(lstm_preds)\n",
    "                \n",
    "                padded_lstm_preds = np.full(len(predictions_df), np.nan)\n",
    "                padded_lstm_probs = np.full(len(predictions_df), np.nan)\n",
    "                \n",
    "                padded_lstm_preds[diff:] = lstm_preds\n",
    "                padded_lstm_probs[diff:] = lstm_preds_proba\n",
    "                \n",
    "                predictions_df['lstm_pred'] = padded_lstm_preds\n",
    "                predictions_df['lstm_pred_proba'] = padded_lstm_probs\n",
    "                \n",
    "                print(f\"Successfully aligned predictions with {diff} NaN padding values\")\n",
    "                \n",
    "                print(f\"LSTM - Accuracy: {metrics['lstm']['accuracy']:.4f}, \"\n",
    "                    f\"F1: {metrics['lstm']['f1']:.4f}, \"\n",
    "                    f\"ROC AUC: {metrics['lstm']['roc_auc']:.4f}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating LSTM model: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        print(f\"LightGBM - Accuracy: {metrics['lightgbm']['accuracy']:.4f}, \"\n",
    "             f\"F1: {metrics['lightgbm']['f1']:.4f}, \"\n",
    "             f\"ROC AUC: {metrics['lightgbm']['roc_auc']:.4f}\")\n",
    "        \n",
    "        metrics[\"predictions\"] = predictions_df\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run_kfold_cv(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Performs k-fold cross-validation on the dataset for all normalization methods.\n",
    "        Uses time series splits to prevent data leakage.\n",
    "        Optimizes both LightGBM and LSTM models for the baseline method.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Results for each normalization method and fold\n",
    "        \"\"\"\n",
    "        print(f\"\\nRunning {self.n_folds}-fold cross-validation\")\n",
    "        \n",
    "        splits = self.create_time_series_splits()\n",
    "        \n",
    "        methods_to_evaluate = self.norm_methods.copy()\n",
    "        if self.baseline_method not in methods_to_evaluate:\n",
    "            methods_to_evaluate.append(self.baseline_method)\n",
    "\n",
    "        self.fold_results = {\n",
    "            method: {\n",
    "                f\"fold_{i+1}\": {} for i in range(len(splits))\n",
    "            } for method in methods_to_evaluate\n",
    "        }\n",
    "        \n",
    "        print(\"\\nOptimizing LightGBM and LSTM parameters for baseline method...\")\n",
    "        optimized_lgb_params = {}\n",
    "        optimized_lstm_params = {}\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(splits):\n",
    "            X_train, X_test, y_train, y_test, dates_test, returns_test = self.prepare_train_test_data(\n",
    "                train_idx, test_idx\n",
    "            )\n",
    "            \n",
    "            if len(X_train) < 30 or len(X_test) < 10:\n",
    "                continue\n",
    "                \n",
    "            class_counts = pd.Series(y_train).value_counts()\n",
    "            if 0 in class_counts and 1 in class_counts:\n",
    "                class_weights = {0: 1.0, 1: class_counts[0] / class_counts[1]}\n",
    "            else:\n",
    "                class_weights = None\n",
    "            \n",
    "            X_train_norm, X_test_norm = self.apply_normalization(self.baseline_method, X_train, X_test)\n",
    "            \n",
    "            if not self.fast_mode:\n",
    "                try:\n",
    "                    print(f\"\\nOptimizing LightGBM for fold {fold_idx+1} using {self.baseline_method} normalization\")\n",
    "                    lgb_model = self.optimize_lightgbm(X_train_norm, y_train.values, class_weights)\n",
    "                    lgb_params = {\n",
    "                        key: value for key, value in lgb_model.get_params().items() \n",
    "                        if key in [\"learning_rate\", \"n_estimators\", \"num_leaves\", \"max_depth\", \n",
    "                                \"subsample\", \"colsample_bytree\", \"reg_alpha\", \"reg_lambda\", \n",
    "                                \"min_child_samples\"]\n",
    "                    }\n",
    "                    optimized_lgb_params[f\"fold_{fold_idx+1}\"] = lgb_params\n",
    "                except Exception as e:\n",
    "                    print(f\"LightGBM optimization failed for fold {fold_idx+1}: {str(e)}\")\n",
    "                    optimized_lgb_params[f\"fold_{fold_idx+1}\"] = None\n",
    "                \n",
    "                try:\n",
    "                    print(f\"\\nOptimizing LSTM for fold {fold_idx+1} using {self.baseline_method} normalization\")\n",
    "                    _, _, _, lookback = self.prepare_lstm_data(X_train_norm, X_test_norm, y_train, y_test)\n",
    "                    \n",
    "                    lstm_params = self.optimize_lstm(X_train_norm, y_train, lookback)\n",
    "                    optimized_lstm_params[f\"fold_{fold_idx+1}\"] = lstm_params\n",
    "                except Exception as e:\n",
    "                    print(f\"LSTM optimization failed for fold {fold_idx+1}: {str(e)}\")\n",
    "                    optimized_lstm_params[f\"fold_{fold_idx+1}\"] = None\n",
    "            else:\n",
    "                optimized_lgb_params[f\"fold_{fold_idx+1}\"] = None\n",
    "                optimized_lstm_params[f\"fold_{fold_idx+1}\"] = None\n",
    "        \n",
    "        print(\"\\nStarting k-fold evaluation for all normalization methods\")\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(splits):\n",
    "            print(f\"\\nProcessing fold {fold_idx+1}/{len(splits)}\")\n",
    "            \n",
    "            X_train, X_test, y_train, y_test, dates_test, returns_test = self.prepare_train_test_data(\n",
    "                train_idx, test_idx\n",
    "            )\n",
    "            \n",
    "            if len(X_train) < 30 or len(X_test) < 10:\n",
    "                print(f\"Warning: Fold {fold_idx+1} has insufficient data (train: {len(X_train)}, test: {len(X_test)})\")\n",
    "                print(\"Skipping this fold\")\n",
    "                continue\n",
    "            \n",
    "            class_counts = pd.Series(y_train).value_counts()\n",
    "            print(f\"Class distribution in fold {fold_idx+1}: {class_counts.to_dict()}\")\n",
    "            \n",
    "            if 0 in class_counts and 1 in class_counts:\n",
    "                class_weights = {0: 1.0, 1: class_counts[0] / class_counts[1]}\n",
    "            else:\n",
    "                class_weights = None\n",
    "            \n",
    "            lgb_params = optimized_lgb_params.get(f\"fold_{fold_idx+1}\")\n",
    "            lstm_params = optimized_lstm_params.get(f\"fold_{fold_idx+1}\")\n",
    "            \n",
    "            for method in tqdm(methods_to_evaluate, desc=f\"Fold {fold_idx+1} - Evaluating methods\"):\n",
    "                print(f\"\\nTesting normalization method: {method} (Fold {fold_idx+1})\")\n",
    "                \n",
    "                try:\n",
    "                    X_train_norm, X_test_norm = self.apply_normalization(method, X_train, X_test)\n",
    "                    \n",
    "                    if lgb_params is not None:\n",
    "                        params = lgb_params.copy()\n",
    "                        params[\"random_state\"] = self.random_seed\n",
    "                        params[\"verbosity\"] = -1\n",
    "                        params[\"objective\"] = \"binary\"\n",
    "                        \n",
    "                        if class_weights is not None and 1 in class_weights and 0 in class_weights:\n",
    "                            params[\"scale_pos_weight\"] = class_weights[1] / class_weights[0]\n",
    "                        \n",
    "                        lgb_model = lgb.LGBMClassifier(**params)\n",
    "                        lgb_model.fit(X_train_norm, y_train.values)\n",
    "                    else:\n",
    "                        lgb_model = self.train_lightgbm_default(X_train_norm, y_train.values, class_weights)\n",
    "                    \n",
    "                    lstm_model = None\n",
    "                    model_metrics = {}\n",
    "                    \n",
    "                    try:\n",
    "                        batch_size = lstm_params.get(\"batch_size\", 32) if lstm_params else 32\n",
    "                        \n",
    "                        train_loader, X_test_lstm, y_test_lstm, _ = self.prepare_lstm_data(\n",
    "                            X_train_norm, X_test_norm, y_train, y_test, batch_size\n",
    "                        )\n",
    "                        \n",
    "                        lstm_model = self.train_lstm(train_loader, X_train.shape[1], lstm_params)\n",
    "                        \n",
    "                        model_metrics = self.evaluate_models(\n",
    "                            lgb_model, lstm_model, \n",
    "                            X_test_norm, y_test, \n",
    "                            X_test_lstm, y_test_lstm,\n",
    "                            returns_test, dates_test\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"LSTM training/evaluation failed: {str(e)}\")\n",
    "                        print(\"Evaluating only LightGBM model\")\n",
    "                        \n",
    "                        lgb_preds = lgb_model.predict(X_test_norm)\n",
    "                        lgb_preds_proba = lgb_model.predict_proba(X_test_norm)[:, 1]\n",
    "                        \n",
    "                        model_metrics = {\n",
    "                            \"lightgbm\": {\n",
    "                                \"accuracy\": accuracy_score(y_test, lgb_preds),\n",
    "                                \"f1\": f1_score(y_test, lgb_preds),\n",
    "                                \"precision\": precision_score(y_test, lgb_preds),\n",
    "                                \"recall\": recall_score(y_test, lgb_preds),\n",
    "                                \"roc_auc\": roc_auc_score(y_test, lgb_preds_proba) if len(np.unique(y_test)) > 1 else 0.5,\n",
    "                                \"confusion_matrix\": confusion_matrix(y_test, lgb_preds),\n",
    "                                \"classification_report\": classification_report(y_test, lgb_preds, output_dict=True)\n",
    "                            }\n",
    "                        }\n",
    "                        \n",
    "                        predictions_df = pd.DataFrame({\n",
    "                            'date': dates_test,\n",
    "                            'actual_return': returns_test,\n",
    "                            'actual_direction': y_test,\n",
    "                            'lgb_pred': lgb_preds,\n",
    "                            'lgb_pred_proba': lgb_preds_proba\n",
    "                        })\n",
    "                        predictions_df.set_index('date', inplace=True)\n",
    "                        \n",
    "                        model_metrics[\"predictions\"] = predictions_df\n",
    "                    \n",
    "                    self.fold_results[method][f\"fold_{fold_idx+1}\"] = model_metrics\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {method} (Fold {fold_idx+1}): {str(e)}\")\n",
    "                    self.fold_results[method][f\"fold_{fold_idx+1}\"] = {\"error\": str(e)}\n",
    "                \n",
    "                if \"lgb_model\" in locals():\n",
    "                    del lgb_model\n",
    "                if \"lstm_model\" in locals():\n",
    "                    del lstm_model\n",
    "                if \"train_loader\" in locals():\n",
    "                    del train_loader\n",
    "                if \"X_test_lstm\" in locals():\n",
    "                    del X_test_lstm\n",
    "                if \"y_test_lstm\" in locals():\n",
    "                    del y_test_lstm\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        self.process_fold_results()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def process_fold_results(self) -> None:\n",
    "        \"\"\"\n",
    "        Process fold results to calculate average metrics and standard deviations.\n",
    "        Includes additional financial and predictive performance metrics.\n",
    "        \"\"\"\n",
    "        print(\"\\nProcessing cross-validation results...\")\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "        for method in self.fold_results:\n",
    "            method_results = {}\n",
    "            \n",
    "            for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "                model_metrics = {}\n",
    "                \n",
    "                for metric in [\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"]:\n",
    "                    metric_values = []\n",
    "                    \n",
    "                    for fold in range(len(self.fold_results[method])):\n",
    "                        fold_name = f\"fold_{fold+1}\"\n",
    "                        if (fold_name in self.fold_results[method] and\n",
    "                            model_name in self.fold_results[method][fold_name] and\n",
    "                            metric in self.fold_results[method][fold_name][model_name]):\n",
    "                            metric_values.append(self.fold_results[method][fold_name][model_name][metric])\n",
    "                    \n",
    "                    if metric_values:\n",
    "                        model_metrics[metric] = np.mean(metric_values)\n",
    "                        model_metrics[f\"{metric}_std\"] = np.std(metric_values)\n",
    "                        model_metrics[f\"{metric}_values\"] = metric_values\n",
    "                \n",
    "                if model_metrics:\n",
    "                    method_results[model_name] = model_metrics\n",
    "            \n",
    "            self.results[method] = method_results\n",
    "            \n",
    "            if method == self.baseline_method:\n",
    "                self.baseline_perf = method_results\n",
    "    \n",
    "    def run_normalization_comparison(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Runs the complete normalization comparison experiment with k-fold CV.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Comparison results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"NORMALIZATION TECHNIQUES COMPARISON WITH {self.n_folds}-FOLD CV\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        self.fetch_data()\n",
    "        self.engineer_features()\n",
    "        self.engineer_advanced_features()\n",
    "        \n",
    "        return self.run_kfold_cv()\n",
    "    \n",
    "    def plot_results(self) -> None:\n",
    "        \"\"\"\n",
    "        Plots detailed results including classifier performance metrics.\n",
    "        \"\"\"\n",
    "        valid_results = {method: results for method, results in self.results.items() \n",
    "                       if isinstance(results, dict) and \"error\" not in results}\n",
    "        \n",
    "        if not valid_results:\n",
    "            print(\"No valid results to plot. All methods encountered errors.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            data = []\n",
    "            baseline_data = []\n",
    "            for norm_method, model_results in valid_results.items():\n",
    "                for model_name, metrics in model_results.items():\n",
    "                    for metric_name, value in metrics.items():\n",
    "                        if not metric_name.endswith(\"_std\") and not metric_name.endswith(\"_values\"):\n",
    "                            try:\n",
    "                                if isinstance(value, (int, float)) and not isinstance(value, bool):\n",
    "                                    data_point = {\n",
    "                                        \"Normalization\": norm_method,\n",
    "                                        \"Model\": model_name,\n",
    "                                        \"Metric\": metric_name,\n",
    "                                        \"Value\": value\n",
    "                                    }\n",
    "                                    \n",
    "                                    data.append(data_point)\n",
    "                                    \n",
    "                                    if norm_method == self.baseline_method:\n",
    "                                        baseline_data.append(data_point)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Warning: Could not process metric {metric_name} with value {value}: {str(e)}\")\n",
    "            \n",
    "            if not data:\n",
    "                print(\"No data available for plotting. Check if results contain valid metrics.\")\n",
    "                return\n",
    "                \n",
    "            df_results = pd.DataFrame(data)\n",
    "            df_baseline = pd.DataFrame(baseline_data) if baseline_data else None\n",
    "            \n",
    "            required_cols = [\"Normalization\", \"Model\", \"Metric\", \"Value\"]\n",
    "            if not all(col in df_results.columns for col in required_cols):\n",
    "                print(f\"Warning: DataFrame missing required columns. Columns found: {df_results.columns.tolist()}\")\n",
    "                return\n",
    "            \n",
    "            self.plot_classification_metrics(df_results, df_baseline)\n",
    "            self.plot_feature_distributions()\n",
    "            self.plot_roc_curves()\n",
    "            self.plot_confusion_matrices()\n",
    "            self.plot_fold_comparison()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in plotting results: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def plot_classification_metrics(self, df_results: pd.DataFrame, df_baseline: pd.DataFrame = None) -> None:\n",
    "        \"\"\"\n",
    "        Plots classification metrics for all normalization methods.\n",
    "        \n",
    "        Args:\n",
    "            df_results: DataFrame with all results\n",
    "            df_baseline: DataFrame with baseline results\n",
    "        \"\"\"\n",
    "        if df_results is None or df_results.empty:\n",
    "            print(\"No data available for classification metrics plot\")\n",
    "            return\n",
    "            \n",
    "        if not all(col in df_results.columns for col in [\"Normalization\", \"Model\", \"Metric\", \"Value\"]):\n",
    "            print(f\"Missing required columns for classification metrics. Available columns: {df_results.columns.tolist()}\")\n",
    "            return\n",
    "        \n",
    "        classification_metrics = [\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "        available_metrics = df_results[\"Metric\"].unique()\n",
    "        \n",
    "        metrics_to_plot = [metric for metric in classification_metrics if metric in available_metrics]\n",
    "        \n",
    "        if not metrics_to_plot:\n",
    "            print(f\"No classification metrics found in data. Available metrics: {available_metrics}\")\n",
    "            return\n",
    "        \n",
    "        n_metrics = len(metrics_to_plot)\n",
    "        n_rows = (n_metrics + 1) // 2\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 6 * n_rows))\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            try:\n",
    "                plt.subplot(n_rows, 2, i+1)\n",
    "                df_metric = df_results[df_results[\"Metric\"] == metric]\n",
    "                \n",
    "                if df_metric.empty:\n",
    "                    continue\n",
    "                \n",
    "                plot = sns.barplot(x=\"Normalization\", y=\"Value\", hue=\"Model\", data=df_metric)\n",
    "                \n",
    "                if df_baseline is not None:\n",
    "                    baseline_metric = df_baseline[df_baseline[\"Metric\"] == metric]\n",
    "                    if not baseline_metric.empty:\n",
    "                        for model in [\"lightgbm\", \"lstm\"]:\n",
    "                            model_baseline = baseline_metric[baseline_metric[\"Model\"] == model]\n",
    "                            if not model_baseline.empty:\n",
    "                                baseline_val = model_baseline[\"Value\"].values\n",
    "                                if len(baseline_val) > 0:\n",
    "                                    plt.axhline(\n",
    "                                        y=baseline_val[0], \n",
    "                                        linestyle=\"--\", \n",
    "                                        color=\"red\" if model == \"lightgbm\" else \"blue\",\n",
    "                                        alpha=0.7,\n",
    "                                        label=f\"{self.baseline_method} {model} baseline\"\n",
    "                                    )\n",
    "                \n",
    "                plt.title(f\"{metric.capitalize()} by Normalization Method\")\n",
    "                plt.xlabel(\"Normalization Method\")\n",
    "                plt.ylabel(metric.capitalize())\n",
    "                plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "                plt.legend(title=\"Model\")\n",
    "                plt.xticks(rotation=45)\n",
    "                \n",
    "                try:\n",
    "                    for container in plot.containers:\n",
    "                        plot.bar_label(container, fmt=\"%.3f\", fontsize=8)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not add bar labels: {str(e)}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting {metric}: {str(e)}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        try:\n",
    "            if not os.path.exists(\"visualizations\"):\n",
    "                os.makedirs(\"visualizations\")\n",
    "            plt.savefig(\"visualizations/classification_metrics.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save figure: {str(e)}\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_feature_distributions(self) -> None:\n",
    "        \"\"\"\n",
    "        Plots feature distributions before and after normalization for selected features.\n",
    "        Handles missing data gracefully.\n",
    "        \"\"\"\n",
    "        if not self.feature_distributions:\n",
    "            print(\"No feature distribution data available.\")\n",
    "            return\n",
    "        \n",
    "        methods_to_plot = [self.baseline_method] + [m for m in self.norm_methods if m != self.baseline_method]\n",
    "        methods_to_plot = [m for m in methods_to_plot if m in self.feature_distributions]\n",
    "        \n",
    "        if not methods_to_plot:\n",
    "            print(\"No valid normalization methods in feature distributions.\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            if self.features is not None and not self.features.empty:\n",
    "                feature_cols = list(self.features.columns)[:min(3, len(self.features.columns))]\n",
    "                \n",
    "                if not feature_cols:\n",
    "                    print(\"No features available for plotting distributions\")\n",
    "                    return\n",
    "                    \n",
    "                for feature in feature_cols:\n",
    "                    feature_exists = True\n",
    "                    for method in methods_to_plot:\n",
    "                        if (method not in self.feature_distributions or\n",
    "                            'before' not in self.feature_distributions[method] or\n",
    "                            'after' not in self.feature_distributions[method] or\n",
    "                            feature not in self.feature_distributions[method]['before']['train'].columns or\n",
    "                            feature not in self.feature_distributions[method]['after']['train'].columns):\n",
    "                            feature_exists = False\n",
    "                            break\n",
    "                    \n",
    "                    if not feature_exists:\n",
    "                        print(f\"Skipping feature {feature} as it's not available in all distributions\")\n",
    "                        continue\n",
    "                        \n",
    "                    plt.figure(figsize=(15, 10))\n",
    "                    \n",
    "                    for i, method in enumerate(methods_to_plot):\n",
    "                        plt.subplot(len(methods_to_plot), 2, i*2 + 1)\n",
    "                        before_train = self.feature_distributions[method]['before']['train'][feature]\n",
    "                        sns.histplot(before_train, kde=True)\n",
    "                        plt.title(f\"{method} - Before Normalization\")\n",
    "                        plt.xlabel(feature)\n",
    "                        \n",
    "                        plt.subplot(len(methods_to_plot), 2, i*2 + 2)\n",
    "                        after_train = self.feature_distributions[method]['after']['train'][feature]\n",
    "                        sns.histplot(after_train, kde=True)\n",
    "                        plt.title(f\"{method} - After Normalization\")\n",
    "                        plt.xlabel(feature)\n",
    "                    \n",
    "                    plt.suptitle(f\"Distribution of '{feature}' Before and After Normalization\")\n",
    "                    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                    plt.savefig(f\"visualizations/feature_distribution_{feature}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "                    plt.close()\n",
    "                \n",
    "                if len(feature_cols) > 0:\n",
    "                    feature = feature_cols[0]\n",
    "                    \n",
    "                    feature_exists = True\n",
    "                    for method in methods_to_plot:\n",
    "                        if (method not in self.feature_distributions or\n",
    "                            'after' not in self.feature_distributions[method] or\n",
    "                            feature not in self.feature_distributions[method]['after']['train'].columns):\n",
    "                            feature_exists = False\n",
    "                            break\n",
    "                    \n",
    "                    if feature_exists:\n",
    "                        n_methods = len(methods_to_plot)\n",
    "                        ncols = min(n_methods, 2)\n",
    "                        nrows = (n_methods + 1) // 2\n",
    "                        \n",
    "                        plt.figure(figsize=(7 * ncols, 5 * nrows))\n",
    "                        \n",
    "                        for i, method in enumerate(methods_to_plot):\n",
    "                            plt.subplot(nrows, ncols, i+1)\n",
    "                            after_train = self.feature_distributions[method]['after']['train'][feature]\n",
    "                            sns.histplot(after_train, kde=True)\n",
    "                            plt.title(f\"{method}\")\n",
    "                            plt.xlabel(feature)\n",
    "                        \n",
    "                        plt.suptitle(f\"Distribution of '{feature}' Across Normalization Methods\")\n",
    "                        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                        plt.savefig(f\"visualizations/normalization_comparison_{feature}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "                        plt.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting feature distributions: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def plot_roc_curves(self) -> None:\n",
    "        \"\"\"\n",
    "        Plots ROC curves for different normalization methods.\n",
    "        Handles cases where LSTM predictions might not be available or contain NaN values.\n",
    "        \"\"\"\n",
    "        fold_name = \"fold_1\"\n",
    "        \n",
    "        methods_to_plot = [self.baseline_method] + [m for m in self.norm_methods if m != self.baseline_method]\n",
    "        \n",
    "        for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "            try:\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                curves_plotted = 0\n",
    "                \n",
    "                for method in methods_to_plot:\n",
    "                    if (method in self.fold_results and \n",
    "                        fold_name in self.fold_results[method] and\n",
    "                        model_name in self.fold_results[method][fold_name] and\n",
    "                        \"predictions\" in self.fold_results[method][fold_name]):\n",
    "                        \n",
    "                        predictions = self.fold_results[method][fold_name][\"predictions\"]\n",
    "                        \n",
    "                        if model_name == \"lightgbm\":\n",
    "                            if all(col in predictions.columns for col in [\"actual_direction\", \"lgb_pred_proba\"]):\n",
    "                                y_true = predictions[\"actual_direction\"]\n",
    "                                y_score = predictions[\"lgb_pred_proba\"]\n",
    "                                \n",
    "                                fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "                                roc_auc = roc_auc_score(y_true, y_score)\n",
    "                                \n",
    "                                plt.plot(fpr, tpr, lw=2, label=f'{method} (AUC = {roc_auc:.3f})')\n",
    "                                curves_plotted += 1\n",
    "                        else:\n",
    "                            if all(col in predictions.columns for col in [\"actual_direction\", \"lstm_pred_proba\"]):\n",
    "                                mask = ~predictions[\"lstm_pred_proba\"].isna()\n",
    "                                if mask.sum() > 10:\n",
    "                                    y_true = predictions.loc[mask, \"actual_direction\"]\n",
    "                                    y_score = predictions.loc[mask, \"lstm_pred_proba\"]\n",
    "                                    \n",
    "                                    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "                                    roc_auc = roc_auc_score(y_true, y_score)\n",
    "                                    \n",
    "                                    plt.plot(fpr, tpr, lw=2, label=f'{method} (AUC = {roc_auc:.3f})')\n",
    "                                    curves_plotted += 1\n",
    "                \n",
    "                if curves_plotted > 0:\n",
    "                    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                    plt.xlim([0.0, 1.0])\n",
    "                    plt.ylim([0.0, 1.05])\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    plt.ylabel('True Positive Rate')\n",
    "                    plt.title(f'ROC Curves for {model_name.upper()}')\n",
    "                    plt.legend(loc=\"lower right\")\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.savefig(f\"visualizations/roc_curves_{model_name}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "                else:\n",
    "                    print(f\"No data available to plot ROC curves for {model_name}\")\n",
    "                \n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting ROC curves for {model_name}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                plt.close()\n",
    "    \n",
    "    def plot_confusion_matrices(self) -> None:\n",
    "        \"\"\"\n",
    "        Plots confusion matrices for different normalization methods.\n",
    "        Handles missing data and NaN values gracefully.\n",
    "        \"\"\"\n",
    "        fold_name = \"fold_1\"\n",
    "        \n",
    "        methods_to_plot = [self.baseline_method] + [m for m in self.norm_methods if m != self.baseline_method]\n",
    "        \n",
    "        for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "            try:\n",
    "                valid_methods = []\n",
    "                matrices = []\n",
    "                \n",
    "                for method in methods_to_plot:\n",
    "                    if (method in self.fold_results and \n",
    "                        fold_name in self.fold_results[method] and\n",
    "                        model_name in self.fold_results[method][fold_name] and\n",
    "                        \"confusion_matrix\" in self.fold_results[method][fold_name][model_name]):\n",
    "                        \n",
    "                        cm = self.fold_results[method][fold_name][model_name][\"confusion_matrix\"]\n",
    "                        if np.sum(cm) > 0:\n",
    "                            valid_methods.append(method)\n",
    "                            matrices.append(cm)\n",
    "                \n",
    "                if valid_methods:\n",
    "                    n_methods = len(valid_methods)\n",
    "                    ncols = min(n_methods, 2)\n",
    "                    nrows = (n_methods + 1) // 2\n",
    "                    \n",
    "                    plt.figure(figsize=(7 * ncols, 5 * nrows))\n",
    "                    \n",
    "                    for i, (method, cm) in enumerate(zip(valid_methods, matrices)):\n",
    "                        plt.subplot(nrows, ncols, i+1)\n",
    "                        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "                        plt.title(f'{method}')\n",
    "                        plt.ylabel('True Label')\n",
    "                        plt.xlabel('Predicted Label')\n",
    "                    \n",
    "                    plt.suptitle(f'Confusion Matrices for {model_name.upper()}')\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "                    plt.savefig(f\"visualizations/confusion_matrices_{model_name}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "                else:\n",
    "                    print(f\"No confusion matrices available for {model_name}\")\n",
    "                \n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting confusion matrices for {model_name}: {str(e)}\")\n",
    "                plt.close()\n",
    "    \n",
    "    def plot_fold_comparison(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot detailed fold-by-fold comparison between baseline and best methods.\n",
    "        \"\"\"\n",
    "        best_methods = {}\n",
    "        for metric in [\"accuracy\", \"f1\", \"roc_auc\"]:\n",
    "            for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "                best_value = 0\n",
    "                best_method = None\n",
    "                \n",
    "                for method, results in self.results.items():\n",
    "                    if (model_name in results and \n",
    "                        metric in results[model_name] and \n",
    "                        results[model_name][metric] > best_value):\n",
    "                        best_value = results[model_name][metric]\n",
    "                        best_method = method\n",
    "                \n",
    "                if best_method:\n",
    "                    key = f\"{model_name}_{metric}\"\n",
    "                    best_methods[key] = best_method\n",
    "        \n",
    "        for metric in [\"accuracy\", \"f1\", \"roc_auc\"]:\n",
    "            for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "                key = f\"{model_name}_{metric}\"\n",
    "                if key not in best_methods:\n",
    "                    continue\n",
    "                    \n",
    "                best_method = best_methods[key]\n",
    "                if best_method == self.baseline_method:\n",
    "                    continue\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                baseline_values = []\n",
    "                best_values = []\n",
    "                fold_labels = []\n",
    "                \n",
    "                num_folds = len(self.fold_results[self.baseline_method])\n",
    "                \n",
    "                for fold in range(num_folds):\n",
    "                    fold_name = f\"fold_{fold+1}\"\n",
    "                    fold_labels.append(f\"Fold {fold+1}\")\n",
    "                    \n",
    "                    if (fold_name in self.fold_results[self.baseline_method] and\n",
    "                        model_name in self.fold_results[self.baseline_method][fold_name] and\n",
    "                        metric in self.fold_results[self.baseline_method][fold_name][model_name]):\n",
    "                        baseline_values.append(\n",
    "                            self.fold_results[self.baseline_method][fold_name][model_name][metric]\n",
    "                        )\n",
    "                    else:\n",
    "                        baseline_values.append(0)\n",
    "                    \n",
    "                    if (fold_name in self.fold_results[best_method] and\n",
    "                        model_name in self.fold_results[best_method][fold_name] and\n",
    "                        metric in self.fold_results[best_method][fold_name][model_name]):\n",
    "                        best_values.append(\n",
    "                            self.fold_results[best_method][fold_name][model_name][metric]\n",
    "                        )\n",
    "                    else:\n",
    "                        best_values.append(0)\n",
    "                \n",
    "                bar_width = 0.35\n",
    "                index = np.arange(len(fold_labels))\n",
    "                \n",
    "                plt.bar(index, baseline_values, bar_width, \n",
    "                       label=f\"Baseline ({self.baseline_method})\", color=\"blue\", alpha=0.7)\n",
    "                plt.bar(index + bar_width, best_values, bar_width,\n",
    "                       label=f\"Best ({best_method})\", color=\"green\", alpha=0.7)\n",
    "                \n",
    "                plt.xlabel(\"Fold\")\n",
    "                plt.ylabel(metric.capitalize())\n",
    "                plt.title(f\"{model_name.upper()} - {metric.capitalize()} Across Folds\")\n",
    "                plt.xticks(index + bar_width / 2, fold_labels)\n",
    "                plt.legend()\n",
    "                plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "                \n",
    "                avg_baseline = np.mean(baseline_values)\n",
    "                avg_best = np.mean(best_values)\n",
    "                improvement = ((avg_best - avg_baseline) / avg_baseline) * 100\n",
    "                \n",
    "                plt.figtext(\n",
    "                    0.5, 0.01, \n",
    "                    f\"Overall improvement: {improvement:.2f}% ({avg_best:.4f} vs {avg_baseline:.4f})\",\n",
    "                    ha=\"center\", fontsize=12\n",
    "                )\n",
    "                \n",
    "                plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "                plt.savefig(f\"visualizations/fold_comparison_{model_name}_{metric}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "    \n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Print detailed summary of cross-validation results.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"NORMALIZATION METHOD PERFORMANCE SUMMARY ({self.n_folds}-FOLD CV)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        best_methods = {}\n",
    "        for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "            best_methods[model_name] = {}\n",
    "            for metric in [\"accuracy\", \"f1\", \"roc_auc\"]:\n",
    "                best_val = 0\n",
    "                best_method = None\n",
    "                \n",
    "                for method, results in self.results.items():\n",
    "                    if (model_name in results and \n",
    "                        metric in results[model_name] and \n",
    "                        results[model_name][metric] > best_val):\n",
    "                        best_val = results[model_name][metric]\n",
    "                        best_method = method\n",
    "                \n",
    "                if best_method:\n",
    "                    best_methods[model_name][metric] = (best_method, best_val)\n",
    "        \n",
    "        for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "            print(f\"\\n{model_name.upper()} MODEL RESULTS:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for metric in [\"accuracy\", \"f1\", \"roc_auc\"]:\n",
    "                if metric in best_methods[model_name]:\n",
    "                    best_method, best_val = best_methods[model_name][metric]\n",
    "                    print(f\"Best {metric}: {best_method} ({best_val:.4f})\")\n",
    "                    \n",
    "                    if (self.baseline_method in self.results and \n",
    "                        model_name in self.results[self.baseline_method] and\n",
    "                        metric in self.results[self.baseline_method][model_name]):\n",
    "                        \n",
    "                        baseline_val = self.results[self.baseline_method][model_name][metric]\n",
    "                        diff = best_val - baseline_val\n",
    "                        pct_change = (diff / baseline_val) * 100 if baseline_val != 0 else float(\"inf\")\n",
    "                        \n",
    "                        print(f\"  vs. baseline ({self.baseline_method}): {baseline_val:.4f} ({pct_change:+.2f}%)\")\n",
    "                        \n",
    "                        if f\"{metric}_std\" in self.results[best_method][model_name]:\n",
    "                            best_std = self.results[best_method][model_name][f\"{metric}_std\"]\n",
    "                            baseline_std = self.results[self.baseline_method][model_name][f\"{metric}_std\"]\n",
    "                            \n",
    "                            print(f\"  Std Dev: {best_std:.4f} vs. baseline: {baseline_std:.4f}\")\n",
    "            \n",
    "            print(\"\\nAll methods:\")\n",
    "            rows = []\n",
    "            for method in self.norm_methods:\n",
    "                if method not in self.results:\n",
    "                    continue\n",
    "                    \n",
    "                if model_name not in self.results[method]:\n",
    "                    continue\n",
    "                \n",
    "                row = [method]\n",
    "                \n",
    "                for metric in [\"accuracy\", \"f1\", \"roc_auc\"]:\n",
    "                    if metric in self.results[method][model_name]:\n",
    "                        val = self.results[method][model_name][metric]\n",
    "                        std = self.results[method][model_name].get(f\"{metric}_std\", 0)\n",
    "                        row.append(f\"{val:.4f} ± {std:.4f}\")\n",
    "                    else:\n",
    "                        row.append(\"N/A\")\n",
    "                \n",
    "                rows.append(row)\n",
    "            \n",
    "            if rows:\n",
    "                headers = [\"Method\", \"Accuracy\", \"F1-Score\", \"ROC AUC\"]\n",
    "                row_format = \"{:<15} {:<20} {:<20} {:<20}\"\n",
    "                print(row_format.format(*headers))\n",
    "                print(\"-\" * 75)\n",
    "                for row in rows:\n",
    "                    print(row_format.format(*row))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"OVERALL BEST NORMALIZATION METHODS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            \n",
    "            method_ranks = {}\n",
    "            for method in self.norm_methods:\n",
    "                if method not in self.results or model_name not in self.results[method]:\n",
    "                    continue\n",
    "                \n",
    "                ranks = []\n",
    "                for metric in [\"accuracy\", \"f1\", \"roc_auc\"]:\n",
    "                    if metric not in self.results[method][model_name]:\n",
    "                        continue\n",
    "                        \n",
    "                    values = [(m, self.results[m][model_name][metric]) \n",
    "                             for m in self.results \n",
    "                             if model_name in self.results[m] and metric in self.results[m][model_name]]\n",
    "                    \n",
    "                    values.sort(key=lambda x: x[1], reverse=True)\n",
    "                    \n",
    "                    for i, (m, _) in enumerate(values):\n",
    "                        if m == method:\n",
    "                            ranks.append(i + 1)\n",
    "                            break\n",
    "                \n",
    "                if ranks:\n",
    "                    method_ranks[method] = sum(ranks) / len(ranks)\n",
    "            \n",
    "            sorted_methods = sorted(method_ranks.items(), key=lambda x: x[1])\n",
    "            \n",
    "            print(\"Top methods by average rank:\")\n",
    "            for i, (method, avg_rank) in enumerate(sorted_methods[:3], 1):\n",
    "                acc = self.results[method][model_name].get(\"accuracy\", 0)\n",
    "                f1 = self.results[method][model_name].get(\"f1\", 0)\n",
    "                roc = self.results[method][model_name].get(\"roc_auc\", 0)\n",
    "                \n",
    "                print(f\"{i}. {method} (avg rank: {avg_rank:.2f}, acc: {acc:.4f}, f1: {f1:.4f}, roc_auc: {roc:.4f})\")\n",
    "        \n",
    "        # print(\"\\n\" + \"=\"*40)\n",
    "        # print(\"CONCLUSION AND RECOMMENDATIONS\")\n",
    "        # print(\"=\"*40)\n",
    "        \n",
    "        # all_ranks = {}\n",
    "        # for method in self.norm_methods:\n",
    "        #     if method not in self.results:\n",
    "        #         continue\n",
    "                \n",
    "        #     ranks = []\n",
    "        #     for model_name in [\"lightgbm\", \"lstm\"]:\n",
    "        #         if model_name not in self.results[method]:\n",
    "        #             continue\n",
    "                    \n",
    "        #         for metric in [\"accuracy\", \"f1\", \"roc_auc\"]:\n",
    "        #             if metric not in self.results[method][model_name]:\n",
    "        #                 continue\n",
    "                        \n",
    "        #             values = [(m, self.results[m][model_name][metric]) \n",
    "        #                      for m in self.results \n",
    "        #                      if model_name in self.results[m] and metric in self.results[m][model_name]]\n",
    "                    \n",
    "        #             values.sort(key=lambda x: x[1], reverse=True)\n",
    "                    \n",
    "        #             for i, (m, _) in enumerate(values):\n",
    "        #                 if m == method:\n",
    "        #                     ranks.append(i + 1)\n",
    "        #                     break\n",
    "            \n",
    "        #     if ranks:\n",
    "        #         all_ranks[method] = sum(ranks) / len(ranks)\n",
    "        \n",
    "        # sorted_all_methods = sorted(all_ranks.items(), key=lambda x: x[1])\n",
    "        \n",
    "        # if sorted_all_methods:\n",
    "        #     best_overall = sorted_all_methods[0][0]\n",
    "        #     print(f\"Based on the cross-validation results, the best overall normalization method is: {best_overall}\")\n",
    "        #     print(\"\\nRecommendations:\")\n",
    "        #     print(f\"1. Consider using {best_overall} normalization for Norwegian stock market prediction tasks\")\n",
    "        #     print(f\"2. The {best_overall} method showed consistent performance across different models and metrics\")\n",
    "        #     print(f\"3. For LightGBM specifically, consider {best_methods['lightgbm'].get('accuracy', ('N/A', 0))[0]} for classification accuracy\")\n",
    "            \n",
    "        #     print(\"\\nNote: These recommendations are based on historical testing of the Norwegian stock market and should be\")\n",
    "        #     print(\"periodically validated as market conditions change.\")\n",
    "\n",
    "    def perform_statistical_tests(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Performs statistical significance tests comparing normalization methods against the baseline.\n",
    "        Uses Wilcoxon signed-rank tests on fold results.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Statistical test results with p-values\n",
    "        \"\"\"\n",
    "        if not self.fold_results:\n",
    "            print(\"No fold results available for statistical testing.\")\n",
    "            return {}\n",
    "        \n",
    "        print(\"\\nPerforming statistical significance tests (Wilcoxon)...\")\n",
    "        \n",
    "        stat_results = {}\n",
    "        \n",
    "        metrics_to_test = [\"accuracy\", \"f1\", \"roc_auc\"]\n",
    "        models_to_test = [\"lightgbm\"]\n",
    "        \n",
    "        for method in self.fold_results:\n",
    "            for fold in self.fold_results[method]:\n",
    "                if \"lstm\" in self.fold_results[method][fold]:\n",
    "                    models_to_test.append(\"lstm\")\n",
    "                    break\n",
    "            break\n",
    "        \n",
    "        for model_name in models_to_test:\n",
    "            stat_results[model_name] = {}\n",
    "            \n",
    "            for metric in metrics_to_test:\n",
    "                stat_results[model_name][metric] = {}\n",
    "                \n",
    "                baseline_values = []\n",
    "                for fold in range(self.n_folds):\n",
    "                    fold_name = f\"fold_{fold+1}\"\n",
    "                    if (fold_name in self.fold_results[self.baseline_method] and\n",
    "                        model_name in self.fold_results[self.baseline_method][fold_name] and\n",
    "                        metric in self.fold_results[self.baseline_method][fold_name][model_name]):\n",
    "                        baseline_values.append(self.fold_results[self.baseline_method][fold_name][model_name][metric])\n",
    "                \n",
    "                if not baseline_values:\n",
    "                    continue\n",
    "                    \n",
    "                for method in self.norm_methods:\n",
    "                    if method == self.baseline_method:\n",
    "                        continue\n",
    "                    \n",
    "                    method_values = []\n",
    "                    for fold in range(self.n_folds):\n",
    "                        fold_name = f\"fold_{fold+1}\"\n",
    "                        if (fold_name in self.fold_results[method] and\n",
    "                            model_name in self.fold_results[method][fold_name] and\n",
    "                            metric in self.fold_results[method][fold_name][model_name]):\n",
    "                            method_values.append(self.fold_results[method][fold_name][model_name][metric])\n",
    "                    \n",
    "                    if not method_values:\n",
    "                        continue\n",
    "                    \n",
    "                    min_length = min(len(baseline_values), len(method_values))\n",
    "                    if min_length < 2:\n",
    "                        print(f\"Warning: Not enough samples for {model_name}/{metric}/{method} - need at least 2.\")\n",
    "                        continue\n",
    "                    \n",
    "                    baseline_values_trimmed = baseline_values[:min_length]\n",
    "                    method_values_trimmed = method_values[:min_length]\n",
    "                    \n",
    "                    mean_diff = np.mean(method_values_trimmed) - np.mean(baseline_values_trimmed)\n",
    "                    \n",
    "                    try:\n",
    "                        w_stat, p_value_w = stats.wilcoxon(method_values_trimmed, baseline_values_trimmed)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in Wilcoxon test for {model_name}/{metric}/{method}: {str(e)}\")\n",
    "                        w_stat, p_value_w = np.nan, np.nan\n",
    "\n",
    "                    is_significant = p_value_w < 0.05 if not np.isnan(p_value_w) else False\n",
    "                    \n",
    "                    stat_results[model_name][metric][method] = {\n",
    "                        \"mean_diff\": mean_diff,\n",
    "                        \"percent_improvement\": (mean_diff / np.mean(baseline_values_trimmed)) * 100,\n",
    "                        \"w_stat\": w_stat,\n",
    "                        \"p_value\": p_value_w,\n",
    "                        \"significant\": is_significant,\n",
    "                        \"baseline_mean\": np.mean(baseline_values_trimmed),\n",
    "                        \"method_mean\": np.mean(method_values_trimmed)\n",
    "                    }\n",
    "        \n",
    "        self.statistical_test_results = stat_results\n",
    "        return stat_results\n",
    "\n",
    "    def print_statistical_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Prints a summary of statistical significance test results.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STATISTICAL SIGNIFICANCE ANALYSIS (WILCOXON TEST)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for model_name in self.statistical_test_results:\n",
    "            print(f\"\\n{model_name.upper()} MODEL RESULTS:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for metric in self.statistical_test_results[model_name]:\n",
    "                print(f\"\\n{metric.upper()} (baseline: {self.baseline_method}):\")\n",
    "                \n",
    "                sorted_methods = sorted(\n",
    "                    self.statistical_test_results[model_name][metric].items(),\n",
    "                    key=lambda x: x[1][\"mean_diff\"],\n",
    "                    reverse=True\n",
    "                )\n",
    "                \n",
    "                headers = [\"Method\", \"Mean Diff\", \"% Improv\", \"p-value\", \"Significant\"]\n",
    "                print(f\"{headers[0]:<15} {headers[1]:<12} {headers[2]:<10} {headers[3]:<12} {headers[4]:<10}\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                for method, results in sorted_methods:\n",
    "                    mean_diff = f\"{results['mean_diff']:.4f}\"\n",
    "                    pct_improv = f\"{results['percent_improvement']:.2f}%\"\n",
    "                    p_value = f\"{results['p_value']:.4f}\" if not np.isnan(results['p_value']) else \"N/A\"\n",
    "                    sig = \"Yes*\" if results['significant'] else \"No\"\n",
    "                    \n",
    "                    if results['significant']:\n",
    "                        p_value += \"*\"\n",
    "                    \n",
    "                    print(f\"{method:<15} {mean_diff:<12} {pct_improv:<10} {p_value:<12} {sig:<10}\")\n",
    "                \n",
    "                print(\"\\n* p < 0.05 indicates statistical significance\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY OF STATISTICAL FINDINGS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        significant_findings = []\n",
    "        \n",
    "        for model_name in self.statistical_test_results:\n",
    "            for metric in self.statistical_test_results[model_name]:\n",
    "                sig_methods = [\n",
    "                    (method, results) \n",
    "                    for method, results in self.statistical_test_results[model_name][metric].items()\n",
    "                    if results['significant'] and results['mean_diff'] > 0\n",
    "                ]\n",
    "                \n",
    "                sig_methods.sort(key=lambda x: x[1]['mean_diff'], reverse=True)\n",
    "                \n",
    "                if sig_methods:\n",
    "                    top_method, top_results = sig_methods[0]\n",
    "                    finding = f\"{top_method} showed statistically significant improvement over {self.baseline_method} \"\n",
    "                    finding += f\"for {metric} in {model_name} (p < 0.05, {top_results['percent_improvement']:.2f}% improvement)\"\n",
    "                    significant_findings.append(finding)\n",
    "        \n",
    "        if significant_findings:\n",
    "            print(\"\\nKey significant findings:\")\n",
    "            for i, finding in enumerate(significant_findings, 1):\n",
    "                print(f\"{i}. {finding}\")\n",
    "        else:\n",
    "            print(\"\\nNo statistically significant improvements were found over the baseline method.\")\n",
    "        \n",
    "        print(\"\\nNote: Statistical significance was tested using the Wilcoxon signed-rank test.\")\n",
    "\n",
    "    def plot_statistical_significance(self, save_path=\"visualizations/\") -> None:\n",
    "        \"\"\"\n",
    "        Creates visualizations highlighting statistical significance of results.\n",
    "        \n",
    "        Args:\n",
    "            save_path: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        print(f\"\\nGenerating statistical significance visualizations in {save_path}\")\n",
    "        \n",
    "        for model_name in self.statistical_test_results:\n",
    "            for metric in self.statistical_test_results[model_name]:\n",
    "                try:\n",
    "                    methods = []\n",
    "                    mean_diffs = []\n",
    "                    is_significant = []\n",
    "                    \n",
    "                    for method, results in self.statistical_test_results[model_name][metric].items():\n",
    "                        methods.append(method)\n",
    "                        mean_diffs.append(results['mean_diff'])\n",
    "                        is_significant.append(results['significant'])\n",
    "                    \n",
    "                    if not methods:\n",
    "                        continue\n",
    "                    \n",
    "                    sorted_indices = np.argsort(mean_diffs)[::-1]  # Descending\n",
    "                    methods = [methods[i] for i in sorted_indices]\n",
    "                    mean_diffs = [mean_diffs[i] for i in sorted_indices]\n",
    "                    is_significant = [is_significant[i] for i in sorted_indices]\n",
    "                    \n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    \n",
    "                    colors = ['green' if sig else 'gray' for sig in is_significant]\n",
    "                    \n",
    "                    bars = plt.bar(range(len(methods)), mean_diffs, color=colors, alpha=0.7)\n",
    "                    \n",
    "                    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "                    \n",
    "                    for i, bar in enumerate(bars):\n",
    "                        height = bar.get_height()\n",
    "                        if is_significant[i]:\n",
    "                            plt.text(\n",
    "                                bar.get_x() + bar.get_width()/2,\n",
    "                                height + 0.001 if height > 0 else height - 0.003,\n",
    "                                '*',\n",
    "                                ha='center',\n",
    "                                va='bottom' if height > 0 else 'top',\n",
    "                                fontsize=16,\n",
    "                                fontweight='bold'\n",
    "                            )\n",
    "                    \n",
    "                    plt.title(f'Mean Difference vs {self.baseline_method} - {metric} ({model_name})', fontsize=14)\n",
    "                    plt.xlabel('Normalization Method')\n",
    "                    plt.ylabel(f'Mean Difference in {metric}')\n",
    "                    plt.xticks(range(len(methods)), methods, rotation=45)\n",
    "                    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "                    legend_elements = [\n",
    "                        Patch(facecolor='green', alpha=0.7, label='Statistically Significant (p < 0.05)'),\n",
    "                        Patch(facecolor='gray', alpha=0.7, label='Not Significant')\n",
    "                    ]\n",
    "                    plt.legend(handles=legend_elements, loc='best')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(f\"{save_path}significance_{model_name}_{metric}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "                    plt.close()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error plotting statistical significance for {model_name}/{metric}: {str(e)}\")\n",
    "                    plt.close()\n",
    "\n",
    "    def plot_pvalue_heatmap(self, save_path=\"visualizations/\") -> None:\n",
    "        \"\"\"\n",
    "        Creates a heatmap visualization of p-values across methods and metrics.\n",
    "        \n",
    "        Args:\n",
    "            save_path: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        for model_name in self.statistical_test_results:\n",
    "            try:\n",
    "                metrics = list(self.statistical_test_results[model_name].keys())\n",
    "                \n",
    "                if not metrics:\n",
    "                    continue\n",
    "                    \n",
    "                all_methods = set()\n",
    "                for metric in metrics:\n",
    "                    all_methods.update(self.statistical_test_results[model_name][metric].keys())\n",
    "                all_methods = sorted(list(all_methods))\n",
    "                \n",
    "                if not all_methods:\n",
    "                    continue\n",
    "                \n",
    "                heatmap_data = np.zeros((len(metrics), len(all_methods)))\n",
    "                annot_data = np.empty((len(metrics), len(all_methods)), dtype=object)\n",
    "                \n",
    "                for i, metric in enumerate(metrics):\n",
    "                    for j, method in enumerate(all_methods):\n",
    "                        if method in self.statistical_test_results[model_name][metric]:\n",
    "                            p_value = self.statistical_test_results[model_name][metric][method]['p_value']\n",
    "                            \n",
    "                            p_value = p_value if not np.isnan(p_value) else 1.0\n",
    "                            \n",
    "                            heatmap_data[i, j] = p_value\n",
    "                            \n",
    "                            annot_data[i, j] = f\"{p_value:.3f}\"\n",
    "                            if p_value < 0.05:\n",
    "                                annot_data[i, j] += \"*\"\n",
    "                            if p_value < 0.01:\n",
    "                                annot_data[i, j] += \"*\"\n",
    "                        else:\n",
    "                            heatmap_data[i, j] = 1.0  # Max p-value for missing data\n",
    "                            annot_data[i, j] = \"N/A\"\n",
    "                \n",
    "                plt.figure(figsize=(12, max(6, len(metrics) * 0.8)))\n",
    "                \n",
    "                cmap = plt.cm.YlGnBu_r\n",
    "                \n",
    "                ax = sns.heatmap(\n",
    "                    heatmap_data,\n",
    "                    annot=annot_data,\n",
    "                    fmt=\"\",\n",
    "                    cmap=cmap,\n",
    "                    vmin=0,\n",
    "                    vmax=0.1,\n",
    "                    linewidths=0.5,\n",
    "                    cbar_kws={'label': 'p-value'}\n",
    "                )\n",
    "                \n",
    "                cbar = ax.collections[0].colorbar\n",
    "                cbar.set_ticks([0, 0.01, 0.05, 0.1])\n",
    "                cbar.set_ticklabels(['0', '0.01', '0.05', '≥0.1'])\n",
    "                \n",
    "                plt.title(f'P-values Heatmap for {model_name}', fontsize=14)\n",
    "                plt.yticks(np.arange(len(metrics)) + 0.5, metrics, rotation=0)\n",
    "                plt.xticks(np.arange(len(all_methods)) + 0.5, all_methods, rotation=45, ha='right')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{save_path}pvalue_heatmap_{model_name}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating p-value heatmap for {model_name}: {str(e)}\")\n",
    "                plt.close()\n",
    "    \n",
    "    def save_results_to_csv(self, filename=\"normalization_results.csv\"):\n",
    "        \"\"\"\n",
    "        Save the results to a CSV file for further analysis.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the CSV file\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        for method in self.results:\n",
    "            for model in self.results[method]:\n",
    "                row = {\"Normalization\": method, \"Model\": model}\n",
    "                \n",
    "                for metric, value in self.results[method][model].items():\n",
    "                    if not metric.endswith(\"_std\") and not metric.endswith(\"_values\"):\n",
    "                        row[metric] = value\n",
    "                        if f\"{metric}_std\" in self.results[method][model]:\n",
    "                            row[f\"{metric}_std\"] = self.results[method][model][f\"{metric}_std\"]\n",
    "                \n",
    "                rows.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Impact of Normalization Techniques with Statistical Significance Testing\n",
      "============================================================\n",
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "NORMALIZATION TECHNIQUES COMPARISON WITH 10-FOLD CV\n",
      "==================================================\n",
      "Fetching OSEBX.OL data from 2014-01-01 to 2024-12-31...\n",
      "Data already downloaded. Loading from file...\n",
      "Engineering features (time-aware approach)...\n",
      "Features prepared: 1709 samples with 19 features\n",
      "Class distribution: {1: 0.5792861322410766, 0: 0.42071386775892333}\n",
      "Adding advanced features (time-aware approach)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:03:18,694] A new study created in memory with name: no-name-8a783415-d8d9-4ff0-af89-efd4600ecb05\n",
      "[I 2025-03-20 03:03:18,779] Trial 0 finished with value: 0.7378460000200813 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.013661178428178952, 'n_estimators': 456, 'num_leaves': 48, 'max_depth': 6, 'subsample': 0.7552942184455931, 'colsample_bytree': 0.7030385741204636, 'reg_alpha': 1.3087232248047334, 'reg_lambda': 0.6869757398160233, 'min_child_samples': 49}. Best is trial 0 with value: 0.7378460000200813.\n",
      "[I 2025-03-20 03:03:18,827] Trial 1 finished with value: 0.7430340219972068 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06323892443630524, 'n_estimators': 282, 'num_leaves': 42, 'max_depth': 3, 'subsample': 0.9077831487751247, 'colsample_bytree': 0.6012684467021177, 'reg_alpha': 0.31439409038438626, 'reg_lambda': 1.0912521853098582, 'min_child_samples': 47}. Best is trial 1 with value: 0.7430340219972068.\n",
      "[I 2025-03-20 03:03:18,877] Trial 2 finished with value: 0.7581315066232281 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.019957907467756848, 'n_estimators': 199, 'num_leaves': 37, 'max_depth': 10, 'subsample': 0.7873081624297713, 'colsample_bytree': 0.6493149518105418, 'reg_alpha': 3.6000609130029044, 'reg_lambda': 4.050134848257446, 'min_child_samples': 21}. Best is trial 2 with value: 0.7581315066232281.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 10-fold cross-validation\n",
      "Creating 10 time series splits with gap=5\n",
      "Fold 1: train=153, test=155, gap=5 days\n",
      "Fold 2: train=308, test=155, gap=5 days\n",
      "Fold 3: train=463, test=155, gap=5 days\n",
      "Fold 4: train=618, test=155, gap=5 days\n",
      "Fold 5: train=773, test=155, gap=5 days\n",
      "Fold 6: train=928, test=155, gap=5 days\n",
      "Fold 7: train=1083, test=155, gap=5 days\n",
      "Fold 8: train=1238, test=155, gap=5 days\n",
      "Fold 9: train=1393, test=155, gap=5 days\n",
      "Fold 10: train=1548, test=155, gap=5 days\n",
      "\n",
      "Optimizing LightGBM and LSTM parameters for baseline method...\n",
      "\n",
      "Optimizing LightGBM for fold 1 using min_max normalization\n",
      "Optimizing LightGBM hyperparameters with CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:03:18,925] Trial 3 finished with value: 0.794725944849784 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03308671790989007, 'n_estimators': 162, 'num_leaves': 10, 'max_depth': 5, 'subsample': 0.9963593754983149, 'colsample_bytree': 0.805256515122397, 'reg_alpha': 3.0841806167907238, 'reg_lambda': 0.1301677362213169, 'min_child_samples': 21}. Best is trial 3 with value: 0.794725944849784.\n",
      "[I 2025-03-20 03:03:18,999] Trial 4 finished with value: 0.7910122485088833 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.029437387715603364, 'n_estimators': 405, 'num_leaves': 47, 'max_depth': 6, 'subsample': 0.9716350708762611, 'colsample_bytree': 0.7998448041733087, 'reg_alpha': 2.304933318498776, 'reg_lambda': 3.3362817514757888, 'min_child_samples': 29}. Best is trial 3 with value: 0.794725944849784.\n",
      "[I 2025-03-20 03:03:19,046] Trial 5 finished with value: 0.7843851075339163 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07045793365915588, 'n_estimators': 210, 'num_leaves': 23, 'max_depth': 3, 'subsample': 0.8092590874861125, 'colsample_bytree': 0.859403208540551, 'reg_alpha': 3.3760828530802196, 'reg_lambda': 0.15012918259918107, 'min_child_samples': 22}. Best is trial 3 with value: 0.794725944849784.\n",
      "[I 2025-03-20 03:03:19,146] Trial 6 finished with value: 0.7922073162849176 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011970252710206382, 'n_estimators': 476, 'num_leaves': 49, 'max_depth': 4, 'subsample': 0.9816084577717837, 'colsample_bytree': 0.7776193095921519, 'reg_alpha': 0.2802529145561249, 'reg_lambda': 1.3744727196612017, 'min_child_samples': 25}. Best is trial 3 with value: 0.794725944849784.\n",
      "[I 2025-03-20 03:03:19,211] Trial 7 finished with value: 0.8081467448959708 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0967088301559984, 'n_estimators': 367, 'num_leaves': 50, 'max_depth': 10, 'subsample': 0.6725825297010686, 'colsample_bytree': 0.7765487989776441, 'reg_alpha': 1.3901311581414446, 'reg_lambda': 1.2894120399825089, 'min_child_samples': 25}. Best is trial 7 with value: 0.8081467448959708.\n",
      "[I 2025-03-20 03:03:19,275] Trial 8 finished with value: 0.7482635311249526 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04669085237595252, 'n_estimators': 388, 'num_leaves': 42, 'max_depth': 5, 'subsample': 0.6388242673991037, 'colsample_bytree': 0.9337670455562896, 'reg_alpha': 0.11037032003620384, 'reg_lambda': 0.25448376172072, 'min_child_samples': 45}. Best is trial 7 with value: 0.8081467448959708.\n",
      "[I 2025-03-20 03:03:19,337] Trial 9 finished with value: 0.8122517234638652 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011232085595448875, 'n_estimators': 177, 'num_leaves': 29, 'max_depth': 4, 'subsample': 0.7436406503252848, 'colsample_bytree': 0.6580654068535353, 'reg_alpha': 0.1699984374960256, 'reg_lambda': 0.1466016632681982, 'min_child_samples': 16}. Best is trial 9 with value: 0.8122517234638652.\n",
      "[I 2025-03-20 03:03:19,403] Trial 10 finished with value: 0.8310803885171231 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.010203239368765429, 'n_estimators': 107, 'num_leaves': 25, 'max_depth': 8, 'subsample': 0.7069078546074001, 'colsample_bytree': 0.6908092364947312, 'reg_alpha': 0.10041221396969888, 'reg_lambda': 0.3643418906227222, 'min_child_samples': 13}. Best is trial 10 with value: 0.8310803885171231.\n",
      "[I 2025-03-20 03:03:19,473] Trial 11 finished with value: 0.8309437672750366 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.010584247001202422, 'n_estimators': 105, 'num_leaves': 24, 'max_depth': 8, 'subsample': 0.7139880178778377, 'colsample_bytree': 0.6972493226467112, 'reg_alpha': 0.10851898378714334, 'reg_lambda': 0.3893436068121292, 'min_child_samples': 11}. Best is trial 10 with value: 0.8310803885171231.\n",
      "[I 2025-03-20 03:03:19,544] Trial 12 finished with value: 0.8515415388443387 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.018163303910654047, 'n_estimators': 100, 'num_leaves': 20, 'max_depth': 8, 'subsample': 0.6970181089776377, 'colsample_bytree': 0.7277729906866098, 'reg_alpha': 0.4890443135321556, 'reg_lambda': 0.3732270949355206, 'min_child_samples': 11}. Best is trial 12 with value: 0.8515415388443387.\n",
      "[I 2025-03-20 03:03:19,618] Trial 13 finished with value: 0.8444035109683246 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.01799896736950928, 'n_estimators': 121, 'num_leaves': 15, 'max_depth': 8, 'subsample': 0.6099175615913681, 'colsample_bytree': 0.7327796917902567, 'reg_alpha': 0.624621297496924, 'reg_lambda': 0.39942926719139454, 'min_child_samples': 10}. Best is trial 12 with value: 0.8515415388443387.\n",
      "[I 2025-03-20 03:03:19,689] Trial 14 finished with value: 0.7844036419675806 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.01875701063732107, 'n_estimators': 242, 'num_leaves': 14, 'max_depth': 8, 'subsample': 0.6026801984668724, 'colsample_bytree': 0.7476580045414775, 'reg_alpha': 0.5675326694633099, 'reg_lambda': 0.6549068695379046, 'min_child_samples': 35}. Best is trial 12 with value: 0.8515415388443387.\n",
      "[I 2025-03-20 03:03:19,763] Trial 15 finished with value: 0.809636157802802 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.018174221903841048, 'n_estimators': 151, 'num_leaves': 17, 'max_depth': 9, 'subsample': 0.6181855746593979, 'colsample_bytree': 0.9760821659339693, 'reg_alpha': 0.6274503046638632, 'reg_lambda': 0.27290575213336515, 'min_child_samples': 16}. Best is trial 12 with value: 0.8515415388443387.\n",
      "[I 2025-03-20 03:03:19,888] Trial 16 finished with value: 0.8560592733196717 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.026164906063309068, 'n_estimators': 314, 'num_leaves': 18, 'max_depth': 7, 'subsample': 0.6651795192374913, 'colsample_bytree': 0.8878755698081912, 'reg_alpha': 1.083173784109097, 'reg_lambda': 0.5039561408129002, 'min_child_samples': 10}. Best is trial 16 with value: 0.8560592733196717.\n",
      "[I 2025-03-20 03:03:19,967] Trial 17 finished with value: 0.7870421379894392 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.02760123638415903, 'n_estimators': 328, 'num_leaves': 20, 'max_depth': 7, 'subsample': 0.8449841175070596, 'colsample_bytree': 0.880382983683888, 'reg_alpha': 1.126088505655428, 'reg_lambda': 0.830339347517797, 'min_child_samples': 37}. Best is trial 16 with value: 0.8560592733196717.\n",
      "[I 2025-03-20 03:03:20,069] Trial 18 finished with value: 0.8421886830134913 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03946372513140026, 'n_estimators': 271, 'num_leaves': 34, 'max_depth': 7, 'subsample': 0.6772470431951765, 'colsample_bytree': 0.85917610767415, 'reg_alpha': 0.3463453486394099, 'reg_lambda': 2.0700082233542947, 'min_child_samples': 16}. Best is trial 16 with value: 0.8560592733196717.\n",
      "[I 2025-03-20 03:03:20,136] Trial 19 finished with value: 0.7645179345041372 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.023573663264591257, 'n_estimators': 318, 'num_leaves': 10, 'max_depth': 9, 'subsample': 0.667362922000207, 'colsample_bytree': 0.9113301919306592, 'reg_alpha': 0.9320380371960342, 'reg_lambda': 0.2099867860913448, 'min_child_samples': 40}. Best is trial 16 with value: 0.8560592733196717.\n",
      "[I 2025-03-20 03:03:20,210] Trial 20 finished with value: 0.8013418967625455 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.015339980850650834, 'n_estimators': 237, 'num_leaves': 29, 'max_depth': 7, 'subsample': 0.8448191737030581, 'colsample_bytree': 0.992827399172982, 'reg_alpha': 1.7131737805393947, 'reg_lambda': 0.4686483013442848, 'min_child_samples': 31}. Best is trial 16 with value: 0.8560592733196717.\n",
      "[I 2025-03-20 03:03:20,294] Trial 21 finished with value: 0.857550924980256 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.02354322019202556, 'n_estimators': 140, 'num_leaves': 15, 'max_depth': 9, 'subsample': 0.6392392740398731, 'colsample_bytree': 0.7472105296614974, 'reg_alpha': 0.47030520300856754, 'reg_lambda': 0.500293696538067, 'min_child_samples': 10}. Best is trial 21 with value: 0.857550924980256.\n",
      "[I 2025-03-20 03:03:20,371] Trial 22 finished with value: 0.8314581833117302 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.023910810212014657, 'n_estimators': 145, 'num_leaves': 19, 'max_depth': 9, 'subsample': 0.6508687513980185, 'colsample_bytree': 0.8184397284830398, 'reg_alpha': 0.8294586440540981, 'reg_lambda': 0.544573162851502, 'min_child_samples': 14}. Best is trial 21 with value: 0.857550924980256.\n",
      "[I 2025-03-20 03:03:20,539] Trial 23 finished with value: 0.8819194897398559 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03444159246436669, 'n_estimators': 352, 'num_leaves': 13, 'max_depth': 9, 'subsample': 0.724476370879777, 'colsample_bytree': 0.7439592921420766, 'reg_alpha': 0.4207815438686873, 'reg_lambda': 0.8207706648796971, 'min_child_samples': 10}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:20,668] Trial 24 finished with value: 0.8436106469919903 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.037874040143961084, 'n_estimators': 346, 'num_leaves': 14, 'max_depth': 10, 'subsample': 0.7344067234607558, 'colsample_bytree': 0.8333961163683786, 'reg_alpha': 0.44663981356432536, 'reg_lambda': 1.0133035355376856, 'min_child_samples': 18}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:20,824] Trial 25 finished with value: 0.8694762398726594 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04810088997815246, 'n_estimators': 419, 'num_leaves': 12, 'max_depth': 9, 'subsample': 0.7697432328779531, 'colsample_bytree': 0.7713549723689853, 'reg_alpha': 0.2124799261404179, 'reg_lambda': 2.0935775675288655, 'min_child_samples': 13}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:20,951] Trial 26 finished with value: 0.8500716636528993 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05454565254072128, 'n_estimators': 416, 'num_leaves': 11, 'max_depth': 9, 'subsample': 0.7760286701775468, 'colsample_bytree': 0.7583695159613084, 'reg_alpha': 0.2024298627320363, 'reg_lambda': 2.0630458023901515, 'min_child_samples': 19}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:21,112] Trial 27 finished with value: 0.8762306085801846 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.045050481503938665, 'n_estimators': 435, 'num_leaves': 13, 'max_depth': 10, 'subsample': 0.8225860826658231, 'colsample_bytree': 0.6607846922043091, 'reg_alpha': 0.21260290231685255, 'reg_lambda': 1.796423624311662, 'min_child_samples': 14}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:21,279] Trial 28 finished with value: 0.8683003183831021 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0464238227176156, 'n_estimators': 497, 'num_leaves': 11, 'max_depth': 10, 'subsample': 0.8317896608315682, 'colsample_bytree': 0.6060633406459508, 'reg_alpha': 0.18103550837089294, 'reg_lambda': 2.2260907675565504, 'min_child_samples': 14}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:21,394] Trial 29 finished with value: 0.8463304653124952 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08707673066717547, 'n_estimators': 431, 'num_leaves': 14, 'max_depth': 10, 'subsample': 0.8721814093671049, 'colsample_bytree': 0.6638443498170109, 'reg_alpha': 0.26214348960623823, 'reg_lambda': 2.4410017047147567, 'min_child_samples': 24}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:21,499] Trial 30 finished with value: 0.8167918682251027 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.035415298285062354, 'n_estimators': 458, 'num_leaves': 22, 'max_depth': 9, 'subsample': 0.7697720387073888, 'colsample_bytree': 0.710645781605167, 'reg_alpha': 0.14289075870846413, 'reg_lambda': 1.5877166901531197, 'min_child_samples': 28}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:21,662] Trial 31 finished with value: 0.8758917103670771 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04759123429765386, 'n_estimators': 491, 'num_leaves': 12, 'max_depth': 10, 'subsample': 0.8159005614373748, 'colsample_bytree': 0.6034320832099629, 'reg_alpha': 0.21941500726068927, 'reg_lambda': 3.2621553273160755, 'min_child_samples': 13}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:21,789] Trial 32 finished with value: 0.8438683366483206 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04520556568032182, 'n_estimators': 431, 'num_leaves': 13, 'max_depth': 10, 'subsample': 0.8981375155566447, 'colsample_bytree': 0.625456395811495, 'reg_alpha': 0.2377287998021623, 'reg_lambda': 4.971620359316415, 'min_child_samples': 18}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:21,966] Trial 33 finished with value: 0.877724280923904 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06632997647757997, 'n_estimators': 499, 'num_leaves': 17, 'max_depth': 10, 'subsample': 0.8170820902015196, 'colsample_bytree': 0.6329141666142772, 'reg_alpha': 0.36288345442122394, 'reg_lambda': 3.0723675209216013, 'min_child_samples': 13}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:22,142] Trial 34 finished with value: 0.8806520970392284 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06274323226881068, 'n_estimators': 492, 'num_leaves': 17, 'max_depth': 10, 'subsample': 0.8167439528673417, 'colsample_bytree': 0.6324958383148344, 'reg_alpha': 0.3573796670350799, 'reg_lambda': 2.999822097073425, 'min_child_samples': 13}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:22,286] Trial 35 finished with value: 0.8498881242820534 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06931601657954174, 'n_estimators': 455, 'num_leaves': 26, 'max_depth': 10, 'subsample': 0.8655932549503513, 'colsample_bytree': 0.6352121521724019, 'reg_alpha': 0.3636189291121942, 'reg_lambda': 3.2639918445042615, 'min_child_samples': 19}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:22,441] Trial 36 finished with value: 0.8730413830571994 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06648813531980188, 'n_estimators': 460, 'num_leaves': 17, 'max_depth': 10, 'subsample': 0.9352537077446509, 'colsample_bytree': 0.681225985220998, 'reg_alpha': 0.37606797219017285, 'reg_lambda': 2.8076232699797936, 'min_child_samples': 16}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:22,552] Trial 37 finished with value: 0.8382145554450038 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05879000044772112, 'n_estimators': 379, 'num_leaves': 21, 'max_depth': 9, 'subsample': 0.7900557414112253, 'colsample_bytree': 0.6393489707583409, 'reg_alpha': 0.14655697487111677, 'reg_lambda': 4.337658805945365, 'min_child_samples': 21}. Best is trial 23 with value: 0.8819194897398559.\n",
      "[I 2025-03-20 03:03:22,706] Trial 38 finished with value: 0.8865656011328376 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0819032584475924, 'n_estimators': 500, 'num_leaves': 17, 'max_depth': 10, 'subsample': 0.8037845297098607, 'colsample_bytree': 0.6626220662335015, 'reg_alpha': 0.3014158656716065, 'reg_lambda': 1.633537839062495, 'min_child_samples': 12}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:22,830] Trial 39 finished with value: 0.8453411416734206 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08344905254004688, 'n_estimators': 476, 'num_leaves': 27, 'max_depth': 6, 'subsample': 0.7953684251544864, 'colsample_bytree': 0.6766468190153823, 'reg_alpha': 0.3028751941079663, 'reg_lambda': 1.0153628555884582, 'min_child_samples': 23}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:22,966] Trial 40 finished with value: 0.869995195386567 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07966697273110204, 'n_estimators': 478, 'num_leaves': 33, 'max_depth': 9, 'subsample': 0.7522260025542231, 'colsample_bytree': 0.6253755412400629, 'reg_alpha': 0.4114324905799834, 'reg_lambda': 1.3084385369734954, 'min_child_samples': 12}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,121] Trial 41 finished with value: 0.8657857884476542 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05833743939878031, 'n_estimators': 449, 'num_leaves': 17, 'max_depth': 10, 'subsample': 0.8168252701182619, 'colsample_bytree': 0.6571418152017943, 'reg_alpha': 0.28113322431787124, 'reg_lambda': 1.69043149807438, 'min_child_samples': 15}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,255] Trial 42 finished with value: 0.8816194834016884 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09836367034268784, 'n_estimators': 497, 'num_leaves': 16, 'max_depth': 10, 'subsample': 0.8726022560498711, 'colsample_bytree': 0.7131265863776833, 'reg_alpha': 0.32286093229719315, 'reg_lambda': 1.7294285313962876, 'min_child_samples': 12}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,418] Trial 43 finished with value: 0.8759384849457538 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0941196119937379, 'n_estimators': 495, 'num_leaves': 23, 'max_depth': 10, 'subsample': 0.9063231328921306, 'colsample_bytree': 0.7115808269945066, 'reg_alpha': 0.32221063650496173, 'reg_lambda': 2.717489964889958, 'min_child_samples': 12}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,492] Trial 44 finished with value: 0.7600580373769901 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07547149216750576, 'n_estimators': 394, 'num_leaves': 16, 'max_depth': 9, 'subsample': 0.879865429553862, 'colsample_bytree': 0.6785726667435138, 'reg_alpha': 4.673879544484267, 'reg_lambda': 3.959186500785523, 'min_child_samples': 17}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,594] Trial 45 finished with value: 0.879409828740829 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09939966797035898, 'n_estimators': 478, 'num_leaves': 19, 'max_depth': 10, 'subsample': 0.8479500645935099, 'colsample_bytree': 0.7141588822222109, 'reg_alpha': 0.5392293731261125, 'reg_lambda': 0.8150907449372574, 'min_child_samples': 10}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,688] Trial 46 finished with value: 0.8782692252782439 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09953910254815977, 'n_estimators': 472, 'num_leaves': 42, 'max_depth': 8, 'subsample': 0.9352042099420312, 'colsample_bytree': 0.7141866827010958, 'reg_alpha': 0.7526764697380042, 'reg_lambda': 0.7653566860796237, 'min_child_samples': 10}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,765] Trial 47 finished with value: 0.7438869746263069 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09173465005263216, 'n_estimators': 355, 'num_leaves': 20, 'max_depth': 5, 'subsample': 0.8561496698204991, 'colsample_bytree': 0.7306416759144715, 'reg_alpha': 0.4847590116196665, 'reg_lambda': 1.1578458953158761, 'min_child_samples': 46}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,846] Trial 48 finished with value: 0.7514018685357359 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07535698407234766, 'n_estimators': 444, 'num_leaves': 19, 'max_depth': 10, 'subsample': 0.8917715635988317, 'colsample_bytree': 0.7932124922710738, 'reg_alpha': 0.5728316932224936, 'reg_lambda': 0.9146734889774805, 'min_child_samples': 49}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,946] Trial 49 finished with value: 0.8408901203341909 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08539311531899803, 'n_estimators': 280, 'num_leaves': 22, 'max_depth': 9, 'subsample': 0.844957223260667, 'colsample_bytree': 0.6969123895120719, 'reg_alpha': 0.6794394059062948, 'reg_lambda': 1.4672482030664549, 'min_child_samples': 20}. Best is trial 38 with value: 0.8865656011328376.\n",
      "[I 2025-03-20 03:03:23,986] A new study created in memory with name: no-name-ced9b34d-43a2-40da-81cb-13a0ca950bde\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LightGBM Parameters:\n",
      "  boosting_type: gbdt\n",
      "  learning_rate: 0.0819032584475924\n",
      "  n_estimators: 500\n",
      "  num_leaves: 17\n",
      "  max_depth: 10\n",
      "  subsample: 0.8037845297098607\n",
      "  colsample_bytree: 0.6626220662335015\n",
      "  reg_alpha: 0.3014158656716065\n",
      "  reg_lambda: 1.633537839062495\n",
      "  min_child_samples: 12\n",
      "\n",
      "Optimizing LSTM for fold 1 using min_max normalization\n",
      "Created LSTM sequences with lookback=10: 143 train, 145 test\n",
      "Optimizing LSTM hyperparameters with CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:03:27,748] Trial 0 finished with value: 0.893012894705603 and parameters: {'hidden_dim': 45, 'dropout': 0.4551406810921512, 'learning_rate': 0.007331811314274292, 'batch_size': 16, 'weight_decay': 2.064121088592392e-05}. Best is trial 0 with value: 0.893012894705603.\n",
      "[I 2025-03-20 03:03:29,725] Trial 1 finished with value: 0.8873530645666062 and parameters: {'hidden_dim': 79, 'dropout': 0.48569536770002286, 'learning_rate': 0.003999161563860724, 'batch_size': 32, 'weight_decay': 3.45872269711161e-05}. Best is trial 0 with value: 0.893012894705603.\n",
      "[I 2025-03-20 03:03:33,402] Trial 2 finished with value: 0.8751796139816973 and parameters: {'hidden_dim': 32, 'dropout': 0.21712375681979076, 'learning_rate': 0.0016665895921660098, 'batch_size': 16, 'weight_decay': 2.1517123363927774e-05}. Best is trial 0 with value: 0.893012894705603.\n",
      "[I 2025-03-20 03:03:35,317] Trial 3 finished with value: 0.7428306502525253 and parameters: {'hidden_dim': 127, 'dropout': 0.2873081624297713, 'learning_rate': 0.00017643094449433023, 'batch_size': 32, 'weight_decay': 1.0947309020486406e-05}. Best is trial 0 with value: 0.893012894705603.\n",
      "[I 2025-03-20 03:03:39,282] Trial 4 finished with value: 0.8414523844211343 and parameters: {'hidden_dim': 47, 'dropout': 0.10585094112968818, 'learning_rate': 0.0004451295365396247, 'batch_size': 16, 'weight_decay': 1.3639281514957935e-06}. Best is trial 0 with value: 0.893012894705603.\n",
      "[I 2025-03-20 03:03:40,724] Trial 5 finished with value: 0.9087774783087283 and parameters: {'hidden_dim': 59, 'dropout': 0.2875597071705984, 'learning_rate': 0.0033384615669657483, 'batch_size': 64, 'weight_decay': 9.98214837316597e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:03:47,613] Trial 6 finished with value: 0.8788922882672883 and parameters: {'hidden_dim': 109, 'dropout': 0.4586320455614753, 'learning_rate': 0.0009209968619749436, 'batch_size': 16, 'weight_decay': 1.6223673683559225e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:03:49,482] Trial 7 finished with value: 0.8717847208732626 and parameters: {'hidden_dim': 82, 'dropout': 0.3594032085405511, 'learning_rate': 0.006298297451196278, 'batch_size': 32, 'weight_decay': 7.511737052942637e-05}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:03:50,779] Trial 8 finished with value: 0.8440601932789433 and parameters: {'hidden_dim': 125, 'dropout': 0.17649687394806254, 'learning_rate': 0.00809174687672524, 'batch_size': 64, 'weight_decay': 5.656354626844694e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:03:54,499] Trial 9 finished with value: 0.82740374498187 and parameters: {'hidden_dim': 127, 'dropout': 0.3668293205677101, 'learning_rate': 0.009614679235250567, 'batch_size': 16, 'weight_decay': 2.2160868002868673e-05}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:03:55,785] Trial 10 finished with value: 0.8952415109967194 and parameters: {'hidden_dim': 68, 'dropout': 0.2783115442415032, 'learning_rate': 0.0020861108338934425, 'batch_size': 64, 'weight_decay': 4.712885048357283e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:03:57,185] Trial 11 finished with value: 0.8841257459747044 and parameters: {'hidden_dim': 69, 'dropout': 0.2839268982313757, 'learning_rate': 0.0019222890575082043, 'batch_size': 64, 'weight_decay': 4.899133576690419e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:03:58,527] Trial 12 finished with value: 0.8861641252266251 and parameters: {'hidden_dim': 64, 'dropout': 0.22885194066795841, 'learning_rate': 0.0026587501928706167, 'batch_size': 64, 'weight_decay': 3.460656555538206e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:03:59,938] Trial 13 finished with value: 0.8516579726215143 and parameters: {'hidden_dim': 97, 'dropout': 0.355353283883613, 'learning_rate': 0.0008030291804136277, 'batch_size': 64, 'weight_decay': 1.0840506859901975e-05}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:01,336] Trial 14 finished with value: 0.8826828649745316 and parameters: {'hidden_dim': 55, 'dropout': 0.31653470353178464, 'learning_rate': 0.0033529860098850885, 'batch_size': 64, 'weight_decay': 3.0743961693868685e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:02,834] Trial 15 finished with value: 0.7798931489035655 and parameters: {'hidden_dim': 84, 'dropout': 0.23443397896592474, 'learning_rate': 0.0003543779292407648, 'batch_size': 64, 'weight_decay': 8.73103087669017e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:04,224] Trial 16 finished with value: 0.8663385322239489 and parameters: {'hidden_dim': 67, 'dropout': 0.4073530676872772, 'learning_rate': 0.0016532604402274915, 'batch_size': 64, 'weight_decay': 2.422514320377802e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:05,631] Trial 17 finished with value: 0.8886194004683589 and parameters: {'hidden_dim': 32, 'dropout': 0.13580248957982521, 'learning_rate': 0.00472778532989426, 'batch_size': 64, 'weight_decay': 5.670885701167959e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:06,967] Trial 18 finished with value: 0.7816689531533281 and parameters: {'hidden_dim': 95, 'dropout': 0.26605777678480524, 'learning_rate': 0.0005307863900589909, 'batch_size': 64, 'weight_decay': 4.997557883497495e-05}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:08,348] Trial 19 finished with value: 0.6797915722394889 and parameters: {'hidden_dim': 55, 'dropout': 0.327392532265835, 'learning_rate': 0.00011600174401008022, 'batch_size': 64, 'weight_decay': 1.333031756030595e-05}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:10,235] Trial 20 finished with value: 0.882898934591643 and parameters: {'hidden_dim': 72, 'dropout': 0.18156862119423717, 'learning_rate': 0.0025297901087907022, 'batch_size': 32, 'weight_decay': 1.1290921652220012e-06}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:13,955] Trial 21 finished with value: 0.8523172092182509 and parameters: {'hidden_dim': 49, 'dropout': 0.3904290136968126, 'learning_rate': 0.005265328934102206, 'batch_size': 16, 'weight_decay': 1.8978753623238552e-05}. Best is trial 5 with value: 0.9087774783087283.\n",
      "[I 2025-03-20 03:04:20,819] Trial 22 finished with value: 0.9103519503259087 and parameters: {'hidden_dim': 43, 'dropout': 0.42649110230530074, 'learning_rate': 0.00132963949040977, 'batch_size': 16, 'weight_decay': 3.6891387241329175e-05}. Best is trial 22 with value: 0.9103519503259087.\n",
      "[I 2025-03-20 03:04:24,607] Trial 23 finished with value: 0.8648902216610549 and parameters: {'hidden_dim': 59, 'dropout': 0.25681776019818714, 'learning_rate': 0.0013374201487084808, 'batch_size': 16, 'weight_decay': 9.527475608374498e-05}. Best is trial 22 with value: 0.9103519503259087.\n",
      "[I 2025-03-20 03:04:25,992] Trial 24 finished with value: 0.8130240033885867 and parameters: {'hidden_dim': 45, 'dropout': 0.4153266668706781, 'learning_rate': 0.0011427880726981286, 'batch_size': 64, 'weight_decay': 7.988124851344944e-06}. Best is trial 22 with value: 0.9103519503259087.\n",
      "[I 2025-03-20 03:04:29,736] Trial 25 finished with value: 0.8935276914443581 and parameters: {'hidden_dim': 42, 'dropout': 0.31829349965704995, 'learning_rate': 0.002552635490016211, 'batch_size': 16, 'weight_decay': 3.645766319861453e-05}. Best is trial 22 with value: 0.9103519503259087.\n",
      "[I 2025-03-20 03:04:31,102] Trial 26 finished with value: 0.8201334593261677 and parameters: {'hidden_dim': 38, 'dropout': 0.18834136045019265, 'learning_rate': 0.0007046586419126022, 'batch_size': 64, 'weight_decay': 1.5156888260101681e-05}. Best is trial 22 with value: 0.9103519503259087.\n",
      "[I 2025-03-20 03:04:32,463] Trial 27 finished with value: 0.9135419701825951 and parameters: {'hidden_dim': 60, 'dropout': 0.3374763172906297, 'learning_rate': 0.0033997294158391186, 'batch_size': 64, 'weight_decay': 4.081574045055095e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:34,359] Trial 28 finished with value: 0.8513907774324441 and parameters: {'hidden_dim': 60, 'dropout': 0.4338145565622533, 'learning_rate': 0.003336333296246742, 'batch_size': 32, 'weight_decay': 1.9113847598607556e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:37,946] Trial 29 finished with value: 0.8250306551087802 and parameters: {'hidden_dim': 74, 'dropout': 0.4913476080831227, 'learning_rate': 0.006105566142656729, 'batch_size': 16, 'weight_decay': 3.425503815110053e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:41,773] Trial 30 finished with value: 0.882620779105154 and parameters: {'hidden_dim': 53, 'dropout': 0.44626966017007064, 'learning_rate': 0.0012908757959236327, 'batch_size': 16, 'weight_decay': 7.79052918224446e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:43,259] Trial 31 finished with value: 0.8488529121341623 and parameters: {'hidden_dim': 64, 'dropout': 0.3392775482589103, 'learning_rate': 0.0020558929841084897, 'batch_size': 64, 'weight_decay': 3.661398970924771e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:44,605] Trial 32 finished with value: 0.8779572231395149 and parameters: {'hidden_dim': 77, 'dropout': 0.3832965152030622, 'learning_rate': 0.003304191450015509, 'batch_size': 64, 'weight_decay': 4.006072892268249e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:45,872] Trial 33 finished with value: 0.8870735938444273 and parameters: {'hidden_dim': 39, 'dropout': 0.2991594311452422, 'learning_rate': 0.0014909196289027654, 'batch_size': 64, 'weight_decay': 6.693895075357755e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:47,203] Trial 34 finished with value: 0.9102537797850297 and parameters: {'hidden_dim': 88, 'dropout': 0.2619023211904471, 'learning_rate': 0.0023230585981956096, 'batch_size': 64, 'weight_decay': 2.2491105744829932e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:49,120] Trial 35 finished with value: 0.8736105715272382 and parameters: {'hidden_dim': 89, 'dropout': 0.25159243666676706, 'learning_rate': 0.0037061034280032237, 'batch_size': 32, 'weight_decay': 2.651253444642282e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:55,905] Trial 36 finished with value: 0.8482168439199689 and parameters: {'hidden_dim': 98, 'dropout': 0.20876301043117906, 'learning_rate': 0.0042411615710725575, 'batch_size': 16, 'weight_decay': 2.0329341442470865e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:57,234] Trial 37 finished with value: 0.8387131868902703 and parameters: {'hidden_dim': 106, 'dropout': 0.47447281896616345, 'learning_rate': 0.0010222687667330171, 'batch_size': 64, 'weight_decay': 1.0561186025576705e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:04:59,069] Trial 38 finished with value: 0.8801489791073124 and parameters: {'hidden_dim': 49, 'dropout': 0.3022693214364958, 'learning_rate': 0.0026634673676071024, 'batch_size': 32, 'weight_decay': 1.5163299137417412e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:02,635] Trial 39 finished with value: 0.8015815203315203 and parameters: {'hidden_dim': 86, 'dropout': 0.334871011699652, 'learning_rate': 0.0002650263504785184, 'batch_size': 16, 'weight_decay': 1.4284255767055812e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:03,943] Trial 40 finished with value: 0.8568864729020979 and parameters: {'hidden_dim': 119, 'dropout': 0.14886626780745005, 'learning_rate': 0.007186323415869851, 'batch_size': 64, 'weight_decay': 2.6484189090236794e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:05,232] Trial 41 finished with value: 0.8961203245057412 and parameters: {'hidden_dim': 79, 'dropout': 0.2819687776929019, 'learning_rate': 0.002063994426559537, 'batch_size': 64, 'weight_decay': 2.567230496044336e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:06,511] Trial 42 finished with value: 0.8802839087995338 and parameters: {'hidden_dim': 77, 'dropout': 0.24363765569427026, 'learning_rate': 0.0018159115840847325, 'batch_size': 64, 'weight_decay': 2.314241522135545e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:07,797] Trial 43 finished with value: 0.8650633269122853 and parameters: {'hidden_dim': 106, 'dropout': 0.28036114436733867, 'learning_rate': 0.0022631718413967795, 'batch_size': 64, 'weight_decay': 2.8464548681726037e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:09,077] Trial 44 finished with value: 0.817279802566261 and parameters: {'hidden_dim': 91, 'dropout': 0.208794907436078, 'learning_rate': 0.0006387994220321746, 'batch_size': 64, 'weight_decay': 5.395025133715409e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:10,346] Trial 45 finished with value: 0.8765243244409912 and parameters: {'hidden_dim': 60, 'dropout': 0.2982806533755087, 'learning_rate': 0.005330219014262644, 'batch_size': 64, 'weight_decay': 4.600654240742409e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:11,636] Trial 46 finished with value: 0.8763474421026505 and parameters: {'hidden_dim': 83, 'dropout': 0.3539323941330506, 'learning_rate': 0.0028864313457387333, 'batch_size': 64, 'weight_decay': 1.7805680032566898e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:15,186] Trial 47 finished with value: 0.8801147829793662 and parameters: {'hidden_dim': 80, 'dropout': 0.27174054791700286, 'learning_rate': 0.0009300913287780889, 'batch_size': 16, 'weight_decay': 1.2832221283815414e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:16,474] Trial 48 finished with value: 0.8498833824354657 and parameters: {'hidden_dim': 32, 'dropout': 0.2257645687807629, 'learning_rate': 0.0017585377580497657, 'batch_size': 64, 'weight_decay': 5.925094996765664e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:17,850] Trial 49 finished with value: 0.8328858834067168 and parameters: {'hidden_dim': 72, 'dropout': 0.30871698930040264, 'learning_rate': 0.009999511468230528, 'batch_size': 64, 'weight_decay': 3.181729509806631e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:19,303] Trial 50 finished with value: 0.8533846410148493 and parameters: {'hidden_dim': 53, 'dropout': 0.374736777367904, 'learning_rate': 0.004295270910074104, 'batch_size': 64, 'weight_decay': 2.0455495762736433e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:20,698] Trial 51 finished with value: 0.8883466745445912 and parameters: {'hidden_dim': 67, 'dropout': 0.2859310803904955, 'learning_rate': 0.0019740872306234884, 'batch_size': 64, 'weight_decay': 4.208226308566398e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:22,064] Trial 52 finished with value: 0.8642447107290857 and parameters: {'hidden_dim': 64, 'dropout': 0.26697871913074167, 'learning_rate': 0.0014390402757564207, 'batch_size': 64, 'weight_decay': 9.915609810515822e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:23,305] Trial 53 finished with value: 0.9064872145601313 and parameters: {'hidden_dim': 70, 'dropout': 0.3466968968673355, 'learning_rate': 0.0030202303363159027, 'batch_size': 64, 'weight_decay': 5.240445279994774e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:27,686] Trial 54 finished with value: 0.8790552088729172 and parameters: {'hidden_dim': 75, 'dropout': 0.34628470741387607, 'learning_rate': 0.0029633958054440736, 'batch_size': 64, 'weight_decay': 5.194185874727245e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:29,020] Trial 55 finished with value: 0.8718580031080032 and parameters: {'hidden_dim': 70, 'dropout': 0.4019878829091252, 'learning_rate': 0.002270433708929638, 'batch_size': 64, 'weight_decay': 6.591108079174895e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:31,034] Trial 56 finished with value: 0.8413948512906847 and parameters: {'hidden_dim': 58, 'dropout': 0.42284134212300384, 'learning_rate': 0.004043419635656621, 'batch_size': 32, 'weight_decay': 1.2430945891763684e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:32,452] Trial 57 finished with value: 0.864859128140378 and parameters: {'hidden_dim': 87, 'dropout': 0.3230535020938028, 'learning_rate': 0.0016404687689011317, 'batch_size': 64, 'weight_decay': 1.710829066154596e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:36,382] Trial 58 finished with value: 0.8718607684753518 and parameters: {'hidden_dim': 63, 'dropout': 0.24371182572736302, 'learning_rate': 0.0011732972240409746, 'batch_size': 16, 'weight_decay': 3.5581968503099347e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:37,771] Trial 59 finished with value: 0.8521132127902962 and parameters: {'hidden_dim': 93, 'dropout': 0.3687914560185438, 'learning_rate': 0.004936516553617544, 'batch_size': 64, 'weight_decay': 9.254339217625982e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:39,185] Trial 60 finished with value: 0.8799847432659932 and parameters: {'hidden_dim': 99, 'dropout': 0.45741509232994393, 'learning_rate': 0.0036434639883787968, 'batch_size': 64, 'weight_decay': 2.35061021713612e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:40,563] Trial 61 finished with value: 0.8853377525252525 and parameters: {'hidden_dim': 68, 'dropout': 0.29005701989970883, 'learning_rate': 0.0024205960656229234, 'batch_size': 64, 'weight_decay': 4.914249488985338e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:41,946] Trial 62 finished with value: 0.9077897363053613 and parameters: {'hidden_dim': 81, 'dropout': 0.2749514358290519, 'learning_rate': 0.003037018100846917, 'batch_size': 64, 'weight_decay': 7.3609643458204e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:43,325] Trial 63 finished with value: 0.888278383460675 and parameters: {'hidden_dim': 81, 'dropout': 0.2611869257186974, 'learning_rate': 0.002865821949539897, 'batch_size': 64, 'weight_decay': 1.1565970672987816e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:44,687] Trial 64 finished with value: 0.8976659287857204 and parameters: {'hidden_dim': 78, 'dropout': 0.32902534026769514, 'learning_rate': 0.0031747785589767486, 'batch_size': 64, 'weight_decay': 7.79579939863857e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:48,480] Trial 65 finished with value: 0.8563681351441769 and parameters: {'hidden_dim': 84, 'dropout': 0.320174967781976, 'learning_rate': 0.005745977988212841, 'batch_size': 16, 'weight_decay': 7.238952593950299e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:49,894] Trial 66 finished with value: 0.8878903214840715 and parameters: {'hidden_dim': 46, 'dropout': 0.33114183615661286, 'learning_rate': 0.0031839603366047822, 'batch_size': 64, 'weight_decay': 5.8898053814414124e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:51,308] Trial 67 finished with value: 0.8661771629219546 and parameters: {'hidden_dim': 72, 'dropout': 0.3515891693920028, 'learning_rate': 0.006888923302789752, 'batch_size': 64, 'weight_decay': 8.445467988827784e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:53,350] Trial 68 finished with value: 0.8873572800656134 and parameters: {'hidden_dim': 51, 'dropout': 0.3093338900876337, 'learning_rate': 0.00814536302174546, 'batch_size': 32, 'weight_decay': 2.309980450927176e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:05:54,736] Trial 69 finished with value: 0.8976377018043685 and parameters: {'hidden_dim': 62, 'dropout': 0.47125471066173363, 'learning_rate': 0.0045419073459525465, 'batch_size': 64, 'weight_decay': 1.0487506393455639e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:01,617] Trial 70 finished with value: 0.8118713510640593 and parameters: {'hidden_dim': 55, 'dropout': 0.3858129402070648, 'learning_rate': 0.0037117477229718187, 'batch_size': 16, 'weight_decay': 6.605068003740393e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:02,987] Trial 71 finished with value: 0.8562027864111198 and parameters: {'hidden_dim': 57, 'dropout': 0.4967043022784297, 'learning_rate': 0.004550641476410134, 'batch_size': 64, 'weight_decay': 1.0526813187072667e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:04,434] Trial 72 finished with value: 0.8883520029353363 and parameters: {'hidden_dim': 62, 'dropout': 0.4394353826396585, 'learning_rate': 0.00333006535032319, 'batch_size': 64, 'weight_decay': 5.8082780729957325e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:05,921] Trial 73 finished with value: 0.8933189399335233 and parameters: {'hidden_dim': 66, 'dropout': 0.3412384000338289, 'learning_rate': 0.003900324785511384, 'batch_size': 64, 'weight_decay': 1.6066957777260342e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:07,386] Trial 74 finished with value: 0.8653368284878701 and parameters: {'hidden_dim': 76, 'dropout': 0.4797475563206075, 'learning_rate': 0.0026556101173575955, 'batch_size': 64, 'weight_decay': 8.863982190379587e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:08,837] Trial 75 finished with value: 0.8718223568483986 and parameters: {'hidden_dim': 70, 'dropout': 0.3647985769846153, 'learning_rate': 0.0052542676233933985, 'batch_size': 64, 'weight_decay': 4.207361668013801e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:10,299] Trial 76 finished with value: 0.8646868997129413 and parameters: {'hidden_dim': 87, 'dropout': 0.4766081819034042, 'learning_rate': 0.0022854870409520717, 'batch_size': 64, 'weight_decay': 7.469022084442012e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:11,690] Trial 77 finished with value: 0.8935886644219978 and parameters: {'hidden_dim': 43, 'dropout': 0.39631459266997876, 'learning_rate': 0.006239966133964507, 'batch_size': 64, 'weight_decay': 1.282298099884592e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:15,517] Trial 78 finished with value: 0.8683162082901665 and parameters: {'hidden_dim': 78, 'dropout': 0.4705597039350056, 'learning_rate': 0.002987012389292693, 'batch_size': 16, 'weight_decay': 8.93670419572201e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:16,980] Trial 79 finished with value: 0.8431180461128379 and parameters: {'hidden_dim': 37, 'dropout': 0.4239914879426934, 'learning_rate': 0.004638512040773063, 'batch_size': 64, 'weight_decay': 5.226400344565309e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:18,413] Trial 80 finished with value: 0.7021484880859882 and parameters: {'hidden_dim': 74, 'dropout': 0.2914948197357245, 'learning_rate': 0.00010789015530993773, 'batch_size': 64, 'weight_decay': 3.08211976570485e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:19,888] Trial 81 finished with value: 0.8731616377449711 and parameters: {'hidden_dim': 90, 'dropout': 0.27240945387961946, 'learning_rate': 0.002046899352983409, 'batch_size': 64, 'weight_decay': 3.1440216671433436e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:21,481] Trial 82 finished with value: 0.8530595417314167 and parameters: {'hidden_dim': 80, 'dropout': 0.24879920194816588, 'learning_rate': 0.0015954386651549387, 'batch_size': 64, 'weight_decay': 2.7458009456168915e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:22,814] Trial 83 finished with value: 0.8722055625701458 and parameters: {'hidden_dim': 83, 'dropout': 0.30780084311530415, 'learning_rate': 0.0025170970596274105, 'batch_size': 64, 'weight_decay': 3.94407842668803e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:24,110] Trial 84 finished with value: 0.8538102040706207 and parameters: {'hidden_dim': 61, 'dropout': 0.2783212112884212, 'learning_rate': 0.0018581625741091211, 'batch_size': 64, 'weight_decay': 1.525572390929018e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:25,982] Trial 85 finished with value: 0.8805268564383147 and parameters: {'hidden_dim': 72, 'dropout': 0.25744766866292246, 'learning_rate': 0.0033688525191153252, 'batch_size': 32, 'weight_decay': 4.322433695763931e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:27,318] Trial 86 finished with value: 0.8456822498489166 and parameters: {'hidden_dim': 66, 'dropout': 0.32944636956248174, 'learning_rate': 0.002141544358629523, 'batch_size': 64, 'weight_decay': 5.905901451755845e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:28,635] Trial 87 finished with value: 0.8553736483423983 and parameters: {'hidden_dim': 80, 'dropout': 0.29206923275865393, 'learning_rate': 0.0013299628384076737, 'batch_size': 64, 'weight_decay': 1.7429573078845426e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:35,263] Trial 88 finished with value: 0.8722674123715791 and parameters: {'hidden_dim': 86, 'dropout': 0.22548882538143938, 'learning_rate': 0.0027464694603694484, 'batch_size': 16, 'weight_decay': 4.486831654311872e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:36,619] Trial 89 finished with value: 0.8754899758806008 and parameters: {'hidden_dim': 93, 'dropout': 0.2348028181684943, 'learning_rate': 0.0011120954999349348, 'batch_size': 64, 'weight_decay': 9.99044753531142e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:38,022] Trial 90 finished with value: 0.8902726854549772 and parameters: {'hidden_dim': 57, 'dropout': 0.4483670203939159, 'learning_rate': 0.003614647141317436, 'batch_size': 64, 'weight_decay': 2.5333629338174616e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:39,408] Trial 91 finished with value: 0.8926676621989124 and parameters: {'hidden_dim': 69, 'dropout': 0.27731455780511755, 'learning_rate': 0.004195719547846613, 'batch_size': 64, 'weight_decay': 3.5211511807599247e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:40,862] Trial 92 finished with value: 0.8900576275576276 and parameters: {'hidden_dim': 75, 'dropout': 0.2614960369372383, 'learning_rate': 0.0024887195055301185, 'batch_size': 64, 'weight_decay': 6.5657987667640685e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:42,345] Trial 93 finished with value: 0.8938031827354743 and parameters: {'hidden_dim': 49, 'dropout': 0.3127492890563076, 'learning_rate': 0.0030648033802700516, 'batch_size': 64, 'weight_decay': 2.1523878062138262e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:43,761] Trial 94 finished with value: 0.8671277073620823 and parameters: {'hidden_dim': 66, 'dropout': 0.3013198992441491, 'learning_rate': 0.0015101530320475974, 'batch_size': 64, 'weight_decay': 5.4197436345758245e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:45,076] Trial 95 finished with value: 0.8456380039713371 and parameters: {'hidden_dim': 77, 'dropout': 0.28524899923455904, 'learning_rate': 0.0007906871088171522, 'batch_size': 64, 'weight_decay': 8.085753467518395e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:46,416] Trial 96 finished with value: 0.8465528009798843 and parameters: {'hidden_dim': 72, 'dropout': 0.32075827560945336, 'learning_rate': 0.0022897774672219155, 'batch_size': 64, 'weight_decay': 1.956372626292333e-05}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:50,135] Trial 97 finished with value: 0.8936332812634896 and parameters: {'hidden_dim': 53, 'dropout': 0.37580954054455673, 'learning_rate': 0.0018728357102175928, 'batch_size': 16, 'weight_decay': 4.6089049019807235e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:51,516] Trial 98 finished with value: 0.8650031295864627 and parameters: {'hidden_dim': 63, 'dropout': 0.23657376445228237, 'learning_rate': 0.0027194643578676387, 'batch_size': 64, 'weight_decay': 2.998438120147414e-06}. Best is trial 27 with value: 0.9135419701825951.\n",
      "[I 2025-03-20 03:06:53,517] Trial 99 finished with value: 0.9201578754964171 and parameters: {'hidden_dim': 82, 'dropout': 0.29492693921665286, 'learning_rate': 0.0034675890750654175, 'batch_size': 32, 'weight_decay': 1.4424846262497513e-05}. Best is trial 99 with value: 0.9201578754964171.\n",
      "[I 2025-03-20 03:06:53,530] A new study created in memory with name: no-name-7497baef-02d9-4778-99d8-c477aa5975d7\n",
      "[I 2025-03-20 03:06:53,676] Trial 0 finished with value: 0.7295625856271017 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.013661178428178952, 'n_estimators': 456, 'num_leaves': 48, 'max_depth': 6, 'subsample': 0.7552942184455931, 'colsample_bytree': 0.7030385741204636, 'reg_alpha': 1.3087232248047334, 'reg_lambda': 0.6869757398160233, 'min_child_samples': 49}. Best is trial 0 with value: 0.7295625856271017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LSTM Parameters:\n",
      "  hidden_dim: 82\n",
      "  dropout: 0.29492693921665286\n",
      "  learning_rate: 0.0034675890750654175\n",
      "  batch_size: 32\n",
      "  weight_decay: 1.4424846262497513e-05\n",
      "\n",
      "Optimizing LightGBM for fold 2 using min_max normalization\n",
      "Optimizing LightGBM hyperparameters with CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:06:53,763] Trial 1 finished with value: 0.7651905297597285 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06323892443630524, 'n_estimators': 282, 'num_leaves': 42, 'max_depth': 3, 'subsample': 0.9077831487751247, 'colsample_bytree': 0.6012684467021177, 'reg_alpha': 0.31439409038438626, 'reg_lambda': 1.0912521853098582, 'min_child_samples': 47}. Best is trial 1 with value: 0.7651905297597285.\n",
      "[I 2025-03-20 03:06:53,855] Trial 2 finished with value: 0.757862830572508 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.019957907467756848, 'n_estimators': 199, 'num_leaves': 37, 'max_depth': 10, 'subsample': 0.7873081624297713, 'colsample_bytree': 0.6493149518105418, 'reg_alpha': 3.6000609130029044, 'reg_lambda': 4.050134848257446, 'min_child_samples': 21}. Best is trial 1 with value: 0.7651905297597285.\n",
      "[I 2025-03-20 03:06:53,926] Trial 3 finished with value: 0.7859902142368324 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03308671790989007, 'n_estimators': 162, 'num_leaves': 10, 'max_depth': 5, 'subsample': 0.9963593754983149, 'colsample_bytree': 0.805256515122397, 'reg_alpha': 3.0841806167907238, 'reg_lambda': 0.1301677362213169, 'min_child_samples': 21}. Best is trial 3 with value: 0.7859902142368324.\n",
      "[I 2025-03-20 03:06:54,059] Trial 4 finished with value: 0.7717277918620271 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.029437387715603364, 'n_estimators': 405, 'num_leaves': 47, 'max_depth': 6, 'subsample': 0.9716350708762611, 'colsample_bytree': 0.7998448041733087, 'reg_alpha': 2.304933318498776, 'reg_lambda': 3.3362817514757888, 'min_child_samples': 29}. Best is trial 3 with value: 0.7859902142368324.\n",
      "[I 2025-03-20 03:06:54,111] Trial 5 finished with value: 0.7594885448850069 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07045793365915588, 'n_estimators': 210, 'num_leaves': 23, 'max_depth': 3, 'subsample': 0.8092590874861125, 'colsample_bytree': 0.859403208540551, 'reg_alpha': 3.3760828530802196, 'reg_lambda': 0.15012918259918107, 'min_child_samples': 22}. Best is trial 3 with value: 0.7859902142368324.\n",
      "[I 2025-03-20 03:06:54,261] Trial 6 finished with value: 0.7947178561049529 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011970252710206382, 'n_estimators': 476, 'num_leaves': 49, 'max_depth': 4, 'subsample': 0.9816084577717837, 'colsample_bytree': 0.7776193095921519, 'reg_alpha': 0.2802529145561249, 'reg_lambda': 1.3744727196612017, 'min_child_samples': 25}. Best is trial 6 with value: 0.7947178561049529.\n",
      "[I 2025-03-20 03:06:54,370] Trial 7 finished with value: 0.8014488808713575 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0967088301559984, 'n_estimators': 367, 'num_leaves': 50, 'max_depth': 10, 'subsample': 0.6725825297010686, 'colsample_bytree': 0.7765487989776441, 'reg_alpha': 1.3901311581414446, 'reg_lambda': 1.2894120399825089, 'min_child_samples': 25}. Best is trial 7 with value: 0.8014488808713575.\n",
      "[I 2025-03-20 03:06:54,475] Trial 8 finished with value: 0.7774135827517722 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04669085237595252, 'n_estimators': 388, 'num_leaves': 42, 'max_depth': 5, 'subsample': 0.6388242673991037, 'colsample_bytree': 0.9337670455562896, 'reg_alpha': 0.11037032003620384, 'reg_lambda': 0.25448376172072, 'min_child_samples': 45}. Best is trial 7 with value: 0.8014488808713575.\n",
      "[I 2025-03-20 03:06:54,560] Trial 9 finished with value: 0.7769623823317892 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011232085595448875, 'n_estimators': 177, 'num_leaves': 29, 'max_depth': 4, 'subsample': 0.7436406503252848, 'colsample_bytree': 0.6580654068535353, 'reg_alpha': 0.1699984374960256, 'reg_lambda': 0.1466016632681982, 'min_child_samples': 16}. Best is trial 7 with value: 0.8014488808713575.\n",
      "[I 2025-03-20 03:06:54,675] Trial 10 finished with value: 0.7998458544764477 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0987558719383339, 'n_estimators': 337, 'num_leaves': 18, 'max_depth': 10, 'subsample': 0.642242017563948, 'colsample_bytree': 0.9614634142305828, 'reg_alpha': 0.7548915517607346, 'reg_lambda': 0.47191940414066896, 'min_child_samples': 37}. Best is trial 7 with value: 0.8014488808713575.\n",
      "[I 2025-03-20 03:06:54,788] Trial 11 finished with value: 0.7991249542987316 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08811492902543544, 'n_estimators': 327, 'num_leaves': 17, 'max_depth': 10, 'subsample': 0.6019793854602531, 'colsample_bytree': 0.993860589036455, 'reg_alpha': 0.7915653922317443, 'reg_lambda': 0.4908802288038389, 'min_child_samples': 38}. Best is trial 7 with value: 0.8014488808713575.\n",
      "[I 2025-03-20 03:06:54,901] Trial 12 finished with value: 0.8008894678832243 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09822003027572318, 'n_estimators': 307, 'num_leaves': 30, 'max_depth': 8, 'subsample': 0.6801173039155465, 'colsample_bytree': 0.893626879385864, 'reg_alpha': 0.8750718436454238, 'reg_lambda': 1.8653713281408886, 'min_child_samples': 37}. Best is trial 7 with value: 0.8014488808713575.\n",
      "[I 2025-03-20 03:06:55,072] Trial 13 finished with value: 0.8155713816983328 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05345803972760133, 'n_estimators': 280, 'num_leaves': 30, 'max_depth': 8, 'subsample': 0.6744176335855437, 'colsample_bytree': 0.8784839768898399, 'reg_alpha': 1.4639178782106737, 'reg_lambda': 2.0673304537938133, 'min_child_samples': 10}. Best is trial 13 with value: 0.8155713816983328.\n",
      "[I 2025-03-20 03:06:55,173] Trial 14 finished with value: 0.8197127723829076 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0514136988240323, 'n_estimators': 106, 'num_leaves': 34, 'max_depth': 8, 'subsample': 0.7003547686087039, 'colsample_bytree': 0.7567506115224731, 'reg_alpha': 1.6036537578041012, 'reg_lambda': 2.327760318505606, 'min_child_samples': 10}. Best is trial 14 with value: 0.8197127723829076.\n",
      "[I 2025-03-20 03:06:55,264] Trial 15 finished with value: 0.817935288343197 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.043288657420412956, 'n_estimators': 102, 'num_leaves': 35, 'max_depth': 8, 'subsample': 0.7162209016787783, 'colsample_bytree': 0.8497272335544289, 'reg_alpha': 1.7337714184739639, 'reg_lambda': 2.4129838631039044, 'min_child_samples': 13}. Best is trial 14 with value: 0.8197127723829076.\n",
      "[I 2025-03-20 03:06:55,335] Trial 16 finished with value: 0.7542746401820283 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04109638618124266, 'n_estimators': 103, 'num_leaves': 38, 'max_depth': 8, 'subsample': 0.8460344390937014, 'colsample_bytree': 0.7332836568252892, 'reg_alpha': 4.970526176538261, 'reg_lambda': 2.6533031074045117, 'min_child_samples': 10}. Best is trial 14 with value: 0.8197127723829076.\n",
      "[I 2025-03-20 03:06:55,433] Trial 17 finished with value: 0.8035156683366882 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.024514901988786787, 'n_estimators': 125, 'num_leaves': 35, 'max_depth': 7, 'subsample': 0.7196804203919662, 'colsample_bytree': 0.8514824173376508, 'reg_alpha': 2.0059984359337593, 'reg_lambda': 3.813415674909581, 'min_child_samples': 15}. Best is trial 14 with value: 0.8197127723829076.\n",
      "[I 2025-03-20 03:06:55,600] Trial 18 finished with value: 0.8299930145694974 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.037144471324082946, 'n_estimators': 227, 'num_leaves': 24, 'max_depth': 9, 'subsample': 0.7137351539393999, 'colsample_bytree': 0.7287996519197543, 'reg_alpha': 0.45729733448074894, 'reg_lambda': 4.824176851953528, 'min_child_samples': 15}. Best is trial 18 with value: 0.8299930145694974.\n",
      "[I 2025-03-20 03:06:55,745] Trial 19 finished with value: 0.8106451157564581 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.019113721561322468, 'n_estimators': 241, 'num_leaves': 25, 'max_depth': 9, 'subsample': 0.7941895058442429, 'colsample_bytree': 0.7222208280865605, 'reg_alpha': 0.42706293782677895, 'reg_lambda': 4.255688968928355, 'min_child_samples': 17}. Best is trial 18 with value: 0.8299930145694974.\n",
      "[I 2025-03-20 03:06:55,850] Trial 20 finished with value: 0.7768998442473989 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03448357662735862, 'n_estimators': 253, 'num_leaves': 24, 'max_depth': 9, 'subsample': 0.8662068662276897, 'colsample_bytree': 0.683699001860462, 'reg_alpha': 0.4785139738442375, 'reg_lambda': 4.815902159571185, 'min_child_samples': 31}. Best is trial 18 with value: 0.8299930145694974.\n",
      "[I 2025-03-20 03:06:55,959] Trial 21 finished with value: 0.8302535205043009 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04796891181125232, 'n_estimators': 140, 'num_leaves': 34, 'max_depth': 7, 'subsample': 0.7177198989806286, 'colsample_bytree': 0.7476579973236518, 'reg_alpha': 0.47030520300856754, 'reg_lambda': 2.402411476912158, 'min_child_samples': 12}. Best is trial 21 with value: 0.8302535205043009.\n",
      "[I 2025-03-20 03:06:56,071] Trial 22 finished with value: 0.8366355496272249 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05691393488808102, 'n_estimators': 147, 'num_leaves': 33, 'max_depth': 7, 'subsample': 0.7140868333158419, 'colsample_bytree': 0.7621521881566689, 'reg_alpha': 0.5058214915040057, 'reg_lambda': 1.7260415524945665, 'min_child_samples': 13}. Best is trial 22 with value: 0.8366355496272249.\n",
      "[I 2025-03-20 03:06:56,170] Trial 23 finished with value: 0.8258412019868835 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06550018786401357, 'n_estimators': 159, 'num_leaves': 27, 'max_depth': 7, 'subsample': 0.7738830569369699, 'colsample_bytree': 0.7448554954626562, 'reg_alpha': 0.457258697678428, 'reg_lambda': 1.675137388227885, 'min_child_samples': 18}. Best is trial 22 with value: 0.8366355496272249.\n",
      "[I 2025-03-20 03:06:56,277] Trial 24 finished with value: 0.8329716578478286 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03914221839101109, 'n_estimators': 141, 'num_leaves': 21, 'max_depth': 7, 'subsample': 0.7396756451743561, 'colsample_bytree': 0.8125433826805081, 'reg_alpha': 0.25884478241560555, 'reg_lambda': 0.8227568369200018, 'min_child_samples': 14}. Best is trial 22 with value: 0.8366355496272249.\n",
      "[I 2025-03-20 03:06:56,383] Trial 25 finished with value: 0.8351342512424719 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.056539182308454124, 'n_estimators': 135, 'num_leaves': 19, 'max_depth': 7, 'subsample': 0.7423397846032438, 'colsample_bytree': 0.8172853988994906, 'reg_alpha': 0.22351524545390877, 'reg_lambda': 0.9688851193203756, 'min_child_samples': 13}. Best is trial 22 with value: 0.8366355496272249.\n",
      "[I 2025-03-20 03:06:56,489] Trial 26 finished with value: 0.8273078946346375 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0771483911440346, 'n_estimators': 183, 'num_leaves': 18, 'max_depth': 6, 'subsample': 0.8411766197382218, 'colsample_bytree': 0.8211475835242629, 'reg_alpha': 0.1981151186970793, 'reg_lambda': 0.9786428051144711, 'min_child_samples': 19}. Best is trial 22 with value: 0.8366355496272249.\n",
      "[I 2025-03-20 03:06:56,592] Trial 27 finished with value: 0.840325474724018 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05883480565766994, 'n_estimators': 135, 'num_leaves': 13, 'max_depth': 7, 'subsample': 0.8122806124009707, 'colsample_bytree': 0.8267484591369892, 'reg_alpha': 0.15808811854735966, 'reg_lambda': 0.6724340412993158, 'min_child_samples': 13}. Best is trial 27 with value: 0.840325474724018.\n",
      "[I 2025-03-20 03:06:56,670] Trial 28 finished with value: 0.8070605680751364 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05704498744164297, 'n_estimators': 138, 'num_leaves': 10, 'max_depth': 5, 'subsample': 0.8804686058791398, 'colsample_bytree': 0.910423244744483, 'reg_alpha': 0.1028504294924159, 'reg_lambda': 0.4697727481183875, 'min_child_samples': 25}. Best is trial 27 with value: 0.840325474724018.\n",
      "[I 2025-03-20 03:06:56,788] Trial 29 finished with value: 0.8308681325892564 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0772710694737384, 'n_estimators': 205, 'num_leaves': 14, 'max_depth': 6, 'subsample': 0.8154322758311401, 'colsample_bytree': 0.7032288894084813, 'reg_alpha': 0.14717468711068732, 'reg_lambda': 0.6440961910927886, 'min_child_samples': 19}. Best is trial 27 with value: 0.840325474724018.\n",
      "[I 2025-03-20 03:06:56,871] Trial 30 finished with value: 0.800388437344733 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05921540648291128, 'n_estimators': 166, 'num_leaves': 13, 'max_depth': 7, 'subsample': 0.7697720387073888, 'colsample_bytree': 0.8374681760700159, 'reg_alpha': 0.20417594048667156, 'reg_lambda': 0.24582514361630925, 'min_child_samples': 30}. Best is trial 27 with value: 0.840325474724018.\n",
      "[I 2025-03-20 03:06:56,982] Trial 31 finished with value: 0.8367302359591641 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04050585260407289, 'n_estimators': 131, 'num_leaves': 21, 'max_depth': 7, 'subsample': 0.7586781148255216, 'colsample_bytree': 0.7803628869865951, 'reg_alpha': 0.28549842694867744, 'reg_lambda': 0.7964644882985981, 'min_child_samples': 13}. Best is trial 27 with value: 0.840325474724018.\n",
      "[I 2025-03-20 03:06:57,092] Trial 32 finished with value: 0.8378345406503576 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06804673650216124, 'n_estimators': 126, 'num_leaves': 15, 'max_depth': 6, 'subsample': 0.7577581168119656, 'colsample_bytree': 0.7790211478702708, 'reg_alpha': 0.1311991840883291, 'reg_lambda': 0.6351819948699057, 'min_child_samples': 13}. Best is trial 27 with value: 0.840325474724018.\n",
      "[I 2025-03-20 03:06:57,236] Trial 33 finished with value: 0.8426023125846227 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.028356052139604872, 'n_estimators': 194, 'num_leaves': 14, 'max_depth': 6, 'subsample': 0.823933313575341, 'colsample_bytree': 0.7837185366763677, 'reg_alpha': 0.13443395035965247, 'reg_lambda': 0.5956602999915758, 'min_child_samples': 12}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:57,381] Trial 34 finished with value: 0.8344264969592754 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.027069572527092703, 'n_estimators': 191, 'num_leaves': 14, 'max_depth': 6, 'subsample': 0.9350814517375146, 'colsample_bytree': 0.792364239675844, 'reg_alpha': 0.13720552707897762, 'reg_lambda': 0.630644245752737, 'min_child_samples': 11}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:57,468] Trial 35 finished with value: 0.7775137304689854 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.02163655073657856, 'n_estimators': 119, 'num_leaves': 12, 'max_depth': 5, 'subsample': 0.8168359013496314, 'colsample_bytree': 0.7738166113326118, 'reg_alpha': 0.3438486054853424, 'reg_lambda': 0.31558636903333975, 'min_child_samples': 22}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:57,588] Trial 36 finished with value: 0.821167701222852 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03236031358922, 'n_estimators': 168, 'num_leaves': 16, 'max_depth': 6, 'subsample': 0.9013524233332678, 'colsample_bytree': 0.7943163752819814, 'reg_alpha': 0.13612571568045906, 'reg_lambda': 0.339806657674805, 'min_child_samples': 18}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:57,712] Trial 37 finished with value: 0.804363701742474 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0181752568574581, 'n_estimators': 218, 'num_leaves': 21, 'max_depth': 6, 'subsample': 0.8295544333641782, 'colsample_bytree': 0.7013004667386881, 'reg_alpha': 0.17010714258241172, 'reg_lambda': 0.7424780574021025, 'min_child_samples': 20}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:57,824] Trial 38 finished with value: 0.8229175488852908 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.02935514360029792, 'n_estimators': 193, 'num_leaves': 10, 'max_depth': 5, 'subsample': 0.7777291916003832, 'colsample_bytree': 0.8708838298126357, 'reg_alpha': 0.11886719888274751, 'reg_lambda': 0.5653934969963305, 'min_child_samples': 16}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:57,987] Trial 39 finished with value: 0.7970536897831382 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.015805522226546066, 'n_estimators': 434, 'num_leaves': 16, 'max_depth': 4, 'subsample': 0.7932744940366798, 'colsample_bytree': 0.60951827827375, 'reg_alpha': 0.33982390179046523, 'reg_lambda': 1.2043220829573837, 'min_child_samples': 23}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,091] Trial 40 finished with value: 0.8021642875222479 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0689166449071245, 'n_estimators': 262, 'num_leaves': 20, 'max_depth': 6, 'subsample': 0.7625785768668903, 'colsample_bytree': 0.8411623217148714, 'reg_alpha': 0.250222669328795, 'reg_lambda': 0.9116841597434181, 'min_child_samples': 34}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,204] Trial 41 finished with value: 0.8283432289134683 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0802105216030138, 'n_estimators': 150, 'num_leaves': 15, 'max_depth': 7, 'subsample': 0.8568114086015312, 'colsample_bytree': 0.7631614379939149, 'reg_alpha': 0.9934455312924253, 'reg_lambda': 1.5610258916261666, 'min_child_samples': 13}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,302] Trial 42 finished with value: 0.8332569355441363 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04545704229114224, 'n_estimators': 127, 'num_leaves': 12, 'max_depth': 6, 'subsample': 0.7501158617542235, 'colsample_bytree': 0.7877070490284679, 'reg_alpha': 0.17788684444652836, 'reg_lambda': 0.3894359791634076, 'min_child_samples': 14}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,370] Trial 43 finished with value: 0.7585899595150376 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06418443841981039, 'n_estimators': 157, 'num_leaves': 41, 'max_depth': 7, 'subsample': 0.8042442353741162, 'colsample_bytree': 0.8306076910954077, 'reg_alpha': 0.6210278503995007, 'reg_lambda': 0.7582498146806043, 'min_child_samples': 50}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,433] Trial 44 finished with value: 0.7530541960125727 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05152031324712323, 'n_estimators': 118, 'num_leaves': 22, 'max_depth': 5, 'subsample': 0.7578389877390594, 'colsample_bytree': 0.8071349539523239, 'reg_alpha': 0.30705442606352856, 'reg_lambda': 0.5498324391174616, 'min_child_samples': 45}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,575] Trial 45 finished with value: 0.8332372475046773 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03638484615704035, 'n_estimators': 176, 'num_leaves': 28, 'max_depth': 8, 'subsample': 0.8260083060345035, 'colsample_bytree': 0.7743384407266947, 'reg_alpha': 0.1179022872866858, 'reg_lambda': 1.0800294677519784, 'min_child_samples': 12}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,657] Trial 46 finished with value: 0.7780940421616801 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.023805185234108378, 'n_estimators': 151, 'num_leaves': 12, 'max_depth': 6, 'subsample': 0.6948758947196713, 'colsample_bytree': 0.7091557397947391, 'reg_alpha': 0.5905353068432418, 'reg_lambda': 0.22000142450849106, 'min_child_samples': 27}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,743] Trial 47 finished with value: 0.8204215659095991 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.02960070095834647, 'n_estimators': 100, 'num_leaves': 31, 'max_depth': 7, 'subsample': 0.6381131886829216, 'colsample_bytree': 0.7534966957842459, 'reg_alpha': 0.15701026819768543, 'reg_lambda': 0.4137166748707009, 'min_child_samples': 16}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,852] Trial 48 finished with value: 0.8307392275581662 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08654742671541144, 'n_estimators': 225, 'num_leaves': 32, 'max_depth': 8, 'subsample': 0.7340174651679425, 'colsample_bytree': 0.7710947174152837, 'reg_alpha': 1.0985008230952682, 'reg_lambda': 0.11123155755853965, 'min_child_samples': 11}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,952] Trial 49 finished with value: 0.8070180448026442 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07115595987045323, 'n_estimators': 205, 'num_leaves': 26, 'max_depth': 4, 'subsample': 0.7835535635130332, 'colsample_bytree': 0.7950510436414103, 'reg_alpha': 0.38788405207879256, 'reg_lambda': 1.420835487241947, 'min_child_samples': 21}. Best is trial 33 with value: 0.8426023125846227.\n",
      "[I 2025-03-20 03:06:58,985] A new study created in memory with name: no-name-e4841115-d1bc-45b7-bb93-996aa08ec497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LightGBM Parameters:\n",
      "  boosting_type: gbdt\n",
      "  learning_rate: 0.028356052139604872\n",
      "  n_estimators: 194\n",
      "  num_leaves: 14\n",
      "  max_depth: 6\n",
      "  subsample: 0.823933313575341\n",
      "  colsample_bytree: 0.7837185366763677\n",
      "  reg_alpha: 0.13443395035965247\n",
      "  reg_lambda: 0.5956602999915758\n",
      "  min_child_samples: 12\n",
      "\n",
      "Optimizing LSTM for fold 2 using min_max normalization\n",
      "Created LSTM sequences with lookback=10: 298 train, 145 test\n",
      "Optimizing LSTM hyperparameters with CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:07:10,316] Trial 0 finished with value: 0.7647054024969847 and parameters: {'hidden_dim': 45, 'dropout': 0.4551406810921512, 'learning_rate': 0.007331811314274292, 'batch_size': 16, 'weight_decay': 2.064121088592392e-05}. Best is trial 0 with value: 0.7647054024969847.\n",
      "[I 2025-03-20 03:07:14,984] Trial 1 finished with value: 0.8473310308503006 and parameters: {'hidden_dim': 79, 'dropout': 0.48569536770002286, 'learning_rate': 0.003999161563860724, 'batch_size': 32, 'weight_decay': 3.45872269711161e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:07:23,367] Trial 2 finished with value: 0.8378526476903758 and parameters: {'hidden_dim': 32, 'dropout': 0.21712375681979076, 'learning_rate': 0.0016665895921660098, 'batch_size': 16, 'weight_decay': 2.1517123363927774e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:07:27,889] Trial 3 finished with value: 0.7079241555154335 and parameters: {'hidden_dim': 127, 'dropout': 0.2873081624297713, 'learning_rate': 0.00017643094449433023, 'batch_size': 32, 'weight_decay': 1.0947309020486406e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:07:36,775] Trial 4 finished with value: 0.811948031496713 and parameters: {'hidden_dim': 47, 'dropout': 0.10585094112968818, 'learning_rate': 0.0004451295365396247, 'batch_size': 16, 'weight_decay': 1.3639281514957935e-06}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:07:41,324] Trial 5 finished with value: 0.8273933254409928 and parameters: {'hidden_dim': 59, 'dropout': 0.2875597071705984, 'learning_rate': 0.0033384615669657483, 'batch_size': 64, 'weight_decay': 9.98214837316597e-06}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:07:49,163] Trial 6 finished with value: 0.7953451701930403 and parameters: {'hidden_dim': 109, 'dropout': 0.4586320455614753, 'learning_rate': 0.0009209968619749436, 'batch_size': 16, 'weight_decay': 1.6223673683559225e-06}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:07:53,450] Trial 7 finished with value: 0.7941034990477182 and parameters: {'hidden_dim': 82, 'dropout': 0.3594032085405511, 'learning_rate': 0.006298297451196278, 'batch_size': 32, 'weight_decay': 7.511737052942637e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:07:56,057] Trial 8 finished with value: 0.7830165252071946 and parameters: {'hidden_dim': 125, 'dropout': 0.17649687394806254, 'learning_rate': 0.00809174687672524, 'batch_size': 64, 'weight_decay': 5.656354626844694e-06}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:04,119] Trial 9 finished with value: 0.7241511812505728 and parameters: {'hidden_dim': 127, 'dropout': 0.3668293205677101, 'learning_rate': 0.009614679235250567, 'batch_size': 16, 'weight_decay': 2.2160868002868673e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:08,315] Trial 10 finished with value: 0.8408351124121914 and parameters: {'hidden_dim': 81, 'dropout': 0.3850400329025248, 'learning_rate': 0.00229500940300285, 'batch_size': 32, 'weight_decay': 8.469738945067809e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:16,186] Trial 11 finished with value: 0.8338230443960667 and parameters: {'hidden_dim': 80, 'dropout': 0.4864413695393805, 'learning_rate': 0.002181551031131038, 'batch_size': 32, 'weight_decay': 7.585251739735352e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:20,616] Trial 12 finished with value: 0.796889346661152 and parameters: {'hidden_dim': 97, 'dropout': 0.39121170409630657, 'learning_rate': 0.0008592895778069696, 'batch_size': 32, 'weight_decay': 4.418290988186971e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:24,856] Trial 13 finished with value: 0.843406180627276 and parameters: {'hidden_dim': 67, 'dropout': 0.4158292438981242, 'learning_rate': 0.003399534777244384, 'batch_size': 32, 'weight_decay': 4.18012810875491e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:29,169] Trial 14 finished with value: 0.8255768281376801 and parameters: {'hidden_dim': 65, 'dropout': 0.4990559726193295, 'learning_rate': 0.004351118630391938, 'batch_size': 32, 'weight_decay': 3.821863404982552e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:33,339] Trial 15 finished with value: 0.8263998481797671 and parameters: {'hidden_dim': 69, 'dropout': 0.42580142494740547, 'learning_rate': 0.0038949815178919477, 'batch_size': 32, 'weight_decay': 3.7504371664630406e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:37,671] Trial 16 finished with value: 0.7746152587176929 and parameters: {'hidden_dim': 96, 'dropout': 0.32180814792851586, 'learning_rate': 0.00046120972793413676, 'batch_size': 32, 'weight_decay': 4.5281394266958185e-06}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:40,255] Trial 17 finished with value: 0.8202227398474863 and parameters: {'hidden_dim': 94, 'dropout': 0.4193360406518387, 'learning_rate': 0.0013451095609937336, 'batch_size': 64, 'weight_decay': 1.2590147439511592e-05}. Best is trial 1 with value: 0.8473310308503006.\n",
      "[I 2025-03-20 03:08:47,995] Trial 18 finished with value: 0.8484464018439678 and parameters: {'hidden_dim': 56, 'dropout': 0.4439213695777281, 'learning_rate': 0.0032284878090172676, 'batch_size': 32, 'weight_decay': 4.8604070414660265e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:08:52,634] Trial 19 finished with value: 0.5976696130854346 and parameters: {'hidden_dim': 52, 'dropout': 0.327392532265835, 'learning_rate': 0.00011600174401008022, 'batch_size': 32, 'weight_decay': 5.402261967089256e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:08:55,438] Trial 20 finished with value: 0.6574512961683348 and parameters: {'hidden_dim': 40, 'dropout': 0.44802071171540514, 'learning_rate': 0.0005265287236936856, 'batch_size': 64, 'weight_decay': 2.4160440148416682e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:08:59,962] Trial 21 finished with value: 0.7983204215962836 and parameters: {'hidden_dim': 68, 'dropout': 0.41776639032857393, 'learning_rate': 0.00284387422508719, 'batch_size': 32, 'weight_decay': 3.0688972445372824e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:04,557] Trial 22 finished with value: 0.8433899274416516 and parameters: {'hidden_dim': 57, 'dropout': 0.47528889001440844, 'learning_rate': 0.005437707796657205, 'batch_size': 32, 'weight_decay': 5.6159876334565605e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:09,320] Trial 23 finished with value: 0.8405072314808623 and parameters: {'hidden_dim': 74, 'dropout': 0.49848175316745474, 'learning_rate': 0.0014548570208501549, 'batch_size': 32, 'weight_decay': 9.527475608374498e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:14,015] Trial 24 finished with value: 0.8206793253699948 and parameters: {'hidden_dim': 90, 'dropout': 0.4256716256791634, 'learning_rate': 0.004774143328057438, 'batch_size': 32, 'weight_decay': 1.5807599393858592e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:21,508] Trial 25 finished with value: 0.8226752991205323 and parameters: {'hidden_dim': 63, 'dropout': 0.397904162020369, 'learning_rate': 0.0026321678891999853, 'batch_size': 32, 'weight_decay': 6.152308135003427e-06}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:25,941] Trial 26 finished with value: 0.8170625220320961 and parameters: {'hidden_dim': 75, 'dropout': 0.35066510746147644, 'learning_rate': 0.001711491820145518, 'batch_size': 32, 'weight_decay': 3.273959797874742e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:30,384] Trial 27 finished with value: 0.8202156916406409 and parameters: {'hidden_dim': 53, 'dropout': 0.452324815730481, 'learning_rate': 0.0034718728681609797, 'batch_size': 32, 'weight_decay': 5.6042971628636865e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:32,990] Trial 28 finished with value: 0.7940577321408965 and parameters: {'hidden_dim': 107, 'dropout': 0.2574861476582606, 'learning_rate': 0.005781365578326072, 'batch_size': 64, 'weight_decay': 3.14011717940646e-06}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:37,316] Trial 29 finished with value: 0.7750393069105037 and parameters: {'hidden_dim': 41, 'dropout': 0.4679389748062319, 'learning_rate': 0.00966677059319195, 'batch_size': 32, 'weight_decay': 2.7557222702729444e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:45,248] Trial 30 finished with value: 0.8240410930066102 and parameters: {'hidden_dim': 86, 'dropout': 0.4410656741454405, 'learning_rate': 0.0006571980116980218, 'batch_size': 16, 'weight_decay': 1.4374740702263345e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:52,720] Trial 31 finished with value: 0.771448044292872 and parameters: {'hidden_dim': 55, 'dropout': 0.47529345195041006, 'learning_rate': 0.0056222568971966095, 'batch_size': 32, 'weight_decay': 5.619148274493082e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:09:57,125] Trial 32 finished with value: 0.7876003260131048 and parameters: {'hidden_dim': 60, 'dropout': 0.4732305381417601, 'learning_rate': 0.0071096489098099475, 'batch_size': 32, 'weight_decay': 5.376340839684544e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:10:01,431] Trial 33 finished with value: 0.829773503860725 and parameters: {'hidden_dim': 73, 'dropout': 0.40099284460719675, 'learning_rate': 0.004352987317951419, 'batch_size': 32, 'weight_decay': 1.8076931900171943e-05}. Best is trial 18 with value: 0.8484464018439678.\n",
      "[I 2025-03-20 03:10:05,755] Trial 34 finished with value: 0.859894774296397 and parameters: {'hidden_dim': 34, 'dropout': 0.45309935929808925, 'learning_rate': 0.001958545338827777, 'batch_size': 32, 'weight_decay': 7.22298639898742e-05}. Best is trial 34 with value: 0.859894774296397.\n",
      "[I 2025-03-20 03:10:13,753] Trial 35 finished with value: 0.8373252292369938 and parameters: {'hidden_dim': 36, 'dropout': 0.43872619735770246, 'learning_rate': 0.0017177067175062383, 'batch_size': 16, 'weight_decay': 4.191750395509523e-05}. Best is trial 34 with value: 0.859894774296397.\n",
      "[I 2025-03-20 03:10:18,140] Trial 36 finished with value: 0.7285601493765794 and parameters: {'hidden_dim': 32, 'dropout': 0.2546232321589963, 'learning_rate': 0.00030635383176865627, 'batch_size': 32, 'weight_decay': 6.712400079388522e-05}. Best is trial 34 with value: 0.859894774296397.\n",
      "[I 2025-03-20 03:10:25,900] Trial 37 finished with value: 0.8436362225408879 and parameters: {'hidden_dim': 46, 'dropout': 0.16589883693649712, 'learning_rate': 0.001264143156657443, 'batch_size': 32, 'weight_decay': 9.693226551747323e-05}. Best is trial 34 with value: 0.859894774296397.\n",
      "[I 2025-03-20 03:10:33,952] Trial 38 finished with value: 0.8472371751529968 and parameters: {'hidden_dim': 47, 'dropout': 0.1399216444007724, 'learning_rate': 0.001993727477152352, 'batch_size': 16, 'weight_decay': 9.238346234705445e-05}. Best is trial 34 with value: 0.859894774296397.\n",
      "[I 2025-03-20 03:10:41,854] Trial 39 finished with value: 0.8568181339733063 and parameters: {'hidden_dim': 49, 'dropout': 0.12429336082410652, 'learning_rate': 0.0021188351467055057, 'batch_size': 16, 'weight_decay': 8.033787949783035e-06}. Best is trial 34 with value: 0.859894774296397.\n",
      "[I 2025-03-20 03:10:49,678] Trial 40 finished with value: 0.8304324750825766 and parameters: {'hidden_dim': 40, 'dropout': 0.2276161279924873, 'learning_rate': 0.0010757134687448117, 'batch_size': 16, 'weight_decay': 9.16780391013495e-06}. Best is trial 34 with value: 0.859894774296397.\n",
      "[I 2025-03-20 03:11:00,898] Trial 41 finished with value: 0.8472499093949399 and parameters: {'hidden_dim': 49, 'dropout': 0.12241288984073462, 'learning_rate': 0.0018285367885149512, 'batch_size': 16, 'weight_decay': 8.156271254562448e-06}. Best is trial 34 with value: 0.859894774296397.\n",
      "[I 2025-03-20 03:11:08,964] Trial 42 finished with value: 0.8639533959767225 and parameters: {'hidden_dim': 49, 'dropout': 0.11285998363654516, 'learning_rate': 0.0028135812961768984, 'batch_size': 16, 'weight_decay': 7.503477631876396e-06}. Best is trial 42 with value: 0.8639533959767225.\n",
      "[I 2025-03-20 03:11:17,125] Trial 43 finished with value: 0.8446825355547465 and parameters: {'hidden_dim': 32, 'dropout': 0.10989966674254059, 'learning_rate': 0.0027688597595214885, 'batch_size': 16, 'weight_decay': 3.693269749092597e-06}. Best is trial 42 with value: 0.8639533959767225.\n",
      "[I 2025-03-20 03:11:25,316] Trial 44 finished with value: 0.8715368228045713 and parameters: {'hidden_dim': 43, 'dropout': 0.1626932075937906, 'learning_rate': 0.0023218804950543865, 'batch_size': 16, 'weight_decay': 2.394079281488213e-06}. Best is trial 44 with value: 0.8715368228045713.\n",
      "[I 2025-03-20 03:11:36,301] Trial 45 finished with value: 0.8355951369643053 and parameters: {'hidden_dim': 42, 'dropout': 0.16236712898351485, 'learning_rate': 0.002195686408894806, 'batch_size': 16, 'weight_decay': 2.3006820206178384e-06}. Best is trial 44 with value: 0.8715368228045713.\n",
      "[I 2025-03-20 03:11:44,064] Trial 46 finished with value: 0.8260673338005994 and parameters: {'hidden_dim': 37, 'dropout': 0.13448060846045945, 'learning_rate': 0.00279052830367328, 'batch_size': 16, 'weight_decay': 1.5557713701610666e-06}. Best is trial 44 with value: 0.8715368228045713.\n",
      "[I 2025-03-20 03:11:51,915] Trial 47 finished with value: 0.8487585249248536 and parameters: {'hidden_dim': 49, 'dropout': 0.19615280538338178, 'learning_rate': 0.001120892750944069, 'batch_size': 16, 'weight_decay': 6.258814590863016e-06}. Best is trial 44 with value: 0.8715368228045713.\n",
      "[I 2025-03-20 03:12:00,274] Trial 48 finished with value: 0.7905154767477282 and parameters: {'hidden_dim': 50, 'dropout': 0.19603730445451786, 'learning_rate': 0.0007752786686780675, 'batch_size': 16, 'weight_decay': 7.135210684956339e-06}. Best is trial 44 with value: 0.8715368228045713.\n",
      "[I 2025-03-20 03:12:11,784] Trial 49 finished with value: 0.8253045279414448 and parameters: {'hidden_dim': 46, 'dropout': 0.1001371746411079, 'learning_rate': 0.001073857276998066, 'batch_size': 16, 'weight_decay': 2.195041930350694e-06}. Best is trial 44 with value: 0.8715368228045713.\n",
      "[I 2025-03-20 03:12:19,799] Trial 50 finished with value: 0.8748025057355687 and parameters: {'hidden_dim': 36, 'dropout': 0.19050735829303594, 'learning_rate': 0.001354250340464109, 'batch_size': 16, 'weight_decay': 1.025194793488667e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:12:28,159] Trial 51 finished with value: 0.8372811960033055 and parameters: {'hidden_dim': 35, 'dropout': 0.18742505224288417, 'learning_rate': 0.0013616174462995991, 'batch_size': 16, 'weight_decay': 1.1213720307837932e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:12:39,652] Trial 52 finished with value: 0.8485349842854912 and parameters: {'hidden_dim': 43, 'dropout': 0.1489322427619982, 'learning_rate': 0.0022327232133972415, 'batch_size': 16, 'weight_decay': 4.773336587482316e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:12:47,615] Trial 53 finished with value: 0.8556405261780515 and parameters: {'hidden_dim': 37, 'dropout': 0.22267196998302993, 'learning_rate': 0.001514810352193557, 'batch_size': 16, 'weight_decay': 2.1021258479744017e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:12:55,664] Trial 54 finished with value: 0.8433106634932193 and parameters: {'hidden_dim': 38, 'dropout': 0.21290651978641842, 'learning_rate': 0.0015114851447527835, 'batch_size': 16, 'weight_decay': 1.0244242630912548e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:13:03,822] Trial 55 finished with value: 0.8220903289108157 and parameters: {'hidden_dim': 44, 'dropout': 0.12142187147249787, 'learning_rate': 0.0008281123118630522, 'batch_size': 16, 'weight_decay': 2.071219001915473e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:13:15,127] Trial 56 finished with value: 0.8523019361001105 and parameters: {'hidden_dim': 35, 'dropout': 0.15285481525098524, 'learning_rate': 0.001983465782986659, 'batch_size': 16, 'weight_decay': 2.9941829464423103e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:13:23,042] Trial 57 finished with value: 0.8456617977282278 and parameters: {'hidden_dim': 61, 'dropout': 0.17423901318979426, 'learning_rate': 0.0035582978961120376, 'batch_size': 16, 'weight_decay': 1.376730635973334e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:13:31,032] Trial 58 finished with value: 0.8406187850914018 and parameters: {'hidden_dim': 32, 'dropout': 0.23660174219060934, 'learning_rate': 0.00237065982975658, 'batch_size': 16, 'weight_decay': 1.0854827664305396e-05}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:13:33,610] Trial 59 finished with value: 0.7710563889569975 and parameters: {'hidden_dim': 40, 'dropout': 0.2781624991473492, 'learning_rate': 0.0016121311639234596, 'batch_size': 64, 'weight_decay': 1.7582721484052059e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:13:44,991] Trial 60 finished with value: 0.8723278214911074 and parameters: {'hidden_dim': 44, 'dropout': 0.21005012950803137, 'learning_rate': 0.0012590900815606107, 'batch_size': 16, 'weight_decay': 4.141871928899893e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:13:53,199] Trial 61 finished with value: 0.839476366732959 and parameters: {'hidden_dim': 44, 'dropout': 0.21417495234027403, 'learning_rate': 0.0009180743968771122, 'batch_size': 16, 'weight_decay': 4.913372600971364e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:14:01,420] Trial 62 finished with value: 0.8633450934160872 and parameters: {'hidden_dim': 37, 'dropout': 0.18329479422642064, 'learning_rate': 0.001291569580778625, 'batch_size': 16, 'weight_decay': 2.63293172580204e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:14:09,591] Trial 63 finished with value: 0.8465109209024018 and parameters: {'hidden_dim': 52, 'dropout': 0.17918061911552344, 'learning_rate': 0.0007223456318925999, 'batch_size': 16, 'weight_decay': 3.8730125177723115e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:14:20,663] Trial 64 finished with value: 0.8038940775249092 and parameters: {'hidden_dim': 39, 'dropout': 0.1279033244489235, 'learning_rate': 0.0006212268041617824, 'batch_size': 16, 'weight_decay': 2.686965976971645e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:14:28,560] Trial 65 finished with value: 0.8593135810174349 and parameters: {'hidden_dim': 54, 'dropout': 0.15075602787937206, 'learning_rate': 0.0012723364985793564, 'batch_size': 16, 'weight_decay': 1.2558064027560627e-05}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:14:31,132] Trial 66 finished with value: 0.7906113241052389 and parameters: {'hidden_dim': 119, 'dropout': 0.15076079558899297, 'learning_rate': 0.001200308020585357, 'batch_size': 64, 'weight_decay': 3.629159260126858e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:14:38,889] Trial 67 finished with value: 0.7942465683591444 and parameters: {'hidden_dim': 56, 'dropout': 0.2041344078935754, 'learning_rate': 0.00036643839139570785, 'batch_size': 16, 'weight_decay': 2.756685177807547e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:14:49,949] Trial 68 finished with value: 0.8189995818545515 and parameters: {'hidden_dim': 35, 'dropout': 0.16763883634646642, 'learning_rate': 0.0009279461677437043, 'batch_size': 16, 'weight_decay': 1.2506770977727964e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:14:57,904] Trial 69 finished with value: 0.8454451453437254 and parameters: {'hidden_dim': 42, 'dropout': 0.1838052957689855, 'learning_rate': 0.002530828042152304, 'batch_size': 16, 'weight_decay': 4.2676238676269356e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:15:05,940] Trial 70 finished with value: 0.8268233752386897 and parameters: {'hidden_dim': 52, 'dropout': 0.3207352127064794, 'learning_rate': 0.0031163468826483644, 'batch_size': 16, 'weight_decay': 3.2971722590929515e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:15:14,064] Trial 71 finished with value: 0.8614672158282706 and parameters: {'hidden_dim': 48, 'dropout': 0.1099831960498762, 'learning_rate': 0.0018848306419049852, 'batch_size': 16, 'weight_decay': 1.2538803221094931e-05}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:15:25,277] Trial 72 finished with value: 0.851155957520055 and parameters: {'hidden_dim': 46, 'dropout': 0.11400623052584943, 'learning_rate': 0.001822821617977136, 'batch_size': 16, 'weight_decay': 1.7464183439103026e-05}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:15:33,166] Trial 73 finished with value: 0.8465948389376381 and parameters: {'hidden_dim': 43, 'dropout': 0.13439840875044345, 'learning_rate': 0.0012193507350741876, 'batch_size': 16, 'weight_decay': 1.2757725416841374e-05}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:15:40,986] Trial 74 finished with value: 0.8631716951443117 and parameters: {'hidden_dim': 34, 'dropout': 0.14172726535417643, 'learning_rate': 0.0013643332147668442, 'batch_size': 16, 'weight_decay': 5.321091306557127e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:15:49,118] Trial 75 finished with value: 0.8149608664568097 and parameters: {'hidden_dim': 34, 'dropout': 0.11386058290124162, 'learning_rate': 0.001489338608839943, 'batch_size': 16, 'weight_decay': 5.2557094312642475e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:15:51,810] Trial 76 finished with value: 0.738818768002338 and parameters: {'hidden_dim': 38, 'dropout': 0.1405472048232842, 'learning_rate': 0.0018939489084232584, 'batch_size': 64, 'weight_decay': 6.928220270367047e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:16:03,133] Trial 77 finished with value: 0.8606080775959072 and parameters: {'hidden_dim': 33, 'dropout': 0.24346629039925355, 'learning_rate': 0.0010005962012011966, 'batch_size': 16, 'weight_decay': 5.543260695796428e-06}. Best is trial 50 with value: 0.8748025057355687.\n",
      "[I 2025-03-20 03:16:11,300] Trial 78 finished with value: 0.8761028740998315 and parameters: {'hidden_dim': 41, 'dropout': 0.24516633378016855, 'learning_rate': 0.0010847094119055844, 'batch_size': 16, 'weight_decay': 5.782211158231535e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:16:19,302] Trial 79 finished with value: 0.8360760971055088 and parameters: {'hidden_dim': 41, 'dropout': 0.20055217092234845, 'learning_rate': 0.0013627216945165392, 'batch_size': 16, 'weight_decay': 9.525012588053956e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:16:30,637] Trial 80 finished with value: 0.8403166099920664 and parameters: {'hidden_dim': 48, 'dropout': 0.26430644028768957, 'learning_rate': 0.0016044841436199868, 'batch_size': 16, 'weight_decay': 4.2164807383998855e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:16:38,678] Trial 81 finished with value: 0.8049004025423905 and parameters: {'hidden_dim': 39, 'dropout': 0.24504547542043342, 'learning_rate': 0.0009709389594101939, 'batch_size': 16, 'weight_decay': 5.644231977915015e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:16:46,649] Trial 82 finished with value: 0.7588073381430379 and parameters: {'hidden_dim': 37, 'dropout': 0.23151299574205836, 'learning_rate': 0.0005922430502333082, 'batch_size': 16, 'weight_decay': 7.97183359824792e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:16:54,550] Trial 83 finished with value: 0.8552695923639129 and parameters: {'hidden_dim': 32, 'dropout': 0.27448417452113755, 'learning_rate': 0.0010565314670409852, 'batch_size': 16, 'weight_decay': 5.9017016427780485e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:17:05,848] Trial 84 finished with value: 0.8512869014390313 and parameters: {'hidden_dim': 45, 'dropout': 0.16023928843829316, 'learning_rate': 0.0006983406279384726, 'batch_size': 16, 'weight_decay': 6.699256307615192e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:17:13,789] Trial 85 finished with value: 0.8411687722539649 and parameters: {'hidden_dim': 58, 'dropout': 0.24236613674297808, 'learning_rate': 0.0011240542607919592, 'batch_size': 16, 'weight_decay': 2.5541606185058435e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:17:21,625] Trial 86 finished with value: 0.8620921533953988 and parameters: {'hidden_dim': 51, 'dropout': 0.18905688918908953, 'learning_rate': 0.0008541741530013634, 'batch_size': 16, 'weight_decay': 1.7479369903558086e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:17:29,438] Trial 87 finished with value: 0.8379298064642892 and parameters: {'hidden_dim': 48, 'dropout': 0.18722186649111364, 'learning_rate': 0.0008734168623773422, 'batch_size': 16, 'weight_decay': 1.8943392411370128e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:17:40,487] Trial 88 finished with value: 0.8535733892731865 and parameters: {'hidden_dim': 50, 'dropout': 0.1725180230659831, 'learning_rate': 0.0024782727797444575, 'batch_size': 16, 'weight_decay': 1.4298267629674109e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:17:48,335] Trial 89 finished with value: 0.8498609346479529 and parameters: {'hidden_dim': 42, 'dropout': 0.20392259618192837, 'learning_rate': 0.003041427270475568, 'batch_size': 16, 'weight_decay': 1.6055052171373408e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:17:56,225] Trial 90 finished with value: 0.8475061421562436 and parameters: {'hidden_dim': 63, 'dropout': 0.10262685302159874, 'learning_rate': 0.0014027279166454907, 'batch_size': 16, 'weight_decay': 3.338443752296114e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:18:04,202] Trial 91 finished with value: 0.820099535540712 and parameters: {'hidden_dim': 45, 'dropout': 0.21935283909353592, 'learning_rate': 0.0007956528969312923, 'batch_size': 16, 'weight_decay': 2.40042089319498e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:18:15,767] Trial 92 finished with value: 0.8009133567957097 and parameters: {'hidden_dim': 36, 'dropout': 0.29687562530746303, 'learning_rate': 0.0005266748729864536, 'batch_size': 16, 'weight_decay': 4.944464032481522e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:18:24,155] Trial 93 finished with value: 0.8677082636768235 and parameters: {'hidden_dim': 40, 'dropout': 0.19033240571835133, 'learning_rate': 0.0009878011026311518, 'batch_size': 16, 'weight_decay': 8.703050582266606e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:18:32,225] Trial 94 finished with value: 0.8320573189487996 and parameters: {'hidden_dim': 40, 'dropout': 0.1933397305678244, 'learning_rate': 0.0016813661995107112, 'batch_size': 16, 'weight_decay': 7.759148314224408e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:18:43,518] Trial 95 finished with value: 0.8146690005209275 and parameters: {'hidden_dim': 51, 'dropout': 0.15985635906568219, 'learning_rate': 0.0011870336908343189, 'batch_size': 16, 'weight_decay': 1.0323311488093715e-05}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:18:51,596] Trial 96 finished with value: 0.8575670601279121 and parameters: {'hidden_dim': 47, 'dropout': 0.14213094638594842, 'learning_rate': 0.0013409938171340479, 'batch_size': 16, 'weight_decay': 1.8508320097291112e-06}. Best is trial 78 with value: 0.8761028740998315.\n",
      "[I 2025-03-20 03:18:59,716] Trial 97 finished with value: 0.8803320355551593 and parameters: {'hidden_dim': 44, 'dropout': 0.21278521272207102, 'learning_rate': 0.002100548251322848, 'batch_size': 16, 'weight_decay': 1.1620798961854362e-05}. Best is trial 97 with value: 0.8803320355551593.\n",
      "[I 2025-03-20 03:19:02,438] Trial 98 finished with value: 0.7789610612886474 and parameters: {'hidden_dim': 44, 'dropout': 0.17920150565783907, 'learning_rate': 0.0007651429838386861, 'batch_size': 64, 'weight_decay': 6.460242500546724e-06}. Best is trial 97 with value: 0.8803320355551593.\n",
      "[I 2025-03-20 03:19:13,792] Trial 99 finished with value: 0.8358495122186805 and parameters: {'hidden_dim': 39, 'dropout': 0.20859203304514592, 'learning_rate': 0.002125648552897958, 'batch_size': 16, 'weight_decay': 8.876142864030716e-06}. Best is trial 97 with value: 0.8803320355551593.\n",
      "[I 2025-03-20 03:19:13,805] A new study created in memory with name: no-name-d5c0c197-9c8f-459e-9e24-2a9c3867ef59\n",
      "[I 2025-03-20 03:19:13,956] Trial 0 finished with value: 0.7520905550655581 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.013661178428178952, 'n_estimators': 456, 'num_leaves': 48, 'max_depth': 6, 'subsample': 0.7552942184455931, 'colsample_bytree': 0.7030385741204636, 'reg_alpha': 1.3087232248047334, 'reg_lambda': 0.6869757398160233, 'min_child_samples': 49}. Best is trial 0 with value: 0.7520905550655581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LSTM Parameters:\n",
      "  hidden_dim: 44\n",
      "  dropout: 0.21278521272207102\n",
      "  learning_rate: 0.002100548251322848\n",
      "  batch_size: 16\n",
      "  weight_decay: 1.1620798961854362e-05\n",
      "\n",
      "Optimizing LightGBM for fold 3 using min_max normalization\n",
      "Optimizing LightGBM hyperparameters with CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:19:14,049] Trial 1 finished with value: 0.7880030383949126 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06323892443630524, 'n_estimators': 282, 'num_leaves': 42, 'max_depth': 3, 'subsample': 0.9077831487751247, 'colsample_bytree': 0.6012684467021177, 'reg_alpha': 0.31439409038438626, 'reg_lambda': 1.0912521853098582, 'min_child_samples': 47}. Best is trial 1 with value: 0.7880030383949126.\n",
      "[I 2025-03-20 03:19:14,164] Trial 2 finished with value: 0.7655191621069479 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.019957907467756848, 'n_estimators': 199, 'num_leaves': 37, 'max_depth': 10, 'subsample': 0.7873081624297713, 'colsample_bytree': 0.6493149518105418, 'reg_alpha': 3.6000609130029044, 'reg_lambda': 4.050134848257446, 'min_child_samples': 21}. Best is trial 1 with value: 0.7880030383949126.\n",
      "[I 2025-03-20 03:19:14,254] Trial 3 finished with value: 0.7896381925193945 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03308671790989007, 'n_estimators': 162, 'num_leaves': 10, 'max_depth': 5, 'subsample': 0.9963593754983149, 'colsample_bytree': 0.805256515122397, 'reg_alpha': 3.0841806167907238, 'reg_lambda': 0.1301677362213169, 'min_child_samples': 21}. Best is trial 3 with value: 0.7896381925193945.\n",
      "[I 2025-03-20 03:19:14,452] Trial 4 finished with value: 0.7797188226244035 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.029437387715603364, 'n_estimators': 405, 'num_leaves': 47, 'max_depth': 6, 'subsample': 0.9716350708762611, 'colsample_bytree': 0.7998448041733087, 'reg_alpha': 2.304933318498776, 'reg_lambda': 3.3362817514757888, 'min_child_samples': 29}. Best is trial 3 with value: 0.7896381925193945.\n",
      "[I 2025-03-20 03:19:14,512] Trial 5 finished with value: 0.7633359417922463 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07045793365915588, 'n_estimators': 210, 'num_leaves': 23, 'max_depth': 3, 'subsample': 0.8092590874861125, 'colsample_bytree': 0.859403208540551, 'reg_alpha': 3.3760828530802196, 'reg_lambda': 0.15012918259918107, 'min_child_samples': 22}. Best is trial 3 with value: 0.7896381925193945.\n",
      "[I 2025-03-20 03:19:14,702] Trial 6 finished with value: 0.7949132420377651 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011970252710206382, 'n_estimators': 476, 'num_leaves': 49, 'max_depth': 4, 'subsample': 0.9816084577717837, 'colsample_bytree': 0.7776193095921519, 'reg_alpha': 0.2802529145561249, 'reg_lambda': 1.3744727196612017, 'min_child_samples': 25}. Best is trial 6 with value: 0.7949132420377651.\n",
      "[I 2025-03-20 03:19:14,827] Trial 7 finished with value: 0.8229468221353724 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0967088301559984, 'n_estimators': 367, 'num_leaves': 50, 'max_depth': 10, 'subsample': 0.6725825297010686, 'colsample_bytree': 0.7765487989776441, 'reg_alpha': 1.3901311581414446, 'reg_lambda': 1.2894120399825089, 'min_child_samples': 25}. Best is trial 7 with value: 0.8229468221353724.\n",
      "[I 2025-03-20 03:19:14,970] Trial 8 finished with value: 0.8047708545568181 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04669085237595252, 'n_estimators': 388, 'num_leaves': 42, 'max_depth': 5, 'subsample': 0.6388242673991037, 'colsample_bytree': 0.9337670455562896, 'reg_alpha': 0.11037032003620384, 'reg_lambda': 0.25448376172072, 'min_child_samples': 45}. Best is trial 7 with value: 0.8229468221353724.\n",
      "[I 2025-03-20 03:19:15,063] Trial 9 finished with value: 0.7915462230828828 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011232085595448875, 'n_estimators': 177, 'num_leaves': 29, 'max_depth': 4, 'subsample': 0.7436406503252848, 'colsample_bytree': 0.6580654068535353, 'reg_alpha': 0.1699984374960256, 'reg_lambda': 0.1466016632681982, 'min_child_samples': 16}. Best is trial 7 with value: 0.8229468221353724.\n",
      "[I 2025-03-20 03:19:15,220] Trial 10 finished with value: 0.8263355427711302 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0987558719383339, 'n_estimators': 337, 'num_leaves': 18, 'max_depth': 10, 'subsample': 0.642242017563948, 'colsample_bytree': 0.9614634142305828, 'reg_alpha': 0.7548915517607346, 'reg_lambda': 0.47191940414066896, 'min_child_samples': 37}. Best is trial 10 with value: 0.8263355427711302.\n",
      "[I 2025-03-20 03:19:15,374] Trial 11 finished with value: 0.8184050135059792 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08811492902543544, 'n_estimators': 327, 'num_leaves': 17, 'max_depth': 10, 'subsample': 0.6019793854602531, 'colsample_bytree': 0.993860589036455, 'reg_alpha': 0.7915653922317443, 'reg_lambda': 0.4908802288038389, 'min_child_samples': 38}. Best is trial 10 with value: 0.8263355427711302.\n",
      "[I 2025-03-20 03:19:15,519] Trial 12 finished with value: 0.8188655520792546 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09822003027572318, 'n_estimators': 307, 'num_leaves': 30, 'max_depth': 8, 'subsample': 0.6801173039155465, 'colsample_bytree': 0.893626879385864, 'reg_alpha': 0.8750718436454238, 'reg_lambda': 1.8653713281408886, 'min_child_samples': 37}. Best is trial 10 with value: 0.8263355427711302.\n",
      "[I 2025-03-20 03:19:15,709] Trial 13 finished with value: 0.8375268806679731 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05345803972760133, 'n_estimators': 371, 'num_leaves': 19, 'max_depth': 8, 'subsample': 0.6866546830080261, 'colsample_bytree': 0.739487019699413, 'reg_alpha': 1.3765039473276297, 'reg_lambda': 0.36073977073087093, 'min_child_samples': 10}. Best is trial 13 with value: 0.8375268806679731.\n",
      "[I 2025-03-20 03:19:15,942] Trial 14 finished with value: 0.851574054099142 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05100525278048549, 'n_estimators': 257, 'num_leaves': 16, 'max_depth': 8, 'subsample': 0.7213915181789433, 'colsample_bytree': 0.9955930766794112, 'reg_alpha': 0.4759657006669087, 'reg_lambda': 0.3531473318787954, 'min_child_samples': 10}. Best is trial 14 with value: 0.851574054099142.\n",
      "[I 2025-03-20 03:19:16,047] Trial 15 finished with value: 0.8348285791439494 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.043288657420412956, 'n_estimators': 128, 'num_leaves': 11, 'max_depth': 8, 'subsample': 0.728281809019484, 'colsample_bytree': 0.7201040728875937, 'reg_alpha': 0.40032058910065316, 'reg_lambda': 0.27407685686214406, 'min_child_samples': 13}. Best is trial 14 with value: 0.851574054099142.\n",
      "[I 2025-03-20 03:19:16,295] Trial 16 finished with value: 0.8568752443232881 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.049153270642365854, 'n_estimators': 257, 'num_leaves': 23, 'max_depth': 8, 'subsample': 0.8460344390937014, 'colsample_bytree': 0.8679612440467525, 'reg_alpha': 0.4703040391963052, 'reg_lambda': 0.2514535341033357, 'min_child_samples': 10}. Best is trial 16 with value: 0.8568752443232881.\n",
      "[I 2025-03-20 03:19:16,503] Trial 17 finished with value: 0.850697646048434 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03350416889348262, 'n_estimators': 255, 'num_leaves': 25, 'max_depth': 7, 'subsample': 0.872602340863694, 'colsample_bytree': 0.8654355879743639, 'reg_alpha': 0.4952065943462705, 'reg_lambda': 0.22075145628542162, 'min_child_samples': 15}. Best is trial 16 with value: 0.8568752443232881.\n",
      "[I 2025-03-20 03:19:16,720] Trial 18 finished with value: 0.8547579456028501 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0276669503148614, 'n_estimators': 247, 'num_leaves': 14, 'max_depth': 9, 'subsample': 0.8556167667539123, 'colsample_bytree': 0.9117968576699139, 'reg_alpha': 0.2193792490608938, 'reg_lambda': 0.6912140904160914, 'min_child_samples': 10}. Best is trial 16 with value: 0.8568752443232881.\n",
      "[I 2025-03-20 03:19:16,934] Trial 19 finished with value: 0.8430588984075691 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.02268851777434901, 'n_estimators': 236, 'num_leaves': 24, 'max_depth': 9, 'subsample': 0.8566733671392915, 'colsample_bytree': 0.9103587281284006, 'reg_alpha': 0.14718132961011152, 'reg_lambda': 0.7268115000906161, 'min_child_samples': 17}. Best is trial 16 with value: 0.8568752443232881.\n",
      "[I 2025-03-20 03:19:17,013] Trial 20 finished with value: 0.7594793181027747 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.01939122134584428, 'n_estimators': 102, 'num_leaves': 13, 'max_depth': 9, 'subsample': 0.917783483703512, 'colsample_bytree': 0.8396404212791115, 'reg_alpha': 0.231201863633108, 'reg_lambda': 0.10341899854233196, 'min_child_samples': 31}. Best is trial 16 with value: 0.8568752443232881.\n",
      "[I 2025-03-20 03:19:17,268] Trial 21 finished with value: 0.8537798114011622 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03874967976830049, 'n_estimators': 274, 'num_leaves': 15, 'max_depth': 7, 'subsample': 0.8362444070837092, 'colsample_bytree': 0.9990162550266966, 'reg_alpha': 0.4851853443851495, 'reg_lambda': 0.6708340206397443, 'min_child_samples': 10}. Best is trial 16 with value: 0.8568752443232881.\n",
      "[I 2025-03-20 03:19:17,567] Trial 22 finished with value: 0.8596451443384033 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.038261080720092266, 'n_estimators': 297, 'num_leaves': 22, 'max_depth': 7, 'subsample': 0.8363068228380048, 'colsample_bytree': 0.9485703106943153, 'reg_alpha': 0.20836809016002875, 'reg_lambda': 0.8007942833689408, 'min_child_samples': 12}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:17,772] Trial 23 finished with value: 0.8334544505279231 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.024573230187368317, 'n_estimators': 226, 'num_leaves': 22, 'max_depth': 9, 'subsample': 0.9032042430937963, 'colsample_bytree': 0.9424799839672464, 'reg_alpha': 0.19030637771004824, 'reg_lambda': 2.593560466402131, 'min_child_samples': 14}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:17,986] Trial 24 finished with value: 0.8473964170987255 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.028200311646237337, 'n_estimators': 305, 'num_leaves': 34, 'max_depth': 7, 'subsample': 0.8041100302663126, 'colsample_bytree': 0.8910864011293338, 'reg_alpha': 0.11974739894585185, 'reg_lambda': 0.9095659496080455, 'min_child_samples': 18}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:18,357] Trial 25 finished with value: 0.847390430152284 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.038649356513073406, 'n_estimators': 426, 'num_leaves': 26, 'max_depth': 9, 'subsample': 0.9381892423135044, 'colsample_bytree': 0.9306719574768028, 'reg_alpha': 0.32910675150462587, 'reg_lambda': 2.0491204012186, 'min_child_samples': 13}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:18,604] Trial 26 finished with value: 0.8376662898262264 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.016236598545921726, 'n_estimators': 348, 'num_leaves': 20, 'max_depth': 7, 'subsample': 0.8711405729105965, 'colsample_bytree': 0.8398323788872903, 'reg_alpha': 0.24368736736346555, 'reg_lambda': 0.46145237339925765, 'min_child_samples': 19}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:18,877] Trial 27 finished with value: 0.8560663192990592 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06339622234305062, 'n_estimators': 292, 'num_leaves': 28, 'max_depth': 8, 'subsample': 0.8247539939838564, 'colsample_bytree': 0.9666101985593151, 'reg_alpha': 0.20209768644723336, 'reg_lambda': 0.19162343172746513, 'min_child_samples': 12}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:19,120] Trial 28 finished with value: 0.855073991093646 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06405780828170501, 'n_estimators': 309, 'num_leaves': 29, 'max_depth': 6, 'subsample': 0.7854608637116649, 'colsample_bytree': 0.9576271686336337, 'reg_alpha': 0.14212250851978403, 'reg_lambda': 0.18386666651483977, 'min_child_samples': 13}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:19,296] Trial 29 finished with value: 0.8352609763501494 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07100932147957231, 'n_estimators': 286, 'num_leaves': 33, 'max_depth': 6, 'subsample': 0.7594198842677128, 'colsample_bytree': 0.9611303673491199, 'reg_alpha': 0.59998402374673, 'reg_lambda': 0.31588427804003605, 'min_child_samples': 24}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:19,428] Trial 30 finished with value: 0.8145103779245572 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.056595538328310095, 'n_estimators': 190, 'num_leaves': 26, 'max_depth': 8, 'subsample': 0.834950384334098, 'colsample_bytree': 0.874810809235518, 'reg_alpha': 1.006479387826592, 'reg_lambda': 0.19460830538811472, 'min_child_samples': 30}. Best is trial 22 with value: 0.8596451443384033.\n",
      "[I 2025-03-20 03:19:19,680] Trial 31 finished with value: 0.861292236822857 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0695047657232799, 'n_estimators': 316, 'num_leaves': 29, 'max_depth': 6, 'subsample': 0.7735182246208606, 'colsample_bytree': 0.9605942242393898, 'reg_alpha': 0.14568751194733093, 'reg_lambda': 0.18609839616575088, 'min_child_samples': 13}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:19,926] Trial 32 finished with value: 0.859868907283178 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07587641539606522, 'n_estimators': 268, 'num_leaves': 32, 'max_depth': 7, 'subsample': 0.8351881336148554, 'colsample_bytree': 0.9648472250748434, 'reg_alpha': 0.10165090283616454, 'reg_lambda': 0.1780446607995051, 'min_child_samples': 13}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:20,101] Trial 33 finished with value: 0.8520916026158192 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08644342577130426, 'n_estimators': 270, 'num_leaves': 33, 'max_depth': 5, 'subsample': 0.7592117426478897, 'colsample_bytree': 0.9148354302725099, 'reg_alpha': 0.10139573553788836, 'reg_lambda': 0.11942786604693309, 'min_child_samples': 19}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:20,298] Trial 34 finished with value: 0.8549680908347046 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07923781116698583, 'n_estimators': 216, 'num_leaves': 39, 'max_depth': 7, 'subsample': 0.7840997785027541, 'colsample_bytree': 0.9332292713691073, 'reg_alpha': 0.1426062470714311, 'reg_lambda': 0.16196406421905932, 'min_child_samples': 15}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:20,529] Trial 35 finished with value: 0.8534866412244432 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04288398412048482, 'n_estimators': 323, 'num_leaves': 36, 'max_depth': 6, 'subsample': 0.8757607203381023, 'colsample_bytree': 0.9753183335378762, 'reg_alpha': 0.34141341514533297, 'reg_lambda': 0.10195611732775527, 'min_child_samples': 17}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:20,725] Trial 36 finished with value: 0.8461487565343384 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0747332609635698, 'n_estimators': 355, 'num_leaves': 22, 'max_depth': 5, 'subsample': 0.8935740782222928, 'colsample_bytree': 0.8288956413653737, 'reg_alpha': 0.17148036687009216, 'reg_lambda': 0.22432082362293987, 'min_child_samples': 22}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:21,090] Trial 37 finished with value: 0.8488726244665561 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.057539922345067766, 'n_estimators': 422, 'num_leaves': 31, 'max_depth': 6, 'subsample': 0.8169343110323913, 'colsample_bytree': 0.8759441262601572, 'reg_alpha': 0.2685716989745317, 'reg_lambda': 4.843323637172309, 'min_child_samples': 12}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:21,269] Trial 38 finished with value: 0.8229766719757183 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03629954661234423, 'n_estimators': 272, 'num_leaves': 27, 'max_depth': 7, 'subsample': 0.9448734468113322, 'colsample_bytree': 0.9811886561000722, 'reg_alpha': 0.12182272277184703, 'reg_lambda': 1.010265836985336, 'min_child_samples': 27}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:21,432] Trial 39 finished with value: 0.8511271673676607 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04932929239740631, 'n_estimators': 233, 'num_leaves': 46, 'max_depth': 6, 'subsample': 0.7830659559786121, 'colsample_bytree': 0.943265460033535, 'reg_alpha': 0.38291199135337395, 'reg_lambda': 0.5890314527823054, 'min_child_samples': 20}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:21,560] Trial 40 finished with value: 0.8201907579565795 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06392147774756182, 'n_estimators': 203, 'num_leaves': 21, 'max_depth': 5, 'subsample': 0.8490862151537418, 'colsample_bytree': 0.8944949745439137, 'reg_alpha': 1.6605656039138235, 'reg_lambda': 0.28047697653431103, 'min_child_samples': 16}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:21,844] Trial 41 finished with value: 0.8578487721236108 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06101131248458437, 'n_estimators': 287, 'num_leaves': 28, 'max_depth': 8, 'subsample': 0.8221244134805158, 'colsample_bytree': 0.9714889945874916, 'reg_alpha': 0.18637640303795683, 'reg_lambda': 0.1382370624920571, 'min_child_samples': 12}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:22,083] Trial 42 finished with value: 0.860088304662443 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08131406368961124, 'n_estimators': 321, 'num_leaves': 31, 'max_depth': 7, 'subsample': 0.8010189900733448, 'colsample_bytree': 0.9482418776560027, 'reg_alpha': 0.15949528715547848, 'reg_lambda': 0.1273119405868242, 'min_child_samples': 11}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:22,338] Trial 43 finished with value: 0.8563321898418984 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08500315524381467, 'n_estimators': 326, 'num_leaves': 31, 'max_depth': 7, 'subsample': 0.7969258193313069, 'colsample_bytree': 0.951973089840402, 'reg_alpha': 0.10033447560762772, 'reg_lambda': 0.14286087927749486, 'min_child_samples': 12}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:22,431] Trial 44 finished with value: 0.7179853223273748 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07726449297039549, 'n_estimators': 387, 'num_leaves': 38, 'max_depth': 7, 'subsample': 0.7738757524904507, 'colsample_bytree': 0.9821062027305634, 'reg_alpha': 4.673879544484267, 'reg_lambda': 0.11693499215976914, 'min_child_samples': 34}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:22,660] Trial 45 finished with value: 0.8591112524382597 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.057799337598184286, 'n_estimators': 296, 'num_leaves': 36, 'max_depth': 6, 'subsample': 0.8129139353737993, 'colsample_bytree': 0.9233394469008651, 'reg_alpha': 0.16718535035694054, 'reg_lambda': 0.1647593239912947, 'min_child_samples': 15}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:22,920] Trial 46 finished with value: 0.8494706756782175 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06636972590276712, 'n_estimators': 342, 'num_leaves': 42, 'max_depth': 6, 'subsample': 0.7375183219639961, 'colsample_bytree': 0.924680820547826, 'reg_alpha': 0.14661293030261738, 'reg_lambda': 1.2770802404446837, 'min_child_samples': 15}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:23,052] Trial 47 finished with value: 0.8116764392464748 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08695980423833646, 'n_estimators': 300, 'num_leaves': 36, 'max_depth': 6, 'subsample': 0.8065784068475574, 'colsample_bytree': 0.6350829276676312, 'reg_alpha': 0.126257697214795, 'reg_lambda': 0.16556965182759417, 'min_child_samples': 46}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:23,179] Trial 48 finished with value: 0.7971700363233004 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07101680906138097, 'n_estimators': 317, 'num_leaves': 41, 'max_depth': 4, 'subsample': 0.7124930486025107, 'colsample_bytree': 0.8964975758384066, 'reg_alpha': 0.1671405632577758, 'reg_lambda': 0.12731741347850342, 'min_child_samples': 49}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:23,394] Trial 49 finished with value: 0.8401158075871595 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04386440677618629, 'n_estimators': 372, 'num_leaves': 34, 'max_depth': 5, 'subsample': 0.7652181761756363, 'colsample_bytree': 0.7757613575253909, 'reg_alpha': 0.2841673912599838, 'reg_lambda': 0.3939688443065976, 'min_child_samples': 21}. Best is trial 31 with value: 0.861292236822857.\n",
      "[I 2025-03-20 03:19:23,463] A new study created in memory with name: no-name-af137a03-fa8b-4347-94f9-bda48382bf95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LightGBM Parameters:\n",
      "  boosting_type: gbdt\n",
      "  learning_rate: 0.0695047657232799\n",
      "  n_estimators: 316\n",
      "  num_leaves: 29\n",
      "  max_depth: 6\n",
      "  subsample: 0.7735182246208606\n",
      "  colsample_bytree: 0.9605942242393898\n",
      "  reg_alpha: 0.14568751194733093\n",
      "  reg_lambda: 0.18609839616575088\n",
      "  min_child_samples: 13\n",
      "\n",
      "Optimizing LSTM for fold 3 using min_max normalization\n",
      "Created LSTM sequences with lookback=10: 453 train, 145 test\n",
      "Optimizing LSTM hyperparameters with CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:19:35,206] Trial 0 finished with value: 0.7360190119021798 and parameters: {'hidden_dim': 45, 'dropout': 0.4551406810921512, 'learning_rate': 0.007331811314274292, 'batch_size': 16, 'weight_decay': 2.064121088592392e-05}. Best is trial 0 with value: 0.7360190119021798.\n",
      "[I 2025-03-20 03:19:41,518] Trial 1 finished with value: 0.7602867078276914 and parameters: {'hidden_dim': 79, 'dropout': 0.48569536770002286, 'learning_rate': 0.003999161563860724, 'batch_size': 32, 'weight_decay': 3.45872269711161e-05}. Best is trial 1 with value: 0.7602867078276914.\n",
      "[I 2025-03-20 03:19:56,473] Trial 2 finished with value: 0.8122610101743096 and parameters: {'hidden_dim': 32, 'dropout': 0.21712375681979076, 'learning_rate': 0.0016665895921660098, 'batch_size': 16, 'weight_decay': 2.1517123363927774e-05}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:20:02,728] Trial 3 finished with value: 0.6639936725280756 and parameters: {'hidden_dim': 127, 'dropout': 0.2873081624297713, 'learning_rate': 0.00017643094449433023, 'batch_size': 32, 'weight_decay': 1.0947309020486406e-05}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:20:14,231] Trial 4 finished with value: 0.7280983411091931 and parameters: {'hidden_dim': 47, 'dropout': 0.10585094112968818, 'learning_rate': 0.0004451295365396247, 'batch_size': 16, 'weight_decay': 1.3639281514957935e-06}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:20:20,645] Trial 5 finished with value: 0.7990239555120645 and parameters: {'hidden_dim': 59, 'dropout': 0.2875597071705984, 'learning_rate': 0.0033384615669657483, 'batch_size': 64, 'weight_decay': 9.98214837316597e-06}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:20:32,162] Trial 6 finished with value: 0.7267373865647936 and parameters: {'hidden_dim': 109, 'dropout': 0.4586320455614753, 'learning_rate': 0.0009209968619749436, 'batch_size': 16, 'weight_decay': 1.6223673683559225e-06}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:20:38,535] Trial 7 finished with value: 0.7242945474358543 and parameters: {'hidden_dim': 82, 'dropout': 0.3594032085405511, 'learning_rate': 0.006298297451196278, 'batch_size': 32, 'weight_decay': 7.511737052942637e-05}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:20:41,913] Trial 8 finished with value: 0.7392123619900115 and parameters: {'hidden_dim': 125, 'dropout': 0.17649687394806254, 'learning_rate': 0.00809174687672524, 'batch_size': 64, 'weight_decay': 5.656354626844694e-06}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:20:56,519] Trial 9 finished with value: 0.6597686973249776 and parameters: {'hidden_dim': 127, 'dropout': 0.3668293205677101, 'learning_rate': 0.009614679235250567, 'batch_size': 16, 'weight_decay': 2.2160868002868673e-05}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:21:08,371] Trial 10 finished with value: 0.8050694102241089 and parameters: {'hidden_dim': 32, 'dropout': 0.198627869896562, 'learning_rate': 0.0015689723478599755, 'batch_size': 16, 'weight_decay': 8.469738945067809e-05}. Best is trial 2 with value: 0.8122610101743096.\n",
      "[I 2025-03-20 03:21:19,984] Trial 11 finished with value: 0.8169292858769884 and parameters: {'hidden_dim': 35, 'dropout': 0.19058312272398745, 'learning_rate': 0.0019833844976595914, 'batch_size': 16, 'weight_decay': 7.238871454280833e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:21:34,999] Trial 12 finished with value: 0.8003700946641376 and parameters: {'hidden_dim': 33, 'dropout': 0.19858417871335965, 'learning_rate': 0.0015088018581849542, 'batch_size': 16, 'weight_decay': 4.505430918133612e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:21:47,069] Trial 13 finished with value: 0.7952840074059192 and parameters: {'hidden_dim': 66, 'dropout': 0.1318560556035414, 'learning_rate': 0.0007285120693189867, 'batch_size': 16, 'weight_decay': 4.459010219894359e-06}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:22:01,969] Trial 14 finished with value: 0.7952180183824146 and parameters: {'hidden_dim': 50, 'dropout': 0.23755223498704398, 'learning_rate': 0.003174392682478581, 'batch_size': 16, 'weight_decay': 4.4060877591817896e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:22:05,356] Trial 15 finished with value: 0.6566199898513453 and parameters: {'hidden_dim': 75, 'dropout': 0.24012784951072286, 'learning_rate': 0.0003399790241608852, 'batch_size': 64, 'weight_decay': 9.688940904195156e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:22:17,079] Trial 16 finished with value: 0.8136180554097895 and parameters: {'hidden_dim': 98, 'dropout': 0.15463121208619401, 'learning_rate': 0.0016532604402274915, 'batch_size': 16, 'weight_decay': 1.9921687411113018e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:22:28,938] Trial 17 finished with value: 0.6506261266333998 and parameters: {'hidden_dim': 96, 'dropout': 0.14639652501452285, 'learning_rate': 0.00010728531826263386, 'batch_size': 16, 'weight_decay': 1.1223829274855612e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:22:38,566] Trial 18 finished with value: 0.80091588194509 and parameters: {'hidden_dim': 96, 'dropout': 0.15598298481736675, 'learning_rate': 0.002302075572047397, 'batch_size': 32, 'weight_decay': 2.7706777476763417e-06}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:22:41,860] Trial 19 finished with value: 0.7090715346891749 and parameters: {'hidden_dim': 97, 'dropout': 0.1014253301056214, 'learning_rate': 0.0007559790188628007, 'batch_size': 64, 'weight_decay': 5.659909112491279e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:22:53,260] Trial 20 finished with value: 0.6856291860563392 and parameters: {'hidden_dim': 113, 'dropout': 0.3334610357759126, 'learning_rate': 0.0004158017143329979, 'batch_size': 16, 'weight_decay': 2.9198087772471702e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:23:07,934] Trial 21 finished with value: 0.8086597245336566 and parameters: {'hidden_dim': 37, 'dropout': 0.23948740921333983, 'learning_rate': 0.0015689824420569813, 'batch_size': 16, 'weight_decay': 1.683711044094791e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:23:19,501] Trial 22 finished with value: 0.8137845550423477 and parameters: {'hidden_dim': 58, 'dropout': 0.20155166752657272, 'learning_rate': 0.0021079415465908758, 'batch_size': 16, 'weight_decay': 1.5152107291720501e-05}. Best is trial 11 with value: 0.8169292858769884.\n",
      "[I 2025-03-20 03:23:30,916] Trial 23 finished with value: 0.8350646921213766 and parameters: {'hidden_dim': 59, 'dropout': 0.17697596120629389, 'learning_rate': 0.0022231274219150095, 'batch_size': 16, 'weight_decay': 7.595089344824128e-06}. Best is trial 23 with value: 0.8350646921213766.\n",
      "[I 2025-03-20 03:23:45,575] Trial 24 finished with value: 0.7703030777811429 and parameters: {'hidden_dim': 58, 'dropout': 0.25315206909388654, 'learning_rate': 0.0047794948021269885, 'batch_size': 16, 'weight_decay': 5.022782034128995e-06}. Best is trial 23 with value: 0.8350646921213766.\n",
      "[I 2025-03-20 03:23:56,907] Trial 25 finished with value: 0.7947600322917802 and parameters: {'hidden_dim': 63, 'dropout': 0.18130980663016705, 'learning_rate': 0.002552635490016211, 'batch_size': 16, 'weight_decay': 7.202905954529353e-06}. Best is trial 23 with value: 0.8350646921213766.\n",
      "[I 2025-03-20 03:24:08,333] Trial 26 finished with value: 0.7917174918473696 and parameters: {'hidden_dim': 72, 'dropout': 0.2697804063040586, 'learning_rate': 0.0011257142597659155, 'batch_size': 16, 'weight_decay': 3.538708743904431e-06}. Best is trial 23 with value: 0.8350646921213766.\n",
      "[I 2025-03-20 03:24:23,223] Trial 27 finished with value: 0.835227743343652 and parameters: {'hidden_dim': 42, 'dropout': 0.3104167454995045, 'learning_rate': 0.0023823155077880463, 'batch_size': 16, 'weight_decay': 2.5032956355233365e-06}. Best is trial 27 with value: 0.835227743343652.\n",
      "[I 2025-03-20 03:24:26,619] Trial 28 finished with value: 0.8123204424982305 and parameters: {'hidden_dim': 40, 'dropout': 0.31455466292239975, 'learning_rate': 0.004452598717358968, 'batch_size': 64, 'weight_decay': 1.8844985691545417e-06}. Best is trial 27 with value: 0.835227743343652.\n",
      "[I 2025-03-20 03:24:32,994] Trial 29 finished with value: 0.7684060134389158 and parameters: {'hidden_dim': 51, 'dropout': 0.4136761646993473, 'learning_rate': 0.0011783045687170392, 'batch_size': 32, 'weight_decay': 3.0566180567572233e-06}. Best is trial 27 with value: 0.835227743343652.\n",
      "[I 2025-03-20 03:24:48,205] Trial 30 finished with value: 0.8036416017771364 and parameters: {'hidden_dim': 41, 'dropout': 0.39531626808507236, 'learning_rate': 0.0027443120920818084, 'batch_size': 16, 'weight_decay': 1.0353633098111883e-06}. Best is trial 27 with value: 0.835227743343652.\n",
      "[I 2025-03-20 03:24:59,702] Trial 31 finished with value: 0.8369782363663685 and parameters: {'hidden_dim': 55, 'dropout': 0.2088370558201038, 'learning_rate': 0.002254292382637437, 'batch_size': 16, 'weight_decay': 1.4840972389397792e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:25:11,224] Trial 32 finished with value: 0.7746312200075764 and parameters: {'hidden_dim': 44, 'dropout': 0.3130839595578089, 'learning_rate': 0.005507352458556795, 'batch_size': 16, 'weight_decay': 7.0203121618885154e-06}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:25:25,832] Trial 33 finished with value: 0.773068541791123 and parameters: {'hidden_dim': 54, 'dropout': 0.2697905993695227, 'learning_rate': 0.0035522687418733144, 'batch_size': 16, 'weight_decay': 2.245358684458668e-06}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:25:37,326] Trial 34 finished with value: 0.8251206971363979 and parameters: {'hidden_dim': 47, 'dropout': 0.22447637087977707, 'learning_rate': 0.0021680057274791275, 'batch_size': 16, 'weight_decay': 3.198995001946066e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:25:43,756] Trial 35 finished with value: 0.7658718865325307 and parameters: {'hidden_dim': 68, 'dropout': 0.2207547090254368, 'learning_rate': 0.00403413957487142, 'batch_size': 32, 'weight_decay': 3.07069243299609e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:25:58,362] Trial 36 finished with value: 0.8159533177427428 and parameters: {'hidden_dim': 47, 'dropout': 0.2707431698722924, 'learning_rate': 0.0028060328808230933, 'batch_size': 16, 'weight_decay': 1.0014358568206149e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:26:10,075] Trial 37 finished with value: 0.7863697043996051 and parameters: {'hidden_dim': 82, 'dropout': 0.21852528320375408, 'learning_rate': 0.0013129185599813063, 'batch_size': 16, 'weight_decay': 2.57962285798017e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:26:25,254] Trial 38 finished with value: 0.8043965801210087 and parameters: {'hidden_dim': 54, 'dropout': 0.2927756345522359, 'learning_rate': 0.0008744476366374167, 'batch_size': 16, 'weight_decay': 1.3552592399545205e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:26:31,631] Trial 39 finished with value: 0.6725073144058369 and parameters: {'hidden_dim': 44, 'dropout': 0.1266051535718195, 'learning_rate': 0.000575801391856934, 'batch_size': 32, 'weight_decay': 7.631983380033869e-06}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:26:35,007] Trial 40 finished with value: 0.8050988105594432 and parameters: {'hidden_dim': 61, 'dropout': 0.16833474863431344, 'learning_rate': 0.0019714957407649582, 'batch_size': 64, 'weight_decay': 3.682086959468894e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:26:46,295] Trial 41 finished with value: 0.8113647608452504 and parameters: {'hidden_dim': 37, 'dropout': 0.19042658737285767, 'learning_rate': 0.0020205842243222233, 'batch_size': 16, 'weight_decay': 6.176059795590168e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:27:00,843] Trial 42 finished with value: 0.7934485549383957 and parameters: {'hidden_dim': 48, 'dropout': 0.33489253310616635, 'learning_rate': 0.0031987823210029077, 'batch_size': 16, 'weight_decay': 5.9067443366268126e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:27:12,422] Trial 43 finished with value: 0.8105715417970094 and parameters: {'hidden_dim': 39, 'dropout': 0.21508370325722745, 'learning_rate': 0.0020412877568422665, 'batch_size': 16, 'weight_decay': 4.1483113546927214e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:27:24,004] Trial 44 finished with value: 0.7486454397300623 and parameters: {'hidden_dim': 53, 'dropout': 0.1695035677436937, 'learning_rate': 0.005981956482151496, 'batch_size': 16, 'weight_decay': 2.0329462033470753e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:27:38,814] Trial 45 finished with value: 0.8197017050139884 and parameters: {'hidden_dim': 35, 'dropout': 0.13716569691332808, 'learning_rate': 0.0012623279651572169, 'batch_size': 16, 'weight_decay': 1.2453035033581023e-05}. Best is trial 31 with value: 0.8369782363663685.\n",
      "[I 2025-03-20 03:27:50,741] Trial 46 finished with value: 0.8374650966460018 and parameters: {'hidden_dim': 44, 'dropout': 0.12113807652547057, 'learning_rate': 0.0010678127284445532, 'batch_size': 16, 'weight_decay': 1.2306041014896444e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:28:05,335] Trial 47 finished with value: 0.7942057530922689 and parameters: {'hidden_dim': 43, 'dropout': 0.12575445568745927, 'learning_rate': 0.0009297080994705731, 'batch_size': 16, 'weight_decay': 8.022908610092988e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:28:16,711] Trial 48 finished with value: 0.6874205588445949 and parameters: {'hidden_dim': 56, 'dropout': 0.11541885782622224, 'learning_rate': 0.0002803345209285929, 'batch_size': 16, 'weight_decay': 6.037983290986282e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:28:28,386] Trial 49 finished with value: 0.7642911506684307 and parameters: {'hidden_dim': 48, 'dropout': 0.15941578087681504, 'learning_rate': 0.0006062082333565724, 'batch_size': 16, 'weight_decay': 9.201034585987026e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:28:31,716] Trial 50 finished with value: 0.7609407618643361 and parameters: {'hidden_dim': 65, 'dropout': 0.3614653247298948, 'learning_rate': 0.007840466958884075, 'batch_size': 64, 'weight_decay': 1.8438410388220326e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:28:46,375] Trial 51 finished with value: 0.8114317383019761 and parameters: {'hidden_dim': 35, 'dropout': 0.13584622484236608, 'learning_rate': 0.0012722032594874335, 'batch_size': 16, 'weight_decay': 1.2678030768933016e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:28:57,876] Trial 52 finished with value: 0.8064014433790468 and parameters: {'hidden_dim': 32, 'dropout': 0.14499613538550835, 'learning_rate': 0.0014117465628381489, 'batch_size': 16, 'weight_decay': 1.1750299343684644e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:29:12,542] Trial 53 finished with value: 0.8291523635381867 and parameters: {'hidden_dim': 44, 'dropout': 0.11526076864256554, 'learning_rate': 0.001700856575662567, 'batch_size': 16, 'weight_decay': 1.449218085865924e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:29:24,186] Trial 54 finished with value: 0.8266103229189121 and parameters: {'hidden_dim': 49, 'dropout': 0.11415794743770642, 'learning_rate': 0.0017076582166543567, 'batch_size': 16, 'weight_decay': 2.3673032076828554e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:29:35,653] Trial 55 finished with value: 0.8171750475808429 and parameters: {'hidden_dim': 51, 'dropout': 0.1094667684149064, 'learning_rate': 0.0017257368870228066, 'batch_size': 16, 'weight_decay': 2.509979198342724e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:29:44,957] Trial 56 finished with value: 0.7616283615475488 and parameters: {'hidden_dim': 69, 'dropout': 0.12012488327525842, 'learning_rate': 0.0010558430839293109, 'batch_size': 32, 'weight_decay': 1.4521078566393873e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:29:56,608] Trial 57 finished with value: 0.8370937234046215 and parameters: {'hidden_dim': 42, 'dropout': 0.11288978762913365, 'learning_rate': 0.001737257092615968, 'batch_size': 16, 'weight_decay': 4.040500198242192e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:30:08,108] Trial 58 finished with value: 0.7935917902957849 and parameters: {'hidden_dim': 86, 'dropout': 0.10462296616346176, 'learning_rate': 0.0027464396124118884, 'batch_size': 16, 'weight_decay': 4.26072294940193e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:30:22,716] Trial 59 finished with value: 0.7704250951422505 and parameters: {'hidden_dim': 43, 'dropout': 0.1516706942470436, 'learning_rate': 0.0007575898779743291, 'batch_size': 16, 'weight_decay': 3.950692816537805e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:30:34,151] Trial 60 finished with value: 0.7888575290402234 and parameters: {'hidden_dim': 61, 'dropout': 0.10030376993912539, 'learning_rate': 0.0036434639883787968, 'batch_size': 16, 'weight_decay': 5.873461363949098e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:30:45,617] Trial 61 finished with value: 0.8240916423639814 and parameters: {'hidden_dim': 39, 'dropout': 0.11693537371472054, 'learning_rate': 0.0016589366084898275, 'batch_size': 16, 'weight_decay': 2.3168151763925553e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:31:00,390] Trial 62 finished with value: 0.7844388042667885 and parameters: {'hidden_dim': 57, 'dropout': 0.4837977098982009, 'learning_rate': 0.0017906007028913803, 'batch_size': 16, 'weight_decay': 1.642875590413459e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:31:11,825] Trial 63 finished with value: 0.824545379024714 and parameters: {'hidden_dim': 51, 'dropout': 0.13840849858287343, 'learning_rate': 0.0014649525032868742, 'batch_size': 16, 'weight_decay': 9.11159402644134e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:31:26,598] Trial 64 finished with value: 0.8044019082203104 and parameters: {'hidden_dim': 46, 'dropout': 0.17864451759600694, 'learning_rate': 0.0024017509810901803, 'batch_size': 16, 'weight_decay': 1.3535418114880587e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:31:30,144] Trial 65 finished with value: 0.7680759100610174 and parameters: {'hidden_dim': 41, 'dropout': 0.15877902874880728, 'learning_rate': 0.00242382644220601, 'batch_size': 64, 'weight_decay': 2.3753031863617614e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:31:41,699] Trial 66 finished with value: 0.8083435314097978 and parameters: {'hidden_dim': 49, 'dropout': 0.3430217908095853, 'learning_rate': 0.0010154824161570201, 'batch_size': 16, 'weight_decay': 4.979494030956068e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:31:56,556] Trial 67 finished with value: 0.8275863257593805 and parameters: {'hidden_dim': 53, 'dropout': 0.12650097146517786, 'learning_rate': 0.0018233234681958328, 'batch_size': 16, 'weight_decay': 3.2547249481765676e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:32:07,960] Trial 68 finished with value: 0.8105046670707027 and parameters: {'hidden_dim': 60, 'dropout': 0.18828766749333029, 'learning_rate': 0.002925401554380544, 'batch_size': 16, 'weight_decay': 3.213903444862597e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:32:14,137] Trial 69 finished with value: 0.8311182530964336 and parameters: {'hidden_dim': 55, 'dropout': 0.13092500029462398, 'learning_rate': 0.001842795280078948, 'batch_size': 32, 'weight_decay': 2.4476001144900783e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:32:20,439] Trial 70 finished with value: 0.7667576559705975 and parameters: {'hidden_dim': 75, 'dropout': 0.31468246778158737, 'learning_rate': 0.004179102692815556, 'batch_size': 32, 'weight_decay': 2.5423826544468874e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:32:29,845] Trial 71 finished with value: 0.8098955145566782 and parameters: {'hidden_dim': 53, 'dropout': 0.14405150450462226, 'learning_rate': 0.0019067324650490745, 'batch_size': 32, 'weight_decay': 1.8796225520228835e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:32:36,422] Trial 72 finished with value: 0.8256737602298673 and parameters: {'hidden_dim': 45, 'dropout': 0.12874413891590408, 'learning_rate': 0.0022717599692472713, 'batch_size': 32, 'weight_decay': 2.957419835052888e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:32:42,713] Trial 73 finished with value: 0.820379384270518 and parameters: {'hidden_dim': 56, 'dropout': 0.16558559725144767, 'learning_rate': 0.0015067411991787599, 'batch_size': 32, 'weight_decay': 3.5872879742412046e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:32:48,759] Trial 74 finished with value: 0.8309963356892472 and parameters: {'hidden_dim': 42, 'dropout': 0.2009726500163924, 'learning_rate': 0.0033615875726547657, 'batch_size': 32, 'weight_decay': 1.4619859922210379e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:32:55,328] Trial 75 finished with value: 0.8106704746611811 and parameters: {'hidden_dim': 37, 'dropout': 0.2552210973751425, 'learning_rate': 0.004908056623226558, 'batch_size': 32, 'weight_decay': 1.3621497461080391e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:33:05,359] Trial 76 finished with value: 0.8224637539316598 and parameters: {'hidden_dim': 43, 'dropout': 0.17541136018356004, 'learning_rate': 0.0034136612450315867, 'batch_size': 32, 'weight_decay': 1.1020065123444903e-06}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:33:11,974] Trial 77 finished with value: 0.7949373172300903 and parameters: {'hidden_dim': 41, 'dropout': 0.20517405207096545, 'learning_rate': 0.0024450889003183214, 'batch_size': 32, 'weight_decay': 1.0418136875659028e-05}. Best is trial 46 with value: 0.8374650966460018.\n",
      "[I 2025-03-20 03:33:18,537] Trial 78 finished with value: 0.8452648176412896 and parameters: {'hidden_dim': 46, 'dropout': 0.20823070018643636, 'learning_rate': 0.002921602795527635, 'batch_size': 32, 'weight_decay': 1.6383383134508966e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:33:24,897] Trial 79 finished with value: 0.8010651881149456 and parameters: {'hidden_dim': 38, 'dropout': 0.24812479208370403, 'learning_rate': 0.003130820725442757, 'batch_size': 32, 'weight_decay': 1.7546865469562335e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:33:31,287] Trial 80 finished with value: 0.7998964630083311 and parameters: {'hidden_dim': 64, 'dropout': 0.2790286629380528, 'learning_rate': 0.0038820280625234335, 'batch_size': 32, 'weight_decay': 2.1428898986310415e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:33:41,155] Trial 81 finished with value: 0.8119831354967583 and parameters: {'hidden_dim': 41, 'dropout': 0.23225851369421782, 'learning_rate': 0.002131149089920331, 'batch_size': 32, 'weight_decay': 2.702399805610489e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:33:47,991] Trial 82 finished with value: 0.7884078537016658 and parameters: {'hidden_dim': 46, 'dropout': 0.2092244708814651, 'learning_rate': 0.0014131526371858724, 'batch_size': 32, 'weight_decay': 1.5124044297779203e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:33:54,304] Trial 83 finished with value: 0.8092656632514634 and parameters: {'hidden_dim': 34, 'dropout': 0.19572510544442842, 'learning_rate': 0.002683800587062197, 'batch_size': 32, 'weight_decay': 1.1369466901962696e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:34:00,581] Trial 84 finished with value: 0.836093789929509 and parameters: {'hidden_dim': 51, 'dropout': 0.18312441674875937, 'learning_rate': 0.0029890144947123058, 'batch_size': 32, 'weight_decay': 2.0893081554650068e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:34:10,004] Trial 85 finished with value: 0.8134313421790332 and parameters: {'hidden_dim': 50, 'dropout': 0.18472668700297032, 'learning_rate': 0.003094022004875771, 'batch_size': 32, 'weight_decay': 2.071420357320169e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:34:16,470] Trial 86 finished with value: 0.7553558948213763 and parameters: {'hidden_dim': 113, 'dropout': 0.23064891859656106, 'learning_rate': 0.004910641670692485, 'batch_size': 32, 'weight_decay': 1.5947395484733223e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:34:22,947] Trial 87 finished with value: 0.6276156067053088 and parameters: {'hidden_dim': 55, 'dropout': 0.3027728150355337, 'learning_rate': 0.00011092627208190724, 'batch_size': 32, 'weight_decay': 1.2545489956111503e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:34:29,439] Trial 88 finished with value: 0.8045298256392691 and parameters: {'hidden_dim': 51, 'dropout': 0.2040407415075126, 'learning_rate': 0.0035611736575898124, 'batch_size': 32, 'weight_decay': 1.5351420852155034e-06}. Best is trial 78 with value: 0.8452648176412896.\n",
      "[I 2025-03-20 03:34:29,454] A new study created in memory with name: no-name-320252d1-41c4-4844-a357-95634bdafe6e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LSTM Parameters:\n",
      "  hidden_dim: 46\n",
      "  dropout: 0.20823070018643636\n",
      "  learning_rate: 0.002921602795527635\n",
      "  batch_size: 32\n",
      "  weight_decay: 1.6383383134508966e-06\n",
      "\n",
      "Optimizing LightGBM for fold 4 using min_max normalization\n",
      "Optimizing LightGBM hyperparameters with CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:34:29,650] Trial 0 finished with value: 0.7079161247417418 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.013661178428178952, 'n_estimators': 456, 'num_leaves': 48, 'max_depth': 6, 'subsample': 0.7552942184455931, 'colsample_bytree': 0.7030385741204636, 'reg_alpha': 1.3087232248047334, 'reg_lambda': 0.6869757398160233, 'min_child_samples': 49}. Best is trial 0 with value: 0.7079161247417418.\n",
      "[I 2025-03-20 03:34:29,744] Trial 1 finished with value: 0.7248946429641542 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06323892443630524, 'n_estimators': 282, 'num_leaves': 42, 'max_depth': 3, 'subsample': 0.9077831487751247, 'colsample_bytree': 0.6012684467021177, 'reg_alpha': 0.31439409038438626, 'reg_lambda': 1.0912521853098582, 'min_child_samples': 47}. Best is trial 1 with value: 0.7248946429641542.\n",
      "[I 2025-03-20 03:34:29,895] Trial 2 finished with value: 0.7465996476669382 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.019957907467756848, 'n_estimators': 199, 'num_leaves': 37, 'max_depth': 10, 'subsample': 0.7873081624297713, 'colsample_bytree': 0.6493149518105418, 'reg_alpha': 3.6000609130029044, 'reg_lambda': 4.050134848257446, 'min_child_samples': 21}. Best is trial 2 with value: 0.7465996476669382.\n",
      "[I 2025-03-20 03:34:29,997] Trial 3 finished with value: 0.7505225074295953 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03308671790989007, 'n_estimators': 162, 'num_leaves': 10, 'max_depth': 5, 'subsample': 0.9963593754983149, 'colsample_bytree': 0.805256515122397, 'reg_alpha': 3.0841806167907238, 'reg_lambda': 0.1301677362213169, 'min_child_samples': 21}. Best is trial 3 with value: 0.7505225074295953.\n",
      "[I 2025-03-20 03:34:30,220] Trial 4 finished with value: 0.7677197995113432 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.029437387715603364, 'n_estimators': 405, 'num_leaves': 47, 'max_depth': 6, 'subsample': 0.9716350708762611, 'colsample_bytree': 0.7998448041733087, 'reg_alpha': 2.304933318498776, 'reg_lambda': 3.3362817514757888, 'min_child_samples': 29}. Best is trial 4 with value: 0.7677197995113432.\n",
      "[I 2025-03-20 03:34:30,301] Trial 5 finished with value: 0.7307820673848363 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07045793365915588, 'n_estimators': 210, 'num_leaves': 23, 'max_depth': 3, 'subsample': 0.8092590874861125, 'colsample_bytree': 0.859403208540551, 'reg_alpha': 3.3760828530802196, 'reg_lambda': 0.15012918259918107, 'min_child_samples': 22}. Best is trial 4 with value: 0.7677197995113432.\n",
      "[I 2025-03-20 03:34:30,509] Trial 6 finished with value: 0.7441998584800948 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011970252710206382, 'n_estimators': 476, 'num_leaves': 49, 'max_depth': 4, 'subsample': 0.9816084577717837, 'colsample_bytree': 0.7776193095921519, 'reg_alpha': 0.2802529145561249, 'reg_lambda': 1.3744727196612017, 'min_child_samples': 25}. Best is trial 4 with value: 0.7677197995113432.\n",
      "[I 2025-03-20 03:34:30,664] Trial 7 finished with value: 0.7937307664969954 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0967088301559984, 'n_estimators': 367, 'num_leaves': 50, 'max_depth': 10, 'subsample': 0.6725825297010686, 'colsample_bytree': 0.7765487989776441, 'reg_alpha': 1.3901311581414446, 'reg_lambda': 1.2894120399825089, 'min_child_samples': 25}. Best is trial 7 with value: 0.7937307664969954.\n",
      "[I 2025-03-20 03:34:30,842] Trial 8 finished with value: 0.7750319127619975 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04669085237595252, 'n_estimators': 388, 'num_leaves': 42, 'max_depth': 5, 'subsample': 0.6388242673991037, 'colsample_bytree': 0.9337670455562896, 'reg_alpha': 0.11037032003620384, 'reg_lambda': 0.25448376172072, 'min_child_samples': 45}. Best is trial 7 with value: 0.7937307664969954.\n",
      "[I 2025-03-20 03:34:30,935] Trial 9 finished with value: 0.7314006195441269 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011232085595448875, 'n_estimators': 177, 'num_leaves': 29, 'max_depth': 4, 'subsample': 0.7436406503252848, 'colsample_bytree': 0.6580654068535353, 'reg_alpha': 0.1699984374960256, 'reg_lambda': 0.1466016632681982, 'min_child_samples': 16}. Best is trial 7 with value: 0.7937307664969954.\n",
      "[I 2025-03-20 03:34:31,157] Trial 10 finished with value: 0.797534840290694 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0987558719383339, 'n_estimators': 337, 'num_leaves': 18, 'max_depth': 10, 'subsample': 0.642242017563948, 'colsample_bytree': 0.9614634142305828, 'reg_alpha': 0.7548915517607346, 'reg_lambda': 0.47191940414066896, 'min_child_samples': 37}. Best is trial 10 with value: 0.797534840290694.\n",
      "[I 2025-03-20 03:34:31,356] Trial 11 finished with value: 0.7908606691234452 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08811492902543544, 'n_estimators': 327, 'num_leaves': 17, 'max_depth': 10, 'subsample': 0.6019793854602531, 'colsample_bytree': 0.993860589036455, 'reg_alpha': 0.7915653922317443, 'reg_lambda': 0.4908802288038389, 'min_child_samples': 38}. Best is trial 10 with value: 0.797534840290694.\n",
      "[I 2025-03-20 03:34:31,555] Trial 12 finished with value: 0.7877234209307469 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09822003027572318, 'n_estimators': 307, 'num_leaves': 30, 'max_depth': 8, 'subsample': 0.6801173039155465, 'colsample_bytree': 0.893626879385864, 'reg_alpha': 0.8750718436454238, 'reg_lambda': 1.8653713281408886, 'min_child_samples': 37}. Best is trial 10 with value: 0.797534840290694.\n",
      "[I 2025-03-20 03:34:31,807] Trial 13 finished with value: 0.816658404728172 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05345803972760133, 'n_estimators': 371, 'num_leaves': 19, 'max_depth': 8, 'subsample': 0.6866546830080261, 'colsample_bytree': 0.739487019699413, 'reg_alpha': 1.3765039473276297, 'reg_lambda': 0.36073977073087093, 'min_child_samples': 10}. Best is trial 13 with value: 0.816658404728172.\n",
      "[I 2025-03-20 03:34:32,071] Trial 14 finished with value: 0.8269353560322836 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05100525278048549, 'n_estimators': 257, 'num_leaves': 16, 'max_depth': 8, 'subsample': 0.7213915181789433, 'colsample_bytree': 0.9955930766794112, 'reg_alpha': 0.4759657006669087, 'reg_lambda': 0.3531473318787954, 'min_child_samples': 10}. Best is trial 14 with value: 0.8269353560322836.\n",
      "[I 2025-03-20 03:34:32,193] Trial 15 finished with value: 0.7798893229740798 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.043288657420412956, 'n_estimators': 128, 'num_leaves': 11, 'max_depth': 8, 'subsample': 0.728281809019484, 'colsample_bytree': 0.7201040728875937, 'reg_alpha': 0.40032058910065316, 'reg_lambda': 0.27407685686214406, 'min_child_samples': 13}. Best is trial 14 with value: 0.8269353560322836.\n",
      "[I 2025-03-20 03:34:32,509] Trial 16 finished with value: 0.8277764640072883 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.049153270642365854, 'n_estimators': 257, 'num_leaves': 23, 'max_depth': 8, 'subsample': 0.8460344390937014, 'colsample_bytree': 0.8679612440467525, 'reg_alpha': 0.4703040391963052, 'reg_lambda': 0.2514535341033357, 'min_child_samples': 10}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:32,788] Trial 17 finished with value: 0.8210476964965393 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03350416889348262, 'n_estimators': 255, 'num_leaves': 25, 'max_depth': 7, 'subsample': 0.872602340863694, 'colsample_bytree': 0.8654355879743639, 'reg_alpha': 0.4952065943462705, 'reg_lambda': 0.22075145628542162, 'min_child_samples': 15}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:33,023] Trial 18 finished with value: 0.8014344999752983 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0276669503148614, 'n_estimators': 247, 'num_leaves': 14, 'max_depth': 9, 'subsample': 0.8556167667539123, 'colsample_bytree': 0.9117968576699139, 'reg_alpha': 0.2193792490608938, 'reg_lambda': 0.6912140904160914, 'min_child_samples': 10}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:33,270] Trial 19 finished with value: 0.7985999420434674 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.019720789477715744, 'n_estimators': 241, 'num_leaves': 24, 'max_depth': 7, 'subsample': 0.8211640614930323, 'colsample_bytree': 0.9966144955579284, 'reg_alpha': 0.5553562322861813, 'reg_lambda': 0.38401393858894123, 'min_child_samples': 17}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:33,373] Trial 20 finished with value: 0.7942533826660656 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06837882431300933, 'n_estimators': 102, 'num_leaves': 36, 'max_depth': 9, 'subsample': 0.9350067851755174, 'colsample_bytree': 0.8461262288632136, 'reg_alpha': 0.1682598346533869, 'reg_lambda': 0.10341899854233196, 'min_child_samples': 31}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:33,638] Trial 21 finished with value: 0.822095150005391 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.03874967976830049, 'n_estimators': 274, 'num_leaves': 25, 'max_depth': 7, 'subsample': 0.8660759529289149, 'colsample_bytree': 0.8633132535489565, 'reg_alpha': 0.4851853443851495, 'reg_lambda': 0.23847026255178824, 'min_child_samples': 16}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:33,917] Trial 22 finished with value: 0.8215038253187691 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.040613550290868924, 'n_estimators': 257, 'num_leaves': 21, 'max_depth': 7, 'subsample': 0.8497923699075266, 'colsample_bytree': 0.9517719818142399, 'reg_alpha': 0.3777879582535491, 'reg_lambda': 0.21113806297, 'min_child_samples': 12}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:34,176] Trial 23 finished with value: 0.822253369217256 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05530052949795767, 'n_estimators': 289, 'num_leaves': 15, 'max_depth': 9, 'subsample': 0.9034998290341414, 'colsample_bytree': 0.8921835573648496, 'reg_alpha': 0.560415345510661, 'reg_lambda': 0.3418710302470147, 'min_child_samples': 18}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:34,372] Trial 24 finished with value: 0.8166151685815791 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05713035493487364, 'n_estimators': 216, 'num_leaves': 14, 'max_depth': 9, 'subsample': 0.9131639376355544, 'colsample_bytree': 0.9060472041175802, 'reg_alpha': 0.9800634394236548, 'reg_lambda': 0.3480326185101311, 'min_child_samples': 19}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:34,678] Trial 25 finished with value: 0.8218764867898484 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05039377002814852, 'n_estimators': 296, 'num_leaves': 15, 'max_depth': 8, 'subsample': 0.9381892423135044, 'colsample_bytree': 0.9656562621130906, 'reg_alpha': 0.5750351119275827, 'reg_lambda': 0.5449512514378998, 'min_child_samples': 13}. Best is trial 16 with value: 0.8277764640072883.\n",
      "[I 2025-03-20 03:34:35,053] Trial 26 finished with value: 0.8303801180414954 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0725535726119176, 'n_estimators': 329, 'num_leaves': 21, 'max_depth': 9, 'subsample': 0.781004413676614, 'colsample_bytree': 0.8212722772769379, 'reg_alpha': 0.25406524866114133, 'reg_lambda': 0.9602434972897838, 'min_child_samples': 11}. Best is trial 26 with value: 0.8303801180414954.\n",
      "[I 2025-03-20 03:34:35,456] Trial 27 finished with value: 0.830152739465424 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08073177395534831, 'n_estimators': 435, 'num_leaves': 28, 'max_depth': 8, 'subsample': 0.7759775336629043, 'colsample_bytree': 0.8267339579941567, 'reg_alpha': 0.23733777101137832, 'reg_lambda': 0.8584447324093051, 'min_child_samples': 10}. Best is trial 26 with value: 0.8303801180414954.\n",
      "[I 2025-03-20 03:34:35,964] Trial 28 finished with value: 0.8370275129702451 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08161162970270688, 'n_estimators': 420, 'num_leaves': 29, 'max_depth': 9, 'subsample': 0.7927640746343508, 'colsample_bytree': 0.8286362360751653, 'reg_alpha': 0.10523475272795739, 'reg_lambda': 0.9097489246635381, 'min_child_samples': 14}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:36,473] Trial 29 finished with value: 0.8331988150280083 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07863272820324091, 'n_estimators': 431, 'num_leaves': 30, 'max_depth': 9, 'subsample': 0.7848555808565313, 'colsample_bytree': 0.8298541601769395, 'reg_alpha': 0.10691283282827799, 'reg_lambda': 0.8868588432747507, 'min_child_samples': 14}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:36,782] Trial 30 finished with value: 0.8154796855650123 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07795617178825999, 'n_estimators': 425, 'num_leaves': 33, 'max_depth': 9, 'subsample': 0.7635270001335205, 'colsample_bytree': 0.7492964883639511, 'reg_alpha': 0.10375174710797096, 'reg_lambda': 2.0103842730476247, 'min_child_samples': 27}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:37,281] Trial 31 finished with value: 0.8342432236355585 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0799007138401909, 'n_estimators': 452, 'num_leaves': 28, 'max_depth': 9, 'subsample': 0.7762805453153975, 'colsample_bytree': 0.8279205699475636, 'reg_alpha': 0.14507218546139516, 'reg_lambda': 0.877833126977965, 'min_child_samples': 15}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:41,014] Trial 32 finished with value: 0.832112854513914 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07482775610076889, 'n_estimators': 495, 'num_leaves': 33, 'max_depth': 9, 'subsample': 0.8121889890189787, 'colsample_bytree': 0.8261506145366456, 'reg_alpha': 0.13671465482313225, 'reg_lambda': 0.9531283063397826, 'min_child_samples': 13}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:41,558] Trial 33 finished with value: 0.8328299048608414 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06284737009604652, 'n_estimators': 499, 'num_leaves': 33, 'max_depth': 10, 'subsample': 0.8170820902015196, 'colsample_bytree': 0.7767653532262795, 'reg_alpha': 0.13443395035965247, 'reg_lambda': 0.7678942307055372, 'min_child_samples': 14}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:41,969] Trial 34 finished with value: 0.821679131261404 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.061414855226453596, 'n_estimators': 455, 'num_leaves': 32, 'max_depth': 10, 'subsample': 0.822144197065979, 'colsample_bytree': 0.770328402711686, 'reg_alpha': 0.14304316826781252, 'reg_lambda': 0.6534336375620594, 'min_child_samples': 20}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:42,344] Trial 35 finished with value: 0.8181578067289432 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06389267487128826, 'n_estimators': 472, 'num_leaves': 37, 'max_depth': 10, 'subsample': 0.7503381068856299, 'colsample_bytree': 0.6778666061291503, 'reg_alpha': 0.19061448038669582, 'reg_lambda': 1.7474641831677669, 'min_child_samples': 23}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:42,806] Trial 36 finished with value: 0.829514431970505 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08963612815485236, 'n_estimators': 430, 'num_leaves': 27, 'max_depth': 10, 'subsample': 0.7973895546891232, 'colsample_bytree': 0.7900134344961934, 'reg_alpha': 0.11712322753741632, 'reg_lambda': 2.616334277005541, 'min_child_samples': 15}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:43,103] Trial 37 finished with value: 0.8090302748357642 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08380782804669332, 'n_estimators': 488, 'num_leaves': 40, 'max_depth': 6, 'subsample': 0.7140579358387672, 'colsample_bytree': 0.7539547955136732, 'reg_alpha': 0.1372618829725515, 'reg_lambda': 1.2931847477056726, 'min_child_samples': 32}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:43,525] Trial 38 finished with value: 0.8160115558322154 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.019044247857628416, 'n_estimators': 450, 'num_leaves': 36, 'max_depth': 10, 'subsample': 0.7967545867059774, 'colsample_bytree': 0.8056578832319851, 'reg_alpha': 0.3153623634253133, 'reg_lambda': 0.7824314494276792, 'min_child_samples': 19}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:43,856] Trial 39 finished with value: 0.8136624089931328 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.02448691445775541, 'n_estimators': 408, 'num_leaves': 46, 'max_depth': 9, 'subsample': 0.8335981354302656, 'colsample_bytree': 0.60951827827375, 'reg_alpha': 0.20028983070202747, 'reg_lambda': 1.1848568190940958, 'min_child_samples': 23}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:44,383] Trial 40 finished with value: 0.8306964180129333 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06366978526726544, 'n_estimators': 473, 'num_leaves': 31, 'max_depth': 10, 'subsample': 0.7652683511662106, 'colsample_bytree': 0.7084985573938939, 'reg_alpha': 0.12629617119075542, 'reg_lambda': 1.5741005153783705, 'min_child_samples': 14}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:44,911] Trial 41 finished with value: 0.8309896342121201 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07468326149978946, 'n_estimators': 497, 'num_leaves': 33, 'max_depth': 9, 'subsample': 0.8112150453973066, 'colsample_bytree': 0.8356847249465785, 'reg_alpha': 0.15283912476064193, 'reg_lambda': 0.9545265799113191, 'min_child_samples': 14}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:45,378] Trial 42 finished with value: 0.8272582693181292 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.07061728074901534, 'n_estimators': 495, 'num_leaves': 34, 'max_depth': 9, 'subsample': 0.8817146120829957, 'colsample_bytree': 0.8065144918235563, 'reg_alpha': 0.1019007030062178, 'reg_lambda': 0.5901232579872585, 'min_child_samples': 17}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:45,560] Trial 43 finished with value: 0.8047526392311977 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08500315524381467, 'n_estimators': 411, 'num_leaves': 27, 'max_depth': 9, 'subsample': 0.7869110358662916, 'colsample_bytree': 0.8431544004916417, 'reg_alpha': 1.9768957255486517, 'reg_lambda': 1.0570684316206826, 'min_child_samples': 13}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:45,682] Trial 44 finished with value: 0.746121158512914 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09139054662962572, 'n_estimators': 450, 'num_leaves': 38, 'max_depth': 10, 'subsample': 0.7478414959622504, 'colsample_bytree': 0.7879572364568634, 'reg_alpha': 4.673879544484267, 'reg_lambda': 0.7841346939518266, 'min_child_samples': 21}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:46,081] Trial 45 finished with value: 0.8147141781862872 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.015329423109695761, 'n_estimators': 386, 'num_leaves': 30, 'max_depth': 10, 'subsample': 0.8336524759621167, 'colsample_bytree': 0.8146304364503378, 'reg_alpha': 0.1280729887899219, 'reg_lambda': 1.399650784403024, 'min_child_samples': 17}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:46,606] Trial 46 finished with value: 0.8339879858023025 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.05893265867975794, 'n_estimators': 471, 'num_leaves': 35, 'max_depth': 9, 'subsample': 0.8069935533586405, 'colsample_bytree': 0.767051953845024, 'reg_alpha': 0.1728968767219544, 'reg_lambda': 0.4466257976980025, 'min_child_samples': 12}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:46,791] Trial 47 finished with value: 0.7710086669031873 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.061769111449550274, 'n_estimators': 357, 'num_leaves': 39, 'max_depth': 5, 'subsample': 0.7335165008914666, 'colsample_bytree': 0.767124427401129, 'reg_alpha': 0.1746562076590726, 'reg_lambda': 0.4481472737159364, 'min_child_samples': 46}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:47,032] Trial 48 finished with value: 0.7880250942425406 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.06686220213778493, 'n_estimators': 470, 'num_leaves': 35, 'max_depth': 8, 'subsample': 0.7066083646773722, 'colsample_bytree': 0.7906688906580686, 'reg_alpha': 0.2828742316367011, 'reg_lambda': 0.6021025613800635, 'min_child_samples': 49}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:47,460] Trial 49 finished with value: 0.8325615609425558 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.04592418658362258, 'n_estimators': 392, 'num_leaves': 44, 'max_depth': 10, 'subsample': 0.7703294162148383, 'colsample_bytree': 0.8816294629837637, 'reg_alpha': 0.11852928395317112, 'reg_lambda': 0.738477091125083, 'min_child_samples': 15}. Best is trial 28 with value: 0.8370275129702451.\n",
      "[I 2025-03-20 03:34:47,578] A new study created in memory with name: no-name-8f97234d-ef1b-4d3f-9adf-a76e8881719a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LightGBM Parameters:\n",
      "  boosting_type: gbdt\n",
      "  learning_rate: 0.08161162970270688\n",
      "  n_estimators: 420\n",
      "  num_leaves: 29\n",
      "  max_depth: 9\n",
      "  subsample: 0.7927640746343508\n",
      "  colsample_bytree: 0.8286362360751653\n",
      "  reg_alpha: 0.10523475272795739\n",
      "  reg_lambda: 0.9097489246635381\n",
      "  min_child_samples: 14\n",
      "\n",
      "Optimizing LSTM for fold 4 using min_max normalization\n",
      "Created LSTM sequences with lookback=10: 608 train, 145 test\n",
      "Optimizing LSTM hyperparameters with CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 03:35:03,477] Trial 0 finished with value: 0.5754726009718998 and parameters: {'hidden_dim': 45, 'dropout': 0.4551406810921512, 'learning_rate': 0.007331811314274292, 'batch_size': 16, 'weight_decay': 2.064121088592392e-05}. Best is trial 0 with value: 0.5754726009718998.\n",
      "[I 2025-03-20 03:35:15,493] Trial 1 finished with value: 0.6835778163308509 and parameters: {'hidden_dim': 79, 'dropout': 0.48569536770002286, 'learning_rate': 0.003999161563860724, 'batch_size': 32, 'weight_decay': 3.45872269711161e-05}. Best is trial 1 with value: 0.6835778163308509.\n",
      "[I 2025-03-20 03:35:31,801] Trial 2 finished with value: 0.7121915131941408 and parameters: {'hidden_dim': 32, 'dropout': 0.21712375681979076, 'learning_rate': 0.0016665895921660098, 'batch_size': 16, 'weight_decay': 2.1517123363927774e-05}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:35:40,215] Trial 3 finished with value: 0.6373089601388842 and parameters: {'hidden_dim': 127, 'dropout': 0.2873081624297713, 'learning_rate': 0.00017643094449433023, 'batch_size': 32, 'weight_decay': 1.0947309020486406e-05}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:35:59,588] Trial 4 finished with value: 0.6819383838727134 and parameters: {'hidden_dim': 47, 'dropout': 0.10585094112968818, 'learning_rate': 0.0004451295365396247, 'batch_size': 16, 'weight_decay': 1.3639281514957935e-06}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:36:04,370] Trial 5 finished with value: 0.6899680564783649 and parameters: {'hidden_dim': 59, 'dropout': 0.2875597071705984, 'learning_rate': 0.0033384615669657483, 'batch_size': 64, 'weight_decay': 9.98214837316597e-06}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:36:23,572] Trial 6 finished with value: 0.6500151654382194 and parameters: {'hidden_dim': 109, 'dropout': 0.4586320455614753, 'learning_rate': 0.0009209968619749436, 'batch_size': 16, 'weight_decay': 1.6223673683559225e-06}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:36:31,866] Trial 7 finished with value: 0.6324577911397068 and parameters: {'hidden_dim': 82, 'dropout': 0.3594032085405511, 'learning_rate': 0.006298297451196278, 'batch_size': 32, 'weight_decay': 7.511737052942637e-05}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:36:36,541] Trial 8 finished with value: 0.6319819973462154 and parameters: {'hidden_dim': 125, 'dropout': 0.17649687394806254, 'learning_rate': 0.00809174687672524, 'batch_size': 64, 'weight_decay': 5.656354626844694e-06}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:36:55,822] Trial 9 finished with value: 0.5985045420237322 and parameters: {'hidden_dim': 127, 'dropout': 0.3668293205677101, 'learning_rate': 0.009614679235250567, 'batch_size': 16, 'weight_decay': 2.2160868002868673e-05}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:37:11,529] Trial 10 finished with value: 0.6804630060009913 and parameters: {'hidden_dim': 32, 'dropout': 0.198627869896562, 'learning_rate': 0.0015689723478599755, 'batch_size': 16, 'weight_decay': 8.469738945067809e-05}. Best is trial 2 with value: 0.7121915131941408.\n",
      "[I 2025-03-20 03:37:16,354] Trial 11 finished with value: 0.714365164093552 and parameters: {'hidden_dim': 64, 'dropout': 0.25668451047832225, 'learning_rate': 0.0020832659212191686, 'batch_size': 64, 'weight_decay': 4.899133576690419e-06}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:37:21,200] Trial 12 finished with value: 0.677213004220274 and parameters: {'hidden_dim': 72, 'dropout': 0.21598500798518633, 'learning_rate': 0.0015088018581849542, 'batch_size': 64, 'weight_decay': 3.460656555538206e-06}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:37:29,280] Trial 13 finished with value: 0.6232077362531769 and parameters: {'hidden_dim': 95, 'dropout': 0.24296708365322092, 'learning_rate': 0.0007285120693189867, 'batch_size': 64, 'weight_decay': 4.459010219894359e-06}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:37:34,037] Trial 14 finished with value: 0.7086120433171387 and parameters: {'hidden_dim': 33, 'dropout': 0.11485150758934834, 'learning_rate': 0.0017038860393305435, 'batch_size': 64, 'weight_decay': 1.2545792342299691e-05}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:37:50,153] Trial 15 finished with value: 0.6413311727720871 and parameters: {'hidden_dim': 58, 'dropout': 0.3402754772779238, 'learning_rate': 0.0003539798922574453, 'batch_size': 16, 'weight_decay': 2.5886593912338396e-06}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:37:54,920] Trial 16 finished with value: 0.6753998924453652 and parameters: {'hidden_dim': 61, 'dropout': 0.26430609192606264, 'learning_rate': 0.0024684247055058992, 'batch_size': 64, 'weight_decay': 4.4383979493096764e-05}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:38:14,326] Trial 17 finished with value: 0.604792942002994 and parameters: {'hidden_dim': 90, 'dropout': 0.1572378497520306, 'learning_rate': 0.00010728531826263386, 'batch_size': 16, 'weight_decay': 5.847918834978513e-06}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:38:19,014] Trial 18 finished with value: 0.6538739754675406 and parameters: {'hidden_dim': 43, 'dropout': 0.32294830382545214, 'learning_rate': 0.0006275250989672774, 'batch_size': 64, 'weight_decay': 1.7445159426190412e-05}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:38:27,118] Trial 19 finished with value: 0.679317249646247 and parameters: {'hidden_dim': 70, 'dropout': 0.3982201913987795, 'learning_rate': 0.0025483949776746854, 'batch_size': 32, 'weight_decay': 7.823169758168759e-06}. Best is trial 11 with value: 0.714365164093552.\n",
      "[I 2025-03-20 03:38:34,960] Trial 20 finished with value: 0.7208837966595434 and parameters: {'hidden_dim': 52, 'dropout': 0.23524611020501152, 'learning_rate': 0.004681922702699792, 'batch_size': 64, 'weight_decay': 3.3438390648003014e-05}. Best is trial 20 with value: 0.7208837966595434.\n",
      "[I 2025-03-20 03:38:39,737] Trial 21 finished with value: 0.6486531193568928 and parameters: {'hidden_dim': 52, 'dropout': 0.23263476623991738, 'learning_rate': 0.004881505957897184, 'batch_size': 64, 'weight_decay': 3.478261170247822e-05}. Best is trial 20 with value: 0.7208837966595434.\n",
      "[I 2025-03-20 03:38:44,599] Trial 22 finished with value: 0.6752254187897434 and parameters: {'hidden_dim': 38, 'dropout': 0.1593592420234631, 'learning_rate': 0.002318999954988156, 'batch_size': 64, 'weight_decay': 5.957050939957761e-05}. Best is trial 20 with value: 0.7208837966595434.\n",
      "[I 2025-03-20 03:38:49,247] Trial 23 finished with value: 0.6822544446140043 and parameters: {'hidden_dim': 66, 'dropout': 0.2555819116296019, 'learning_rate': 0.0013374201487084808, 'batch_size': 64, 'weight_decay': 2.6934926307977124e-05}. Best is trial 20 with value: 0.7208837966595434.\n",
      "[I 2025-03-20 03:38:53,867] Trial 24 finished with value: 0.7268495182481997 and parameters: {'hidden_dim': 53, 'dropout': 0.19423905211637288, 'learning_rate': 0.004880040523791641, 'batch_size': 64, 'weight_decay': 1.3986612110065837e-05}. Best is trial 24 with value: 0.7268495182481997.\n",
      "[I 2025-03-20 03:38:58,598] Trial 25 finished with value: 0.746154607645331 and parameters: {'hidden_dim': 55, 'dropout': 0.18438257487142248, 'learning_rate': 0.004697852565029639, 'batch_size': 64, 'weight_decay': 1.4909521958280094e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:06,647] Trial 26 finished with value: 0.7154582862902092 and parameters: {'hidden_dim': 52, 'dropout': 0.13366729650566153, 'learning_rate': 0.005009906107160121, 'batch_size': 64, 'weight_decay': 1.4916503610592851e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:11,481] Trial 27 finished with value: 0.687462940353957 and parameters: {'hidden_dim': 53, 'dropout': 0.18731708219015683, 'learning_rate': 0.003383836781772956, 'batch_size': 64, 'weight_decay': 8.435281121384504e-06}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:16,037] Trial 28 finished with value: 0.6507187141872894 and parameters: {'hidden_dim': 75, 'dropout': 0.14936473131964567, 'learning_rate': 0.00623973190826259, 'batch_size': 64, 'weight_decay': 3.031369713639327e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:20,813] Trial 29 finished with value: 0.6310487935123233 and parameters: {'hidden_dim': 43, 'dropout': 0.19913619807977329, 'learning_rate': 0.009777962062434373, 'batch_size': 64, 'weight_decay': 1.4224891712957988e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:25,512] Trial 30 finished with value: 0.6583335356615818 and parameters: {'hidden_dim': 48, 'dropout': 0.30347745635750045, 'learning_rate': 0.003910307993144254, 'batch_size': 64, 'weight_decay': 5.5010173144763924e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:30,011] Trial 31 finished with value: 0.6876588697405307 and parameters: {'hidden_dim': 54, 'dropout': 0.13340119811184512, 'learning_rate': 0.004711880153865995, 'batch_size': 64, 'weight_decay': 1.6166760426684973e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:34,540] Trial 32 finished with value: 0.6661751620888909 and parameters: {'hidden_dim': 39, 'dropout': 0.17710877613752257, 'learning_rate': 0.0056123225315516095, 'batch_size': 64, 'weight_decay': 2.1728967041535217e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:42,310] Trial 33 finished with value: 0.6696011748706109 and parameters: {'hidden_dim': 52, 'dropout': 0.12991987565042104, 'learning_rate': 0.006972815628155757, 'batch_size': 64, 'weight_decay': 4.017685738427029e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:50,453] Trial 34 finished with value: 0.6787785483773362 and parameters: {'hidden_dim': 67, 'dropout': 0.22229432754066142, 'learning_rate': 0.002987876894037972, 'batch_size': 32, 'weight_decay': 7.349469360853257e-06}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:39:54,780] Trial 35 finished with value: 0.7156644641613289 and parameters: {'hidden_dim': 81, 'dropout': 0.10214055723548332, 'learning_rate': 0.004527822428490007, 'batch_size': 64, 'weight_decay': 2.5686769020987364e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:40:02,837] Trial 36 finished with value: 0.6994613353286404 and parameters: {'hidden_dim': 90, 'dropout': 0.1084149606736535, 'learning_rate': 0.00395526492920058, 'batch_size': 32, 'weight_decay': 2.5848649712030152e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:40:07,426] Trial 37 finished with value: 0.63309800163855 and parameters: {'hidden_dim': 80, 'dropout': 0.20779889406856303, 'learning_rate': 0.007226103949730325, 'batch_size': 64, 'weight_decay': 9.95184279129232e-06}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:40:15,472] Trial 38 finished with value: 0.6683052702186445 and parameters: {'hidden_dim': 103, 'dropout': 0.27283030510898065, 'learning_rate': 0.0029879047382652337, 'batch_size': 64, 'weight_decay': 1.9478536155086967e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:40:20,465] Trial 39 finished with value: 0.657419065899489 and parameters: {'hidden_dim': 60, 'dropout': 0.1642255217338801, 'learning_rate': 0.0011986298808486018, 'batch_size': 64, 'weight_decay': 5.078424653359389e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:40:29,887] Trial 40 finished with value: 0.6600783755916982 and parameters: {'hidden_dim': 85, 'dropout': 0.2326171615200422, 'learning_rate': 0.0042985625023967, 'batch_size': 32, 'weight_decay': 3.4937421492042685e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:40:34,420] Trial 41 finished with value: 0.7150811653721209 and parameters: {'hidden_dim': 76, 'dropout': 0.12699324785245938, 'learning_rate': 0.005400996803692855, 'batch_size': 64, 'weight_decay': 1.2062320653433233e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:40:38,976] Trial 42 finished with value: 0.6666636552769022 and parameters: {'hidden_dim': 47, 'dropout': 0.1443451071033302, 'learning_rate': 0.008015215485919762, 'batch_size': 64, 'weight_decay': 1.5179567014220298e-05}. Best is trial 25 with value: 0.746154607645331.\n",
      "[I 2025-03-20 03:40:46,633] Trial 43 finished with value: 0.7274081627531989 and parameters: {'hidden_dim': 49, 'dropout': 0.10857552312215266, 'learning_rate': 0.0037655908918452757, 'batch_size': 64, 'weight_decay': 2.478155352883063e-05}. Best is trial 25 with value: 0.746154607645331.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Impact of Normalization Techniques with Statistical Significance Testing\")\n",
    "print(\"=\"*60)\n",
    "norm_methods = [\n",
    "    \"min_max\",\n",
    "    \"z_score\",\n",
    "    \"median\",\n",
    "    \"sigmoid\",\n",
    "    \"tanh_estimator\"\n",
    "]\n",
    "baseline = \"min_max\"\n",
    "fast_mode = False\n",
    "n_folds = 10\n",
    "time_series_split = True\n",
    "gap = 5\n",
    "\n",
    "research = NormalizationResearch(\n",
    "    cache_path=\"data\",\n",
    "    fast_mode=fast_mode,\n",
    "    start_date=\"2014-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    norm_methods=norm_methods,\n",
    "    baseline_method=baseline,\n",
    "    market_index=\"OSEBX.OL\",\n",
    "    n_folds=n_folds,\n",
    "    time_series_split=time_series_split,\n",
    "    gap=gap,\n",
    "    lookback_window=10,\n",
    "    random_seed=2025\n",
    ")\n",
    "\n",
    "results = research.run_normalization_comparison()\n",
    "\n",
    "research.perform_statistical_tests()\n",
    "research.print_statistical_summary()\n",
    "research.plot_statistical_significance()\n",
    "research.plot_pvalue_heatmap()\n",
    "\n",
    "research.print_summary()\n",
    "research.save_results_to_csv(\"results/normalization_results_osebx_with_stats.csv\")\n",
    "research.plot_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-exam",
   "language": "python",
   "name": "big-data-exam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
